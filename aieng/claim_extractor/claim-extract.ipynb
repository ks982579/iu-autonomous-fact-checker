{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddb4a4f8-6852-4470-8c79-b836c8477b90",
   "metadata": {},
   "source": [
    "# Statement Classifier\n",
    "\n",
    "This project's goal is to train a model that can determine if a statement is either a claim that can be fact-checked, or some other statement like an opinion that cannot be fact checked. \n",
    "\n",
    "## TODO:\n",
    "\n",
    "- [ ] Before training again, setup file structure for saving the 'latest' model, and moving them back into time-stamped dirs, either save some metadata file or something. \n",
    "- [ ] Config at top to control what runs when you click GO.\n",
    "- [ ] function-ize processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06276f02-8227-4961-926d-68698649d640",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    pipeline\n",
    ")\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.utils import resample\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import re\n",
    "from typing import List\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecb714a-dd3c-4522-a851-08f102d32b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%lsmagic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099785cc-6380-4a5a-848f-1ad363d1a3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03ed394",
   "metadata": {},
   "source": [
    "Received the following error whilst training the model in first few attempts:\n",
    "\n",
    "```\n",
    "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
    "To disable this warning, you can either:\n",
    "    - Avoid using `tokenizers` before the fork if possible\n",
    "    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
    "```\n",
    "\n",
    "To address this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b847075",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715da55e-e7f7-4eb2-8932-3953fb368d85",
   "metadata": {},
   "source": [
    "## Step 2: Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec527565-22e2-44c0-b0c6-813108d0d6ce",
   "metadata": {},
   "source": [
    "### Sample Data\n",
    "\n",
    "I started with the CSV data, but I did not need the extra information so settling with the JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ee1883",
   "metadata": {},
   "outputs": [],
   "source": [
    "here = Path().cwd()\n",
    "cbdata_path = here / \".data_sets\" / \"ClaimBuster_Datasets\" / \"datasets\" # ClaimBuster data location\n",
    "raw_dfs: List[pd.DataFrame] = []\n",
    "\n",
    "for file_path in cbdata_path.iterdir():\n",
    "    if file_path.exists() and file_path.is_file() and file_path.suffix == \".json\":\n",
    "        with open(file_path, 'r') as fileo:\n",
    "            raw_dfs.append(pd.DataFrame(json.load(fileo)))\n",
    "\n",
    "assert len(raw_dfs) > 0\n",
    "\n",
    "for i, j in enumerate(raw_dfs):\n",
    "    assert j is not None\n",
    "    assert type(j) is pd.DataFrame\n",
    "    print(f\"--- part {i+1:02} ---\")\n",
    "    print(j.head())\n",
    "    print(j.describe())\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59e4630",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(raw_dfs)\n",
    "print(df.describe())\n",
    "print(f\"Dataset Size: {len(df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf2798f",
   "metadata": {},
   "source": [
    "### Additional Data Exploring\n",
    "\n",
    "After building the model and performing some manual testing, the statement, \"Barack Obama was president from 2009 to 2017,\" kept being returned as an opinion when it is actually a verifiable claim.\n",
    "\n",
    "Ended up moving this to its own file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a925905",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    for sent in df[\"text\"]:\n",
    "        if \"obama\" in sent.casefold():\n",
    "            print(sent)\n",
    "\n",
    "if False:\n",
    "    obama_mask = df[\"text\"].str.contains(\"Obama\", case=False, na=False)\n",
    "    obama_df = df.copy()[obama_mask] # Only Obama entries\n",
    "    obama_claims_df = obama_df[obama_df[\"label\"] == 1 ]\n",
    "    obama_opinions_df = obama_df[obama_df[\"label\"] == 0 ]\n",
    "\n",
    "    obama_mentions_count = len(obama_df)\n",
    "    obama_claims_count = len(obama_claims_df)\n",
    "    obama_opinions_count = len(obama_opinions_df)\n",
    "    print(f\"Total Obama mentions: {obama_mentions_count}\")\n",
    "    print(f\"Obama Claims (LABEL_1): {obama_claims_count}\")\n",
    "    print(f\"Obama Opinions (LABEL_0): {obama_opinions_count}\")\n",
    "    print(f\"Obama Claim Percentage: {(obama_claims_count / obama_mentions_count) * 100}%\")\n",
    "\n",
    "    print(\"\\nSample Obama Entries as Claims\")\n",
    "    print(\"---\" * 5 + \" Claims \" + \"---\" * 5)\n",
    "    print(obama_claims_df.head(10))\n",
    "    print(\"---\" * 5 + \" Opinions \" + \"---\" * 5)\n",
    "    print(obama_opinions_df.head(10))\n",
    "    # for i, text in enumerate(obama_claims_df[\"text\"].head(10)):\n",
    "    #     print(f\"{i}.) \\\"{text}\\\"\")\n",
    "    # print(\"\\nSample Obama Entries as Claims\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f2bdf8-3fc2-475c-ac28-8adac2cf89b9",
   "metadata": {},
   "source": [
    "## Data Fixing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4eee1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== CLAIMBUSTERS DATA QUALITY ANALYSIS ===\\n\")\n",
    "\n",
    "# 1. Check label distribution\n",
    "print(\"1. LABEL DISTRIBUTION:\")\n",
    "print(f\"Total samples: {len(df)}\")\n",
    "print(f\"Claims (LABEL_1): {len(df[df['label'] == 1])} ({len(df[df['label'] == 1])/len(df)*100:.1f}%)\")\n",
    "print(f\"Non-claims (LABEL_0): {len(df[df['label'] == 0])} ({len(df[df['label'] == 0])/len(df)*100:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602f61b9",
   "metadata": {},
   "source": [
    "### Balancing Data\n",
    "\n",
    "The current data is unbalanced - caused issues in the first model.\n",
    "\n",
    "Options appear to be \"Downsampling\" or computing weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50246d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into majority / minority\n",
    "df_nonclaims_majority = df[df[\"label\"] == 0] # non-claims\n",
    "df_claims_minority = df[df[\"label\"] == 1] # claims\n",
    "\n",
    "print(f\"Non-Claims: {len(df_nonclaims_majority)}\")\n",
    "print(f\"Claims: {len(df_claims_minority)}\")\n",
    "\n",
    "df_nonclaims_downsampled = resample(\n",
    "    df_nonclaims_majority,\n",
    "    replace=False, # sample without replacement\n",
    "    n_samples=len(df_claims_minority),\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Non-Claims: {len(df_nonclaims_majority)}\")\n",
    "print(f\"Non-Claims Down Sampled: {len(df_nonclaims_downsampled)}\")\n",
    "print(f\"Claims: {len(df_claims_minority)}\")\n",
    "\n",
    "# Data Frame Balanced: equal parts claim and non-claim\n",
    "dfb = pd.concat([df_nonclaims_downsampled, df_claims_minority])\n",
    "dfb.describe()\n",
    "\n",
    "unused_df = df_nonclaims_majority.drop(df_nonclaims_downsampled.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3cb6ed-5c27-4318-9b4c-39402fd7b19e",
   "metadata": {},
   "source": [
    "### Split Data\n",
    "\n",
    "Splitting the data into train and validation/test. \n",
    "Also, I think above we sorted the data by label, so [with this strategy](https://stackoverflow.com/questions/29576430/shuffle-dataframe-rows) we can shuffle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16678117-8b8a-4724-a9b9-97aca917cc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle\n",
    "dfb = dfb.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Split into train/validation sets\n",
    "# Data Frame Training\n",
    "# Data Frame Validation\n",
    "dft, dfv = train_test_split(\n",
    "    dfb,\n",
    "    test_size=0.2, \n",
    "    random_state=42,\n",
    "    shuffle=True,\n",
    "    stratify=dfb[\"label\"],\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(dft)}\")\n",
    "print(f\"Validation samples: {len(dfv)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc2e45e-15a3-4daa-ab59-8066867ac160",
   "metadata": {},
   "source": [
    "## Step 3: Load and Setup BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351c3397-3f67-484f-bec4-de0f32417a92",
   "metadata": {},
   "source": [
    "### Initialize Tokenizer and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef607283-a81b-4ac3-ad58-e1c5047eeee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose your BERT variant\n",
    "# TODO: Add in config at top\n",
    "model_name = \"bert-base-uncased\"  # Good starting point\n",
    "# Alternatives: \"roberta-base\", \"distilbert-base-uncased\" (faster)\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# Ran into tokenization issue - All tensors in a batch should be same length\n",
    "# Some were 100 and but one was 187.\n",
    "# Use padding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name, \n",
    "    num_labels=2  # Binary classification: claim vs opinion\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "print(f\"Model loaded: {model_name}\")\n",
    "print(f\"Vocabulary size: {tokenizer.vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443c15ed-27fe-4269-a29a-1d34df6e8117",
   "metadata": {},
   "source": [
    "### Tokenize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee539200",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples['text'], \n",
    "        truncation=True, \n",
    "        padding=True, \n",
    "        max_length=256  # Adjust based on your text length\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb9b832-9c74-4d30-ad9b-fd218b948b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert pandas DataFrames to ðŸ¤— Dataset objects\n",
    "dst = Dataset.from_pandas(dft)\n",
    "dsv = Dataset.from_pandas(dfv)\n",
    "\n",
    "# Apply tokenization\n",
    "train_dataset = dst.map(tokenize_function, batched=True)\n",
    "val_dataset = dsv.map(tokenize_function, batched=True)\n",
    "\n",
    "print(\"Data tokenized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c760390-6573-4142-9d3c-4f2d6fba541f",
   "metadata": {},
   "source": [
    "## Step 4: Fine-Tune Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23d54fa-fa26-49da-886b-7953a0313307",
   "metadata": {},
   "source": [
    "We are doing **transfer learning** with **fine-tuning**. \n",
    "BERT was pre-trained to understand language - Thank you!\n",
    "We fine-tuning the model for a specific task - claim vs opinion here.\n",
    "The technique = Supervised learning with backpropagation\n",
    "\n",
    "Deep dive: BERT has millions of weights to understand language. We are adjusting these to suit our classification task. Only our final classification layer is learning from scratch. The rest of BERT is merely adapting instead of being completely retrained. \n",
    "BERT (I think) expects a \"[MASK]\" token to predict values. \n",
    "By fine-tuning, we add a layer like: `input text -> BERT Encoder -> Classification Head -> [Claim, Opinion] probabilities`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f8686b-5379-4460-aa4f-b61f01096350",
   "metadata": {},
   "source": [
    "### Define Training Arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb8524b-2529-4bc4-bf37-2e108cd72272",
   "metadata": {},
   "source": [
    "[transformers.TrainingArguments](https://huggingface.co/docs/transformers/v4.52.3/en/main_classes/trainer#transformers.TrainingArguments) has a lot of parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27a93b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up directories for saving\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "# TODO: Give model name at top\n",
    "move_path = Path().cwd() / \"trainingresults\" / f'hide-bert_{timestamp}'\n",
    "out_path = Path().cwd() / \"trainingresults\" / \"latest\"\n",
    "metatdata_file_path = out_path / \"metadata.json\"\n",
    "if False:\n",
    "    if metatdata_file_path.exists():\n",
    "        # A model exists in latest already - move to it's timestamp\n",
    "        with open(metatdata_file_path, 'r') as file:\n",
    "            tmp = json.load(file)\n",
    "            ts_path = Path(tmp.path)\n",
    "            out_path.rename(ts_path)\n",
    "        assert not out_path.exists()\n",
    "\n",
    "    with open(out_path / \"metadata.json\", 'w') as file:\n",
    "        json.dump({\"path\": str(out_path), \"foundation\": model_name}, file, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f45727b-67f8-4f14-85a9-0c9e27c51bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=out_path, # Working directory during training for logs and checkpoints.\n",
    "    num_train_epochs=3,              # Start with 3, adjust based on results\n",
    "    per_device_train_batch_size=16,  # Reduce if memory issues\n",
    "    per_device_eval_batch_size=16,\n",
    "    warmup_steps=500, # gradually increase learning rate over 500 steps | prevents huge descrutive changes early on\n",
    "    weight_decay=0.01, # Very mild 1% to prevent memorizing training data exactly. \n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    dataloader_pin_memory=False, # can help with GPU transfer speed\n",
    "    fp16=True, # mixed precision can speedup training if supported\n",
    "    dataloader_num_workers=4, # parallel data loading\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ccafc1-ad8d-4f2e-b5a2-72942d938e2d",
   "metadata": {},
   "source": [
    "### Define Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289bdcd7-1660-4980-b12b-3d8552b3a583",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels, predictions, average='weighted'\n",
    "    )\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585a5fb7-bb35-4f47-bf03-c712c233cf8e",
   "metadata": {},
   "source": [
    "### Initialize and Train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801df9d2-3381-4c84-a1c1-f0369de2bc8f",
   "metadata": {},
   "source": [
    "This is the fun part we all want to do :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04c1ff8-c218-4086-a2d9-aa722c602f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "print(\"Starting training...\")\n",
    "if False:\n",
    "    trainer.train()\n",
    "\n",
    "    # TODO: Update Path - the latest idea and switching...\n",
    "    # Save the model\n",
    "    trainer.save_model(out_path) # Where to save model weights and config\n",
    "    tokenizer.save_pretrained(out_path) # for tokenizer stuff\n",
    "    print(\"Model saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dadf6e7-9380-4abd-9f70-d0621a8e2890",
   "metadata": {},
   "source": [
    "## Step 5: Test Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a071d9c-eeda-430d-ba9c-c6899d498285",
   "metadata": {},
   "source": [
    "### Load Trained Model for Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f66975a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValidationEntry:\n",
    "    def __init__(self, statement: str, expected: int):\n",
    "        self.statement = statement\n",
    "        self.expected = expected\n",
    "    \n",
    "    def __str__(self):\n",
    "        line1 = f\"{self.__class__}\\n\"\n",
    "        line2 = f\"  Statement: {self.statement}\\n\"\n",
    "        line3 = f\"  Expectation: {'Opinion' if 0 else 'Claim'}\\n\"\n",
    "        return line1 + line2 + line3\n",
    "\n",
    "# ADD LATER\n",
    "manual_tests = [\n",
    "    (\"John Smith was elected mayor in 2020\", 1),\n",
    "    (\"The company reported $2 million in revenue\", 1),\n",
    "    (\"She graduated from Harvard University\", 1),\n",
    "    (\"Billy Joe graduated from Harvard University\", 1),\n",
    "    (\"The meeting was scheduled for 3 PM\", 1),\n",
    "    (\"COVID-19 cases increased by 15% last month\", 1),\n",
    "    (\"This is the best restaurant in town\", 0),\n",
    "    (\"We should invest more in education\", 0),\n",
    "    (\"That movie was terrible\", 0),\n",
    "    (\"This policy is unfair to working families\", 0),\n",
    "    (\"Climate change is the most important issue\", 0),\n",
    "    (\"Barack Obama was president from 2009 to 2017\", 1),\n",
    "    (\"Pizza is the most delicious food ever\", 0),\n",
    "    (\"The stock market closed at 4,500 points\", 1),\n",
    "    (\"This movie deserves an Oscar\", 0),\n",
    "    (\"The man Barack Obama served as Senator from Illinois before becoming president.\", 1),\n",
    "    (\"The man John Doe served as Senator from Illinois before becoming president.\", 1),\n",
    "    (\"Barack Obama won the Nobel Peace Prize in 2009\", 1),\n",
    "    (\"George Washington won the Nobel Peace Prize in 2009\", 1),\n",
    "    (\"Ada Lovelace wrote the first computer program way back in the 1840s!\", 1),\n",
    "    (\"The unemployment rate in the artic is close to 0, that's amazing!\", 1),\n",
    "    (\"Donald Trump only serves himself and the top 1%.\", 0),\n",
    "    (\"Donald Trump's Big Beautiful Bill implements the biggest cut to medicaid in American history.\", 1),\n",
    "]\n",
    "\n",
    "validation_items = []\n",
    "\n",
    "for thing in manual_tests:\n",
    "    validation_items.append(ValidationEntry(thing[0], thing[1]))\n",
    "\n",
    "# for item in validation_items:\n",
    "#     print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225e1608-98ec-4887-ad9b-2c9673b82ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\")\n",
    "# Load your fine-tuned model\n",
    "# TODO: UPDATE!!!\n",
    "classifier = pipeline(\n",
    "    task=\"text-classification\",\n",
    "    model=str(out_path),\n",
    "    tokenizer=str(out_path),\n",
    "    device='cuda'\n",
    ")\n",
    "\n",
    "success_cnt = 0\n",
    "print(\"=== Testing the model ===\")\n",
    "print(\"-\" * 50)\n",
    "for i, item in enumerate(validation_items):\n",
    "    result = classifier(item.statement)\n",
    "    actual = 0 if result[0]['label'] == 'LABEL_0' else 1\n",
    "    success = actual == item.expected\n",
    "    success_label = \"PASS\" if success else \"FAIL\"\n",
    "    if success:\n",
    "        success_cnt += 1\n",
    "    prediction_label = \"Claim\" if result[0]['label'] == 'LABEL_1' else \"Opinion\"\n",
    "    expected_label = \"Claim\" if item.expected == 1 else \"Opinion\"\n",
    "    confidence = result[0]['score']\n",
    "    print(f\"Test {i+1}: {success_label}\")\n",
    "    print(result)\n",
    "    print(f\"Text: '{item.statement}'\")\n",
    "    print(f\"Prediction: {prediction_label} (confidence: {confidence:.3f})\")\n",
    "    print(f\"Expected: {expected_label}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(f\"{success_cnt} Correct\")\n",
    "print(f\"{len(validation_items) - success_cnt} Wrong\")\n",
    "print(f\"{len(validation_items)} Total\")\n",
    "print(f\"Rate of Success: {(success_cnt / len(validation_items))*100:.4f}%\")\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97e1955-2f55-4616-be90-8a728c543d59",
   "metadata": {},
   "source": [
    "### Manual Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c8f05c-239c-487e-b10f-58cda5550b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(texts, true_labels):\n",
    "    \"\"\"Evaluate model on a list of texts with known labels\"\"\"\n",
    "    predictions = []\n",
    "    \n",
    "    for text in texts:\n",
    "        result = classifier(text)\n",
    "        # Convert to binary (0 or 1)\n",
    "        pred = 1 if result[0]['label'] == 'LABEL_1' else 0\n",
    "        predictions.append(pred)\n",
    "    \n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        true_labels, predictions, average='weighted'\n",
    "    )\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy:.3f}\")\n",
    "    print(f\"Precision: {precision:.3f}\")\n",
    "    print(f\"Recall: {recall:.3f}\")\n",
    "    print(f\"F1-score: {f1:.3f}\")\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# Warning of using \"pipeline\" sequentially on GPU - use dataset instead.\n",
    "predictions = evaluate_model(dfv[\"text\"], dfv[\"label\"])\n",
    "# predictions = evaluate_model(val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74403df4-62bc-42e0-9d0e-a1a0617727b7",
   "metadata": {},
   "source": [
    "## Step 6: Integration With Fact-Checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f9eb55-bc44-44fe-865f-3c273e10c4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_claims_from_text(text):\n",
    "    \"\"\"\n",
    "    Extract potential factual claims from text\n",
    "    Returns list of sentences classified as factual claims\n",
    "    \"\"\"\n",
    "    # Simple sentence splitting (you might want to use spaCy for better results)\n",
    "    sentences = text.split('. ')\n",
    "    print(sentences)\n",
    "    \n",
    "    claims = []\n",
    "    for sentence in sentences:\n",
    "        if len(sentence.strip()) > 10:  # Skip very short sentences\n",
    "            print(sentence)\n",
    "            result = classifier(sentence)\n",
    "            print(result)\n",
    "            if result[0]['label'] == 'LABEL_1':  # Factual claim\n",
    "                claims.append({\n",
    "                    'text': sentence,\n",
    "                    'confidence': result[0]['score']\n",
    "                })\n",
    "    \n",
    "    return claims\n",
    "\n",
    "# Test with a Twitter example\n",
    "twitter_text = \"\"\"My opponent Denver Riggleman, running mate of Corey Stewart, was caught on camera campaigning with a white supremacist. Now he has been exposed as a devotee of Bigfoot erotica. This is not what we need on Capitol Hill.\"\"\"\n",
    "\n",
    "claims = extract_claims_from_text(twitter_text)\n",
    "print(f\"Extracted claims: {claims}\")\n",
    "for claim in claims:\n",
    "    print(f\"- {claim['text']} (confidence: {claim['confidence']:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77962dd3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
