{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddb4a4f8-6852-4470-8c79-b836c8477b90",
   "metadata": {},
   "source": [
    "# Statement Classifier\n",
    "\n",
    "This project's goal is to train a model that can determine if a statement is either a claim that can be fact-checked, or some other statement like an opinion that cannot be fact checked. \n",
    "\n",
    "## TODO:\n",
    "\n",
    "- [ ] Before training again, setup file structure for saving the 'latest' model, and moving them back into time-stamped dirs, either save some metadata file or something. \n",
    "- [ ] Config at top to control what runs when you click GO.\n",
    "- [ ] function-ize processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06276f02-8227-4961-926d-68698649d640",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ksull18/code/iu-autonomous-fact-checker/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    pipeline\n",
    ")\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.utils import resample\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import re\n",
    "from typing import List\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecb714a-dd3c-4522-a851-08f102d32b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%lsmagic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099785cc-6380-4a5a-848f-1ad363d1a3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03ed394",
   "metadata": {},
   "source": [
    "Received the following error whilst training the model in first few attempts:\n",
    "\n",
    "```\n",
    "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
    "To disable this warning, you can either:\n",
    "    - Avoid using `tokenizers` before the fork if possible\n",
    "    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
    "```\n",
    "\n",
    "To address this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b847075",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715da55e-e7f7-4eb2-8932-3953fb368d85",
   "metadata": {},
   "source": [
    "## Step 2: Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec527565-22e2-44c0-b0c6-813108d0d6ce",
   "metadata": {},
   "source": [
    "### Sample Data\n",
    "\n",
    "I started with the CSV data, but I did not need the extra information so settling with the JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0ee1883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- part 01 ---\n",
      "   sentence_id  label                                               text\n",
      "0        27247      1                We're 9 million jobs short of that.\n",
      "1        10766      1  You know, last year up to this time, we've los...\n",
      "2         3327      1  And in November of 1975 I was the first presid...\n",
      "3        19700      1  And what we've done during the Bush administra...\n",
      "4        12600      1  Do you know we don't have a single program spo...\n",
      "        sentence_id        label\n",
      "count   9674.000000  9674.000000\n",
      "mean   16268.353628     0.285714\n",
      "std     9388.575939     0.451777\n",
      "min       16.000000     0.000000\n",
      "25%     8344.000000     0.000000\n",
      "50%    16455.500000     0.000000\n",
      "75%    24086.250000     1.000000\n",
      "max    34458.000000     1.000000\n",
      "\n",
      "\n",
      "--- part 02 ---\n",
      "   sentence_id  label                                               text\n",
      "0        15083      1  When I made my decision to stop all trade with...\n",
      "1        16799      1  We've got the highest inflation we've had in t...\n",
      "2        32570      1  They started from that little area, and now th...\n",
      "3        17644      1  Yes, there has been an increase in poverty, bu...\n",
      "4        32512      1  And, yes, when I was a senator, I did vote to ...\n",
      "        sentence_id        label\n",
      "count   8292.000000  8292.000000\n",
      "mean   16231.974433     0.333333\n",
      "std     9412.544929     0.471433\n",
      "min       20.000000     0.000000\n",
      "25%     8379.500000     0.000000\n",
      "50%    16394.500000     0.000000\n",
      "75%    24028.250000     1.000000\n",
      "max    34458.000000     1.000000\n",
      "\n",
      "\n",
      "--- part 03 ---\n",
      "   sentence_id  label                                               text\n",
      "0         8967      1  In other words, I have seen his program costed...\n",
      "1        27385      1  Our Navy is old -- excuse me, our Navy is smal...\n",
      "2         9818      1  The unemployment, the number of people who are...\n",
      "3        16794      1  Mr. Ford uh - actually has fewer people now in...\n",
      "4        17588      1  Today it is up to about $38,000 of earnings th...\n",
      "        sentence_id         label\n",
      "count  11056.000000  11056.000000\n",
      "mean   16330.066299      0.250000\n",
      "std     9405.549195      0.433032\n",
      "min       16.000000      0.000000\n",
      "25%     8419.750000      0.000000\n",
      "50%    16491.500000      0.000000\n",
      "75%    24134.250000      0.250000\n",
      "max    34457.000000      1.000000\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "here = Path().cwd()\n",
    "cbdata_path = here / \".data_sets\" / \"ClaimBuster_Datasets\" / \"datasets\" # ClaimBuster data location\n",
    "raw_dfs: List[pd.DataFrame] = []\n",
    "\n",
    "for file_path in cbdata_path.iterdir():\n",
    "    if file_path.exists() and file_path.is_file() and file_path.suffix == \".json\":\n",
    "        with open(file_path, 'r') as fileo:\n",
    "            raw_dfs.append(pd.DataFrame(json.load(fileo)))\n",
    "\n",
    "assert len(raw_dfs) > 0\n",
    "\n",
    "for i, j in enumerate(raw_dfs):\n",
    "    assert j is not None\n",
    "    assert type(j) is pd.DataFrame\n",
    "    print(f\"--- part {i+1:02} ---\")\n",
    "    print(j.head())\n",
    "    print(j.describe())\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d59e4630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        sentence_id         label\n",
      "count  29022.000000  29022.000000\n",
      "mean   16281.469161      0.285714\n",
      "std     9401.659478      0.451762\n",
      "min       16.000000      0.000000\n",
      "25%     8384.500000      0.000000\n",
      "50%    16455.500000      0.000000\n",
      "75%    24089.000000      1.000000\n",
      "max    34458.000000      1.000000\n",
      "Dataset Size: 29022\n"
     ]
    }
   ],
   "source": [
    "df = pd.concat(raw_dfs)\n",
    "print(df.describe())\n",
    "print(f\"Dataset Size: {len(df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf2798f",
   "metadata": {},
   "source": [
    "### Additional Data Exploring\n",
    "\n",
    "After building the model and performing some manual testing, the statement, \"Barack Obama was president from 2009 to 2017,\" kept being returned as an opinion when it is actually a verifiable claim.\n",
    "\n",
    "Ended up moving this to its own file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a925905",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    for sent in df[\"text\"]:\n",
    "        if \"obama\" in sent.casefold():\n",
    "            print(sent)\n",
    "\n",
    "if False:\n",
    "    obama_mask = df[\"text\"].str.contains(\"Obama\", case=False, na=False)\n",
    "    obama_df = df.copy()[obama_mask] # Only Obama entries\n",
    "    obama_claims_df = obama_df[obama_df[\"label\"] == 1 ]\n",
    "    obama_opinions_df = obama_df[obama_df[\"label\"] == 0 ]\n",
    "\n",
    "    obama_mentions_count = len(obama_df)\n",
    "    obama_claims_count = len(obama_claims_df)\n",
    "    obama_opinions_count = len(obama_opinions_df)\n",
    "    print(f\"Total Obama mentions: {obama_mentions_count}\")\n",
    "    print(f\"Obama Claims (LABEL_1): {obama_claims_count}\")\n",
    "    print(f\"Obama Opinions (LABEL_0): {obama_opinions_count}\")\n",
    "    print(f\"Obama Claim Percentage: {(obama_claims_count / obama_mentions_count) * 100}%\")\n",
    "\n",
    "    print(\"\\nSample Obama Entries as Claims\")\n",
    "    print(\"---\" * 5 + \" Claims \" + \"---\" * 5)\n",
    "    print(obama_claims_df.head(10))\n",
    "    print(\"---\" * 5 + \" Opinions \" + \"---\" * 5)\n",
    "    print(obama_opinions_df.head(10))\n",
    "    # for i, text in enumerate(obama_claims_df[\"text\"].head(10)):\n",
    "    #     print(f\"{i}.) \\\"{text}\\\"\")\n",
    "    # print(\"\\nSample Obama Entries as Claims\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f2bdf8-3fc2-475c-ac28-8adac2cf89b9",
   "metadata": {},
   "source": [
    "## Data Fixing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4eee1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== CLAIMBUSTERS DATA QUALITY ANALYSIS ===\\n\")\n",
    "\n",
    "# 1. Check label distribution\n",
    "print(\"1. LABEL DISTRIBUTION:\")\n",
    "print(f\"Total samples: {len(df)}\")\n",
    "print(f\"Claims (LABEL_1): {len(df[df['label'] == 1])} ({len(df[df['label'] == 1])/len(df)*100:.1f}%)\")\n",
    "print(f\"Non-claims (LABEL_0): {len(df[df['label'] == 0])} ({len(df[df['label'] == 0])/len(df)*100:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602f61b9",
   "metadata": {},
   "source": [
    "### Balancing Data\n",
    "\n",
    "The current data is unbalanced - caused issues in the first model.\n",
    "\n",
    "Options appear to be \"Downsampling\" or computing weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f50246d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-Claims: 20730\n",
      "Claims: 8292\n",
      "Non-Claims: 20730\n",
      "Non-Claims Down Sampled: 8292\n",
      "Claims: 8292\n"
     ]
    }
   ],
   "source": [
    "# split into majority / minority\n",
    "df_nonclaims_majority = df[df[\"label\"] == 0] # non-claims\n",
    "df_claims_minority = df[df[\"label\"] == 1] # claims\n",
    "\n",
    "print(f\"Non-Claims: {len(df_nonclaims_majority)}\")\n",
    "print(f\"Claims: {len(df_claims_minority)}\")\n",
    "\n",
    "df_nonclaims_downsampled = resample(\n",
    "    df_nonclaims_majority,\n",
    "    replace=False, # sample without replacement\n",
    "    n_samples=len(df_claims_minority),\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Non-Claims: {len(df_nonclaims_majority)}\")\n",
    "print(f\"Non-Claims Down Sampled: {len(df_nonclaims_downsampled)}\")\n",
    "print(f\"Claims: {len(df_claims_minority)}\")\n",
    "\n",
    "# Data Frame Balanced: equal parts claim and non-claim\n",
    "dfb = pd.concat([df_nonclaims_downsampled, df_claims_minority])\n",
    "dfb.describe()\n",
    "\n",
    "unused_df = df_nonclaims_majority.drop(df_nonclaims_downsampled.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3cb6ed-5c27-4318-9b4c-39402fd7b19e",
   "metadata": {},
   "source": [
    "### Split Data\n",
    "\n",
    "Splitting the data into train and validation/test. \n",
    "Also, I think above we sorted the data by label, so [with this strategy](https://stackoverflow.com/questions/29576430/shuffle-dataframe-rows) we can shuffle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "16678117-8b8a-4724-a9b9-97aca917cc66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 13267\n",
      "Validation samples: 3317\n"
     ]
    }
   ],
   "source": [
    "# Shuffle\n",
    "dfb = dfb.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Split into train/validation sets\n",
    "# Data Frame Training\n",
    "# Data Frame Validation\n",
    "dft, dfv = train_test_split(\n",
    "    dfb,\n",
    "    test_size=0.2, \n",
    "    random_state=42,\n",
    "    shuffle=True,\n",
    "    stratify=dfb[\"label\"],\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(dft)}\")\n",
    "print(f\"Validation samples: {len(dfv)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc2e45e-15a3-4daa-ab59-8066867ac160",
   "metadata": {},
   "source": [
    "## Step 3: Load and Setup BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351c3397-3f67-484f-bec4-de0f32417a92",
   "metadata": {},
   "source": [
    "### Initialize Tokenizer and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef607283-a81b-4ac3-ad58-e1c5047eeee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose your BERT variant\n",
    "# TODO: Add in config at top\n",
    "model_name = \"bert-base-uncased\"  # Good starting point\n",
    "# Alternatives: \"roberta-base\", \"distilbert-base-uncased\" (faster)\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# Ran into tokenization issue - All tensors in a batch should be same length\n",
    "# Some were 100 and but one was 187.\n",
    "# Use padding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name, \n",
    "    num_labels=2  # Binary classification: claim vs opinion\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "print(f\"Model loaded: {model_name}\")\n",
    "print(f\"Vocabulary size: {tokenizer.vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443c15ed-27fe-4269-a29a-1d34df6e8117",
   "metadata": {},
   "source": [
    "### Tokenize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ee539200",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples['text'], \n",
    "        truncation=True, \n",
    "        padding=True, \n",
    "        max_length=256  # Adjust based on your text length\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fcb9b832-9c74-4d30-ad9b-fd218b948b9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/13267 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m dsv = Dataset.from_pandas(dfv)\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Apply tokenization\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m train_dataset = \u001b[43mdst\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenize_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatched\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m val_dataset = dsv.map(tokenize_function, batched=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mData tokenized successfully!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/iu-autonomous-fact-checker/.venv/lib/python3.11/site-packages/datasets/arrow_dataset.py:560\u001b[39m, in \u001b[36mtransmit_format.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    553\u001b[39m self_format = {\n\u001b[32m    554\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_type,\n\u001b[32m    555\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mformat_kwargs\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_kwargs,\n\u001b[32m    556\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcolumns\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_columns,\n\u001b[32m    557\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33moutput_all_columns\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._output_all_columns,\n\u001b[32m    558\u001b[39m }\n\u001b[32m    559\u001b[39m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m560\u001b[39m out: Union[\u001b[33m\"\u001b[39m\u001b[33mDataset\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mDatasetDict\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    561\u001b[39m datasets: \u001b[38;5;28mlist\u001b[39m[\u001b[33m\"\u001b[39m\u001b[33mDataset\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mlist\u001b[39m(out.values()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[32m    562\u001b[39m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/iu-autonomous-fact-checker/.venv/lib/python3.11/site-packages/datasets/arrow_dataset.py:3318\u001b[39m, in \u001b[36mDataset.map\u001b[39m\u001b[34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc, try_original_type)\u001b[39m\n\u001b[32m   3316\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3317\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m unprocessed_kwargs \u001b[38;5;129;01min\u001b[39;00m unprocessed_kwargs_per_job:\n\u001b[32m-> \u001b[39m\u001b[32m3318\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mDataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_map_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43munprocessed_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3319\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcheck_if_shard_done\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3321\u001b[39m \u001b[38;5;66;03m# Avoids PermissionError on Windows (the error: https://github.com/huggingface/datasets/actions/runs/4026734820/jobs/6921621805)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/iu-autonomous-fact-checker/.venv/lib/python3.11/site-packages/datasets/arrow_dataset.py:3674\u001b[39m, in \u001b[36mDataset._map_single\u001b[39m\u001b[34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset, try_original_type)\u001b[39m\n\u001b[32m   3672\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3673\u001b[39m     _time = time.time()\n\u001b[32m-> \u001b[39m\u001b[32m3674\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miter_outputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshard_iterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3675\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_examples_in_batch\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3676\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mupdate_data\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/iu-autonomous-fact-checker/.venv/lib/python3.11/site-packages/datasets/arrow_dataset.py:3624\u001b[39m, in \u001b[36mDataset._map_single.<locals>.iter_outputs\u001b[39m\u001b[34m(shard_iterable)\u001b[39m\n\u001b[32m   3622\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3623\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i, example \u001b[38;5;129;01min\u001b[39;00m shard_iterable:\n\u001b[32m-> \u001b[39m\u001b[32m3624\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m i, \u001b[43mapply_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffset\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/iu-autonomous-fact-checker/.venv/lib/python3.11/site-packages/datasets/arrow_dataset.py:3547\u001b[39m, in \u001b[36mDataset._map_single.<locals>.apply_function\u001b[39m\u001b[34m(pa_inputs, indices, offset)\u001b[39m\n\u001b[32m   3545\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Utility to apply the function on a selection of columns.\"\"\"\u001b[39;00m\n\u001b[32m   3546\u001b[39m inputs, fn_args, additional_args, fn_kwargs = prepare_inputs(pa_inputs, indices, offset=offset)\n\u001b[32m-> \u001b[39m\u001b[32m3547\u001b[39m processed_inputs = \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfn_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43madditional_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3548\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m prepare_outputs(pa_inputs, inputs, processed_inputs)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 2\u001b[39m, in \u001b[36mtokenize_function\u001b[39m\u001b[34m(examples)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtokenize_function\u001b[39m(examples):\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer\u001b[49m(\n\u001b[32m      3\u001b[39m         examples[\u001b[33m'\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m'\u001b[39m], \n\u001b[32m      4\u001b[39m         truncation=\u001b[38;5;28;01mTrue\u001b[39;00m, \n\u001b[32m      5\u001b[39m         padding=\u001b[38;5;28;01mTrue\u001b[39;00m, \n\u001b[32m      6\u001b[39m         max_length=\u001b[32m256\u001b[39m  \u001b[38;5;66;03m# Adjust based on your text length\u001b[39;00m\n\u001b[32m      7\u001b[39m     )\n",
      "\u001b[31mNameError\u001b[39m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "# Convert pandas DataFrames to 🤗 Dataset objects\n",
    "dst = Dataset.from_pandas(dft)\n",
    "dsv = Dataset.from_pandas(dfv)\n",
    "\n",
    "# Apply tokenization\n",
    "train_dataset = dst.map(tokenize_function, batched=True)\n",
    "val_dataset = dsv.map(tokenize_function, batched=True)\n",
    "\n",
    "print(\"Data tokenized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c760390-6573-4142-9d3c-4f2d6fba541f",
   "metadata": {},
   "source": [
    "## Step 4: Fine-Tune Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23d54fa-fa26-49da-886b-7953a0313307",
   "metadata": {},
   "source": [
    "We are doing **transfer learning** with **fine-tuning**. \n",
    "BERT was pre-trained to understand language - Thank you!\n",
    "We fine-tuning the model for a specific task - claim vs opinion here.\n",
    "The technique = Supervised learning with backpropagation\n",
    "\n",
    "Deep dive: BERT has millions of weights to understand language. We are adjusting these to suit our classification task. Only our final classification layer is learning from scratch. The rest of BERT is merely adapting instead of being completely retrained. \n",
    "BERT (I think) expects a \"[MASK]\" token to predict values. \n",
    "By fine-tuning, we add a layer like: `input text -> BERT Encoder -> Classification Head -> [Claim, Opinion] probabilities`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f8686b-5379-4460-aa4f-b61f01096350",
   "metadata": {},
   "source": [
    "### Define Training Arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb8524b-2529-4bc4-bf37-2e108cd72272",
   "metadata": {},
   "source": [
    "[transformers.TrainingArguments](https://huggingface.co/docs/transformers/v4.52.3/en/main_classes/trainer#transformers.TrainingArguments) has a lot of parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d27a93b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up directories for saving\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "# TODO: Give model name at top\n",
    "move_path = Path().cwd() / \"trainingresults\" / f'hide-bert_{timestamp}'\n",
    "out_path = Path().cwd() / \"trainingresults\" / \"latest\"\n",
    "metatdata_file_path = out_path / \"metadata.json\"\n",
    "if False:\n",
    "    if metatdata_file_path.exists():\n",
    "        # A model exists in latest already - move to it's timestamp\n",
    "        with open(metatdata_file_path, 'r') as file:\n",
    "            tmp = json.load(file)\n",
    "            ts_path = Path(tmp.path)\n",
    "            out_path.rename(ts_path)\n",
    "        assert not out_path.exists()\n",
    "\n",
    "    with open(out_path / \"metadata.json\", 'w') as file:\n",
    "        json.dump({\"path\": str(out_path), \"foundation\": model_name}, file, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f45727b-67f8-4f14-85a9-0c9e27c51bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=out_path, # Working directory during training for logs and checkpoints.\n",
    "    num_train_epochs=3,              # Start with 3, adjust based on results\n",
    "    per_device_train_batch_size=16,  # Reduce if memory issues\n",
    "    per_device_eval_batch_size=16,\n",
    "    warmup_steps=500, # gradually increase learning rate over 500 steps | prevents huge descrutive changes early on\n",
    "    weight_decay=0.01, # Very mild 1% to prevent memorizing training data exactly. \n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    dataloader_pin_memory=False, # can help with GPU transfer speed\n",
    "    fp16=True, # mixed precision can speedup training if supported\n",
    "    dataloader_num_workers=4, # parallel data loading\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ccafc1-ad8d-4f2e-b5a2-72942d938e2d",
   "metadata": {},
   "source": [
    "### Define Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289bdcd7-1660-4980-b12b-3d8552b3a583",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels, predictions, average='weighted'\n",
    "    )\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585a5fb7-bb35-4f47-bf03-c712c233cf8e",
   "metadata": {},
   "source": [
    "### Initialize and Train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801df9d2-3381-4c84-a1c1-f0369de2bc8f",
   "metadata": {},
   "source": [
    "This is the fun part we all want to do :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04c1ff8-c218-4086-a2d9-aa722c602f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "print(\"Starting training...\")\n",
    "if False:\n",
    "    trainer.train()\n",
    "\n",
    "    # TODO: Update Path - the latest idea and switching...\n",
    "    # Save the model\n",
    "    trainer.save_model(out_path) # Where to save model weights and config\n",
    "    tokenizer.save_pretrained(out_path) # for tokenizer stuff\n",
    "    print(\"Model saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dadf6e7-9380-4abd-9f70-d0621a8e2890",
   "metadata": {},
   "source": [
    "## Step 5: Test Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a071d9c-eeda-430d-ba9c-c6899d498285",
   "metadata": {},
   "source": [
    "### Load Trained Model for Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f66975a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValidationEntry:\n",
    "    def __init__(self, statement: str, expected: int):\n",
    "        self.statement = statement\n",
    "        self.expected = expected\n",
    "    \n",
    "    def __str__(self):\n",
    "        line1 = f\"{self.__class__}\\n\"\n",
    "        line2 = f\"  Statement: {self.statement}\\n\"\n",
    "        line3 = f\"  Expectation: {'Opinion' if 0 else 'Claim'}\\n\"\n",
    "        return line1 + line2 + line3\n",
    "\n",
    "# ADD LATER\n",
    "manual_tests = [\n",
    "    (\"John Smith was elected mayor in 2020\", 1),\n",
    "    (\"The company reported $2 million in revenue\", 1),\n",
    "    (\"She graduated from Harvard University\", 1),\n",
    "    (\"Billy Joe graduated from Harvard University\", 1),\n",
    "    (\"The meeting was scheduled for 3 PM\", 1),\n",
    "    (\"COVID-19 cases increased by 15% last month\", 1),\n",
    "    (\"This is the best restaurant in town\", 0),\n",
    "    (\"We should invest more in education\", 0),\n",
    "    (\"That movie was terrible\", 0),\n",
    "    (\"This policy is unfair to working families\", 0),\n",
    "    (\"Climate change is the most important issue\", 0),\n",
    "    (\"Barack Obama was president from 2009 to 2017\", 1),\n",
    "    (\"Pizza is the most delicious food ever\", 0),\n",
    "    (\"The stock market closed at 4,500 points\", 1),\n",
    "    (\"This movie deserves an Oscar\", 0),\n",
    "    (\"The man Barack Obama served as Senator from Illinois before becoming president.\", 1),\n",
    "    (\"The man John Doe served as Senator from Illinois before becoming president.\", 1),\n",
    "    (\"Barack Obama won the Nobel Peace Prize in 2009\", 1),\n",
    "    (\"George Washington won the Nobel Peace Prize in 2009\", 1),\n",
    "    (\"Ada Lovelace wrote the first computer program way back in the 1840s!\", 1),\n",
    "    (\"The unemployment rate in the artic is close to 0, that's amazing!\", 1),\n",
    "    (\"Donald Trump only serves himself and the top 1%.\", 0),\n",
    "    (\"Donald Trump's Big Beautiful Bill implements the biggest cut to medicaid in American history.\", 1),\n",
    "]\n",
    "\n",
    "validation_items = []\n",
    "\n",
    "for thing in manual_tests:\n",
    "    validation_items.append(ValidationEntry(thing[0], thing[1]))\n",
    "\n",
    "# for item in validation_items:\n",
    "#     print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "225e1608-98ec-4887-ad9b-2c9673b82ae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Testing the model ===\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ksull18/code/iu-autonomous-fact-checker/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 1: FAIL\n",
      "[{'label': 'LABEL_0', 'score': 0.9942793846130371}]\n",
      "Text: 'John Smith was elected mayor in 2020'\n",
      "Prediction: Opinion (confidence: 0.994)\n",
      "Expected: Claim\n",
      "--------------------------------------------------\n",
      "Test 2: PASS\n",
      "[{'label': 'LABEL_1', 'score': 0.9977515339851379}]\n",
      "Text: 'The company reported $2 million in revenue'\n",
      "Prediction: Claim (confidence: 0.998)\n",
      "Expected: Claim\n",
      "--------------------------------------------------\n",
      "Test 3: FAIL\n",
      "[{'label': 'LABEL_0', 'score': 0.9954456090927124}]\n",
      "Text: 'She graduated from Harvard University'\n",
      "Prediction: Opinion (confidence: 0.995)\n",
      "Expected: Claim\n",
      "--------------------------------------------------\n",
      "Test 4: FAIL\n",
      "[{'label': 'LABEL_0', 'score': 0.9969039559364319}]\n",
      "Text: 'Billy Joe graduated from Harvard University'\n",
      "Prediction: Opinion (confidence: 0.997)\n",
      "Expected: Claim\n",
      "--------------------------------------------------\n",
      "Test 5: FAIL\n",
      "[{'label': 'LABEL_0', 'score': 0.9953515529632568}]\n",
      "Text: 'The meeting was scheduled for 3 PM'\n",
      "Prediction: Opinion (confidence: 0.995)\n",
      "Expected: Claim\n",
      "--------------------------------------------------\n",
      "Test 6: PASS\n",
      "[{'label': 'LABEL_1', 'score': 0.9982427358627319}]\n",
      "Text: 'COVID-19 cases increased by 15% last month'\n",
      "Prediction: Claim (confidence: 0.998)\n",
      "Expected: Claim\n",
      "--------------------------------------------------\n",
      "Test 7: PASS\n",
      "[{'label': 'LABEL_0', 'score': 0.9987187385559082}]\n",
      "Text: 'This is the best restaurant in town'\n",
      "Prediction: Opinion (confidence: 0.999)\n",
      "Expected: Opinion\n",
      "--------------------------------------------------\n",
      "Test 8: PASS\n",
      "[{'label': 'LABEL_0', 'score': 0.9989656209945679}]\n",
      "Text: 'We should invest more in education'\n",
      "Prediction: Opinion (confidence: 0.999)\n",
      "Expected: Opinion\n",
      "--------------------------------------------------\n",
      "Test 9: PASS\n",
      "[{'label': 'LABEL_0', 'score': 0.9985939860343933}]\n",
      "Text: 'That movie was terrible'\n",
      "Prediction: Opinion (confidence: 0.999)\n",
      "Expected: Opinion\n",
      "--------------------------------------------------\n",
      "Test 10: PASS\n",
      "[{'label': 'LABEL_0', 'score': 0.9987159967422485}]\n",
      "Text: 'This policy is unfair to working families'\n",
      "Prediction: Opinion (confidence: 0.999)\n",
      "Expected: Opinion\n",
      "--------------------------------------------------\n",
      "Test 11: PASS\n",
      "[{'label': 'LABEL_0', 'score': 0.9988341927528381}]\n",
      "Text: 'Climate change is the most important issue'\n",
      "Prediction: Opinion (confidence: 0.999)\n",
      "Expected: Opinion\n",
      "--------------------------------------------------\n",
      "Test 12: FAIL\n",
      "[{'label': 'LABEL_0', 'score': 0.9953344464302063}]\n",
      "Text: 'Barack Obama was president from 2009 to 2017'\n",
      "Prediction: Opinion (confidence: 0.995)\n",
      "Expected: Claim\n",
      "--------------------------------------------------\n",
      "Test 13: PASS\n",
      "[{'label': 'LABEL_0', 'score': 0.9983810186386108}]\n",
      "Text: 'Pizza is the most delicious food ever'\n",
      "Prediction: Opinion (confidence: 0.998)\n",
      "Expected: Opinion\n",
      "--------------------------------------------------\n",
      "Test 14: PASS\n",
      "[{'label': 'LABEL_1', 'score': 0.997996985912323}]\n",
      "Text: 'The stock market closed at 4,500 points'\n",
      "Prediction: Claim (confidence: 0.998)\n",
      "Expected: Claim\n",
      "--------------------------------------------------\n",
      "Test 15: PASS\n",
      "[{'label': 'LABEL_0', 'score': 0.9985142350196838}]\n",
      "Text: 'This movie deserves an Oscar'\n",
      "Prediction: Opinion (confidence: 0.999)\n",
      "Expected: Opinion\n",
      "--------------------------------------------------\n",
      "Test 16: FAIL\n",
      "[{'label': 'LABEL_0', 'score': 0.9973509311676025}]\n",
      "Text: 'The man Barack Obama served as Senator from Illinois before becoming president.'\n",
      "Prediction: Opinion (confidence: 0.997)\n",
      "Expected: Claim\n",
      "--------------------------------------------------\n",
      "Test 17: FAIL\n",
      "[{'label': 'LABEL_0', 'score': 0.9973539113998413}]\n",
      "Text: 'The man John Doe served as Senator from Illinois before becoming president.'\n",
      "Prediction: Opinion (confidence: 0.997)\n",
      "Expected: Claim\n",
      "--------------------------------------------------\n",
      "Test 18: FAIL\n",
      "[{'label': 'LABEL_0', 'score': 0.9776122570037842}]\n",
      "Text: 'Barack Obama won the Nobel Peace Prize in 2009'\n",
      "Prediction: Opinion (confidence: 0.978)\n",
      "Expected: Claim\n",
      "--------------------------------------------------\n",
      "Test 19: FAIL\n",
      "[{'label': 'LABEL_0', 'score': 0.9627904295921326}]\n",
      "Text: 'George Washington won the Nobel Peace Prize in 2009'\n",
      "Prediction: Opinion (confidence: 0.963)\n",
      "Expected: Claim\n",
      "--------------------------------------------------\n",
      "Test 20: FAIL\n",
      "[{'label': 'LABEL_0', 'score': 0.9884048700332642}]\n",
      "Text: 'Ada Lovelace wrote the first computer program way back in the 1840s!'\n",
      "Prediction: Opinion (confidence: 0.988)\n",
      "Expected: Claim\n",
      "--------------------------------------------------\n",
      "Test 21: PASS\n",
      "[{'label': 'LABEL_1', 'score': 0.9959955215454102}]\n",
      "Text: 'The unemployment rate in the artic is close to 0, that's amazing!'\n",
      "Prediction: Claim (confidence: 0.996)\n",
      "Expected: Claim\n",
      "--------------------------------------------------\n",
      "Test 22: FAIL\n",
      "[{'label': 'LABEL_1', 'score': 0.952358603477478}]\n",
      "Text: 'Donald Trump only serves himself and the top 1%.'\n",
      "Prediction: Claim (confidence: 0.952)\n",
      "Expected: Opinion\n",
      "--------------------------------------------------\n",
      "Test 23: PASS\n",
      "[{'label': 'LABEL_1', 'score': 0.997689962387085}]\n",
      "Text: 'Donald Trump's Big Beautiful Bill implements the biggest cut to medicaid in American history.'\n",
      "Prediction: Claim (confidence: 0.998)\n",
      "Expected: Claim\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "12 Correct\n",
      "11 Wrong\n",
      "23 Total\n",
      "Rate of Success: 52.1739%\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\")\n",
    "# Load your fine-tuned model\n",
    "# TODO: UPDATE!!!\n",
    "classifier = pipeline(\n",
    "    task=\"text-classification\",\n",
    "    model=str(out_path),\n",
    "    tokenizer=str(out_path),\n",
    "    device='cuda'\n",
    ")\n",
    "\n",
    "success_cnt = 0\n",
    "print(\"=== Testing the model ===\")\n",
    "print(\"-\" * 50)\n",
    "for i, item in enumerate(validation_items):\n",
    "    result = classifier(item.statement)\n",
    "    actual = 0 if result[0]['label'] == 'LABEL_0' else 1\n",
    "    success = actual == item.expected\n",
    "    success_label = \"PASS\" if success else \"FAIL\"\n",
    "    if success:\n",
    "        success_cnt += 1\n",
    "    prediction_label = \"Claim\" if result[0]['label'] == 'LABEL_1' else \"Opinion\"\n",
    "    expected_label = \"Claim\" if item.expected == 1 else \"Opinion\"\n",
    "    confidence = result[0]['score']\n",
    "    print(f\"Test {i+1}: {success_label}\")\n",
    "    print(result)\n",
    "    print(f\"Text: '{item.statement}'\")\n",
    "    print(f\"Prediction: {prediction_label} (confidence: {confidence:.3f})\")\n",
    "    print(f\"Expected: {expected_label}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(f\"{success_cnt} Correct\")\n",
    "print(f\"{len(validation_items) - success_cnt} Wrong\")\n",
    "print(f\"{len(validation_items)} Total\")\n",
    "print(f\"Rate of Success: {(success_cnt / len(validation_items))*100:.4f}%\")\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97e1955-2f55-4616-be90-8a728c543d59",
   "metadata": {},
   "source": [
    "### Manual Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c5c8f05c-239c-487e-b10f-58cda5550b3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ksull18/code/iu-autonomous-fact-checker/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.970\n",
      "Precision: 0.970\n",
      "Recall: 0.970\n",
      "F1-score: 0.970\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(texts, true_labels):\n",
    "    \"\"\"Evaluate model on a list of texts with known labels\"\"\"\n",
    "    predictions = []\n",
    "    \n",
    "    for text in texts:\n",
    "        result = classifier(text)\n",
    "        # Convert to binary (0 or 1)\n",
    "        pred = 1 if result[0]['label'] == 'LABEL_1' else 0\n",
    "        predictions.append(pred)\n",
    "    \n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        true_labels, predictions, average='weighted'\n",
    "    )\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy:.3f}\")\n",
    "    print(f\"Precision: {precision:.3f}\")\n",
    "    print(f\"Recall: {recall:.3f}\")\n",
    "    print(f\"F1-score: {f1:.3f}\")\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# Warning of using \"pipeline\" sequentially on GPU - use dataset instead.\n",
    "predictions = evaluate_model(dfv[\"text\"], dfv[\"label\"])\n",
    "# predictions = evaluate_model(val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74403df4-62bc-42e0-9d0e-a1a0617727b7",
   "metadata": {},
   "source": [
    "## Step 6: Integration With Fact-Checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f9eb55-bc44-44fe-865f-3c273e10c4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_claims_from_text(text):\n",
    "    \"\"\"\n",
    "    Extract potential factual claims from text\n",
    "    Returns list of sentences classified as factual claims\n",
    "    \"\"\"\n",
    "    # Simple sentence splitting (you might want to use spaCy for better results)\n",
    "    sentences = text.split('. ')\n",
    "    print(sentences)\n",
    "    \n",
    "    claims = []\n",
    "    for sentence in sentences:\n",
    "        if len(sentence.strip()) > 10:  # Skip very short sentences\n",
    "            print(sentence)\n",
    "            result = classifier(sentence)\n",
    "            print(result)\n",
    "            if result[0]['label'] == 'LABEL_1':  # Factual claim\n",
    "                claims.append({\n",
    "                    'text': sentence,\n",
    "                    'confidence': result[0]['score']\n",
    "                })\n",
    "    \n",
    "    return claims\n",
    "\n",
    "# Test with a Twitter example\n",
    "twitter_text = \"\"\"My opponent Denver Riggleman, running mate of Corey Stewart, was caught on camera campaigning with a white supremacist. Now he has been exposed as a devotee of Bigfoot erotica. This is not what we need on Capitol Hill.\"\"\"\n",
    "\n",
    "claims = extract_claims_from_text(twitter_text)\n",
    "print(f\"Extracted claims: {claims}\")\n",
    "for claim in claims:\n",
    "    print(f\"- {claim['text']} (confidence: {claim['confidence']:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77962dd3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
