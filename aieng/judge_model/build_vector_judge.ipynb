{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83d0b833",
   "metadata": {},
   "source": [
    "Maybe from switching models, but I got a wierd error that the trainer couldn't train.\n",
    "I thought perhaps it was a Torch setting, hence below.\n",
    "But it was to install `sudo apt-get install python3.11-dev`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171d70cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ.update({\n",
    "#     # \"TORCH_COMPILE_DISABLE\": \"1\",\n",
    "#     # \"PYTORCH_DISABLE_TRITON_INFERENCE\": \"1\", \n",
    "#     # \"TRITON_DISABLE_LINE_INFO\": \"1\"\n",
    "#     \"TOKENIZERS_PARALLELISM\": True\n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652b349b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import json\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import re\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    EarlyStoppingCallback,\n",
    "    DataCollatorWithPadding,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    pipeline\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.utils import resample, shuffle\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import List\n",
    "from enum import Enum\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import shutil\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af449963",
   "metadata": {},
   "source": [
    "The following is just a work in progress - but this config will help for building and testing I hope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec004d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FileConfig:\n",
    "    __FullRunContext = \"\"\"denotes using the complete dataset of just a smaller portion of it for testing.\"\"\"\n",
    "    FullRun = True\n",
    "\n",
    "    __MiniRunContext = \"\"\"denotes using a mini dataset for testing. If FullRun is True this is ignored.\"\"\"\n",
    "    MiniRun = True\n",
    "\n",
    "    __ModelVersionContext = \"\"\"To keep things in order, you may set model version here.\"\"\"\n",
    "    ModelVersion = \"v0.1.4\"\n",
    "\n",
    "    ## NOTE: currently not in use for this file\n",
    "    __Percentage = \"\"\"If NOT a full run, what percentage of data do we use? (Think decimal values 0 < pc < 1)\"\"\"\n",
    "    Percentage = 0.1\n",
    "    \n",
    "    __ToBuildContext = \"\"\"Do we want to build another model or run without build for testing purposes.\"\"\"\n",
    "    ToBuild = True\n",
    "\n",
    "    __CustomLossFnContext = \"\"\"For certain cases with class imbalance we need a custom loss function.\"\"\"\n",
    "    CustomLossFn = False\n",
    "    \n",
    "    __ChunkOverlapContext = \"\"\"For the vector store, chunks are 64 words with 8 word overlap.\"\"\"\n",
    "    ChunkOverlap = 8\n",
    "\n",
    "    # Don't know why an option, it's basically required.\n",
    "    UsePadding = True\n",
    "\n",
    "    # BaseModelName = \"bert-base-uncased\" # Context too small\n",
    "    BaseModelName = \"answerdotai/ModernBERT-base\"\n",
    "    # I think ModernBert allows for 512 * 16 = 8192\n",
    "    MaxTokens = 512 * 2\n",
    "    Hardware = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# class Labels(Enum)\n",
    "LabelMap = Enum(\n",
    "    'LabelMap', \n",
    "    [\n",
    "        ('SUPPORTS', 0),\n",
    "        ('REFUTES', 1),\n",
    "        ('NOT ENOUGH INFO', 2),\n",
    "    ]\n",
    ")\n",
    "print(f\"HARDWARE: {FileConfig.Hardware}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2305bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FULL_RUN denotes using all the dataset or a small bit of it for testing the process\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76affa0",
   "metadata": {},
   "source": [
    "Turned out the database was 52GB of Wikipedia articles, but the dataset fit OK in one file so chunking wasn't necessary...\n",
    "Keeping the logic though."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d502a8d",
   "metadata": {},
   "source": [
    "## Loading Data\n",
    "\n",
    "I've vectorized the evidence best I can so it matches the inputs it will receive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218cba25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_processed_fever(output_dir: Path, filestart: str,  max_chunks=None):\n",
    "    \"\"\"\n",
    "    Load processed FEVER data from disk\n",
    "    \n",
    "    Args:\n",
    "        output_dir: Directory containing processed chunks\n",
    "        max_chunks: Maximum number of chunks to load (None for all)\n",
    "    \"\"\"\n",
    "    output_path = Path(output_dir).resolve()\n",
    "    assert output_path.exists()\n",
    "    \n",
    "    # Find all parquet files\n",
    "    parquet_files = sorted(output_path.glob(f\"{filestart}*.parquet\"))\n",
    "    \n",
    "    if max_chunks:\n",
    "        parquet_files = parquet_files[:max_chunks]\n",
    "    \n",
    "    logger.info(f\"Loading {len(parquet_files)} chunks from {output_path}\")\n",
    "    \n",
    "    # Load and combine all chunks\n",
    "    dfs = []\n",
    "    for file in parquet_files:\n",
    "        df = pd.read_parquet(file)\n",
    "        dfs.append(df)\n",
    "        logger.info(f\"Loaded {file.name}: {len(df)} samples\")\n",
    "    \n",
    "    if dfs:\n",
    "        combined_df = pd.concat(dfs, ignore_index=True)\n",
    "        logger.info(f\"Total samples loaded: {len(combined_df)}\")\n",
    "        return Dataset.from_pandas(combined_df)\n",
    "    else:\n",
    "        logger.warning(\"No data files found!\")\n",
    "        return None\n",
    "    \n",
    "# processed_data_home = Path(__file__).resolve().parent / '.datasets' / 'vectorized'\n",
    "processed_data_home = Path('.').resolve() / '.datasets' / 'vectorized'\n",
    "pda = load_processed_fever(processed_data_home, 'fever_train_balanced') ##all I have now\n",
    "pdfa = pd.DataFrame(pda)\n",
    "\n",
    "x = pdfa.drop('label', axis=1)\n",
    "y = pdfa['label']\n",
    "\n",
    "# x_train, x_test, y_train, y_test = train_test_split(\n",
    "#     x, y,\n",
    "#     test_size=0.1,\n",
    "#     stratify=y,\n",
    "#     random_state=42,\n",
    "# )\n",
    "\n",
    "# x_train, x_test, y_train, y_test = train_test_split(\n",
    "#     x, y,\n",
    "#     test_size=0.1,\n",
    "#     stratify=y,\n",
    "#     random_state=42,\n",
    "# )\n",
    "\n",
    "# -----------------------------------\n",
    "# -----------------------------------\n",
    "x_all = pdfa.drop('label', axis=1)\n",
    "y_all = pdfa['label']\n",
    "print(len(x))\n",
    "print(len(y))\n",
    "lth_all = len(x_all)\n",
    "tenpc = int(len(x_all) * 0.1)\n",
    "print(tenpc)\n",
    "x_test, x_tmp, y_test, y_tmp = train_test_split(\n",
    "    x_all, y_all,\n",
    "    test_size=(len(x_all) - tenpc),\n",
    "    stratify=y,\n",
    "    random_state=42,\n",
    ")\n",
    "print(f\"X Tmp: {len(x_tmp)}\")\n",
    "print(f\"X test: {len(x_test)}\")\n",
    "print(f\"Y Tmp: {len(y_tmp)}\")\n",
    "print(f\"y Test: {len(y_test)}\")\n",
    "print(tenpc)\n",
    "\n",
    "x_val, x_train, y_val, y_train = train_test_split(\n",
    "    x_tmp, y_tmp,\n",
    "    test_size=(len(x_tmp) - tenpc),\n",
    "    stratify=y_tmp,\n",
    "    random_state=42,\n",
    ")\n",
    "print(f\"X Train: {len(x_train)}\")\n",
    "print(f\"X Val: {len(x_val)}\")\n",
    "print(f\"Y Tmp: {len(y_train)}\")\n",
    "print(f\"y Test: {len(y_val)}\")\n",
    "# -----------------------------------\n",
    "# -----------------------------------\n",
    "\n",
    "# Train for training\n",
    "pdft = x_train.copy()\n",
    "pdft['label'] = y_train.values\n",
    "\n",
    "# Dev for testing\n",
    "pdfd = x_test.copy()\n",
    "pdfd['label'] = y_test.values\n",
    "\n",
    "# Validation for validation\n",
    "pdfv = x_val.copy()\n",
    "pdfv['label'] = y_val.values\n",
    "\n",
    "\n",
    "# I don't have split data at moment. \n",
    "# pds = load_processed_fever(processed_data_home, 'fever_dev_chunk')\n",
    "# pdfd = pd.DataFrame(pds)\n",
    "pdft.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa25d7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pdft['label'].value_counts())\n",
    "print(len(pdft))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e55629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is not enough data?\n",
    "mask = pdft[\"label\"] == \"NOT ENOUGH INFO\"\n",
    "print(mask)\n",
    "baddf = pdft[mask]\n",
    "\n",
    "# could also go with iloc\n",
    "for index, row in baddf.head(10).iterrows():\n",
    "    print(f\"{index + 1}.) {row['claim']}\")\n",
    "    print(\"Evidence:\")\n",
    "    for t in row['evidence']:\n",
    "        print(f\"  {t}\")\n",
    "    print(\"---\"*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646f2b0b",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "\n",
    "Going to follow pretty close to how I trained the claim extractor / detector model.\n",
    "Going to finetune BERT first I think as the context might not be big enough in DistilBERT for the evidence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb362d6",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "I saved the entire evidence in the 'vector_evidence' so not to lose something and need to recreate datasets... again.\n",
    "However, now we must update our 'evidence' column here... \n",
    "Don't overwrite any files!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf90fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdft.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e7bddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdfd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80bfd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,j in pdfd.iterrows():\n",
    "    print(f\"{i} {j['vector_evidence']}\")\n",
    "    break\n",
    "    # if i > 3:\n",
    "    #     break\n",
    "\n",
    "# each row (j) = Index(['id', 'claim', 'evidence', 'challenge', 'titles', 'pages',\n",
    "# 'vector_evidence', 'evidence_count', 'label'], dtype='object')\n",
    "# -------------\n",
    "# vector_evidence is a list\n",
    "# elements (k) of list (l) are dict_keys(['chunk_id', 'content', 'metadata', 'query_text', 'query_type', 'similarity_score'])\n",
    "# metadata = 1 dict_keys(['chunk_index', 'page', 'source_url', 'title', 'total_chunks'])\n",
    "spacy_nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def combine_chunks_in_data(df: pd.DataFrame) -> List[List[str]]:\n",
    "    store = []\n",
    "    for i, j in df.iterrows():\n",
    "        row_store = []\n",
    "        l = j['vector_evidence']\n",
    "        ev = j['evidence']\n",
    "\n",
    "        if len(l) == 0:\n",
    "            if ev is None or len(ev) == 0:\n",
    "                store.append([])\n",
    "                continue\n",
    "            else:\n",
    "                store.append(ev)\n",
    "                continue\n",
    "\n",
    "        # Group by URL\n",
    "        groups = {}\n",
    "        for k in l:\n",
    "            md = k['metadata']\n",
    "            url = md.get('source_url')\n",
    "            if url is None:\n",
    "                continue\n",
    "\n",
    "            datum = {\n",
    "                'evidence': k.get('content'),\n",
    "                'url': url,\n",
    "                'index': md.get('chunk_index')\n",
    "            }\n",
    "            if url not in groups:\n",
    "                groups[url] = []\n",
    "            groups[url].append(datum)\n",
    "        \n",
    "        # Process each URL group\n",
    "        for url, chunks in groups.items():\n",
    "            chunks.sort(key=lambda x: x['index'])\n",
    "            \n",
    "            if not chunks:\n",
    "                continue\n",
    "            \n",
    "            # Combine consecutive chunks\n",
    "            i = 0\n",
    "            while i < len(chunks):\n",
    "                current_chunk = chunks[i]['evidence']\n",
    "                \n",
    "                # Look ahead for consecutive chunks\n",
    "                j = i + 1\n",
    "                while j < len(chunks) and chunks[j]['index'] == chunks[j-1]['index'] + 1:\n",
    "                    next_content = chunks[j]['evidence']\n",
    "                    words = next_content.split()\n",
    "                    if len(words) > FileConfig.ChunkOverlap:\n",
    "                        next_content_no_overlap = ' '.join(words[FileConfig.ChunkOverlap:])\n",
    "                        current_chunk = f\"{current_chunk} {next_content_no_overlap}\"\n",
    "                    j += 1\n",
    "                \n",
    "                # Add the combined chunk\n",
    "                row_store.append(current_chunk)\n",
    "                \n",
    "                # Move to next non-consecutive chunk\n",
    "                i = j\n",
    "        \n",
    "        store.append(row_store)\n",
    "    \n",
    "    return store\n",
    "\n",
    "pdft['evidence'] = combine_chunks_in_data(pdft)\n",
    "pdfd['evidence'] = combine_chunks_in_data(pdfd)\n",
    "pdft.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971d5dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(pdft.iloc[0]['evidence'], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b55f133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Shuffle\n",
    "for _ in range(3):\n",
    "    pdft = pdft.sample(frac=1, replace=False, ignore_index=True)\n",
    "    pdfd = pdfd.sample(frac=1, replace=False, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc35f72",
   "metadata": {},
   "source": [
    "## Loading and Setup with BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076d61b1",
   "metadata": {},
   "source": [
    "### Initialize Tokenizer and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06423edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading tokenizer for this model\n",
    "tokenizer = AutoTokenizer.from_pretrained(FileConfig.BaseModelName)\n",
    "# dir(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f02269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ran into tokenization issue - All tensors in a batch should be same length\n",
    "# Some were 100 and but one was 187.\n",
    "# Use padding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166ee00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    FileConfig.BaseModelName, \n",
    "    num_labels=3,  # Yay, Nay, Not enough info\n",
    ")\n",
    "model.to(FileConfig.Hardware)\n",
    "\n",
    "print(f\"Model loaded: {FileConfig.BaseModelName}\")\n",
    "print(f\"Vocabulary size: {tokenizer.vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60924fa3",
   "metadata": {},
   "source": [
    "### Tokenize Data\n",
    "\n",
    "Unlike the claim_extractor model, which just had the claim and a label, this model has claims and evidence, and 3 labels. \n",
    "I believe our tokenizer must handle the proper combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727ed53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_transform(claim: str, evidence: List[str]):\n",
    "    \"\"\"\n",
    "    This will be used later when we need to actually use this model.\n",
    "    \"\"\"\n",
    "    if isinstance(evidence, list):\n",
    "        evidence_text = \" \".join(evidence)\n",
    "    else:\n",
    "        evidence_text = str(evidence)\n",
    "    \n",
    "    # Update for BERT Specific\n",
    "    return f\"[CLS] CLAIM: {claim} [SEP] EVIDENCE: {evidence_text} [SEP]\"\n",
    "    # return f\"CLAIM: {claim} EVIDENCE: {evidence_text}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fc7de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find good max token length...\n",
    "def analyze_token_lengths(df, text_column='text', tokenizer_name='answerdotai/ModernBERT-base'):\n",
    "    \"\"\"\n",
    "    Analyze token lengths in your dataset\n",
    "    \n",
    "    Args:\n",
    "        df: pandas DataFrame\n",
    "        text_column: name of column containing text\n",
    "        tokenizer_name: HuggingFace tokenizer to use\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load tokenizer\n",
    "    print(f\"Loading tokenizer: {tokenizer_name}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "    \n",
    "    # Get all texts\n",
    "    texts = df[text_column].tolist()\n",
    "    print(f\"Analyzing {len(texts)} texts...\")\n",
    "    \n",
    "    # Tokenize without truncation to get true lengths\n",
    "    token_lengths = []\n",
    "    \n",
    "    # Process in batches for efficiency\n",
    "    batch_size = 100\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "        \n",
    "        # Tokenize batch\n",
    "        tokenized = tokenizer(\n",
    "            batch_texts, \n",
    "            truncation=False,  # Don't truncate - we want true lengths\n",
    "            padding=False,     # Don't pad\n",
    "            return_tensors=None\n",
    "        )\n",
    "        \n",
    "        # Get lengths\n",
    "        batch_lengths = [len(tokens) for tokens in tokenized['input_ids']]\n",
    "        token_lengths.extend(batch_lengths)\n",
    "        \n",
    "        if (i // batch_size + 1) % 10 == 0:\n",
    "            print(f\"Processed {i + len(batch_texts)} texts...\")\n",
    "    \n",
    "    # Calculate statistics\n",
    "    token_lengths = np.array(token_lengths)\n",
    "    \n",
    "    stats = {\n",
    "        'count': len(token_lengths),\n",
    "        'mean': np.mean(token_lengths),\n",
    "        'median': np.median(token_lengths),\n",
    "        'std': np.std(token_lengths),\n",
    "        'min': np.min(token_lengths),\n",
    "        'max': np.max(token_lengths),\n",
    "        '25th_percentile': np.percentile(token_lengths, 25),\n",
    "        '50th_percentile': np.percentile(token_lengths, 50),\n",
    "        '75th_percentile': np.percentile(token_lengths, 75),\n",
    "        '90th_percentile': np.percentile(token_lengths, 90),\n",
    "        '95th_percentile': np.percentile(token_lengths, 95),\n",
    "        '99th_percentile': np.percentile(token_lengths, 99)\n",
    "    }\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"TOKEN LENGTH STATISTICS\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Total samples: {stats['count']:,}\")\n",
    "    print(f\"Average length: {stats['mean']:.1f} tokens\")\n",
    "    print(f\"Median length: {stats['median']:.1f} tokens\")\n",
    "    print(f\"Standard deviation: {stats['std']:.1f}\")\n",
    "    print(f\"Min length: {stats['min']} tokens\")\n",
    "    print(f\"Max length: {stats['max']:,} tokens\")\n",
    "    print()\n",
    "    print(\"PERCENTILES:\")\n",
    "    print(f\"25th percentile: {stats['25th_percentile']:.1f} tokens\")\n",
    "    print(f\"50th percentile: {stats['50th_percentile']:.1f} tokens\")\n",
    "    print(f\"75th percentile: {stats['75th_percentile']:.1f} tokens\")\n",
    "    print(f\"90th percentile: {stats['90th_percentile']:.1f} tokens\")\n",
    "    print(f\"95th percentile: {stats['95th_percentile']:.1f} tokens\")\n",
    "    print(f\"99th percentile: {stats['99th_percentile']:.1f} tokens\")\n",
    "    \n",
    "    # Recommendations based on percentiles\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"RECOMMENDATIONS:\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    if stats['95th_percentile'] <= 256:\n",
    "        print(\"✅ Recommended max_length: 256 (covers 95% of data)\")\n",
    "    elif stats['95th_percentile'] <= 512:\n",
    "        print(\"✅ Recommended max_length: 512 (covers 95% of data)\")\n",
    "    elif stats['90th_percentile'] <= 512:\n",
    "        print(\"⚠️  Consider max_length: 512 (covers 90% of data)\")\n",
    "        print(f\"   Note: {100-90:.1f}% of samples will be truncated\")\n",
    "    elif stats['95th_percentile'] <= 1024:\n",
    "        print(\"✅ Recommended max_length: 1024 (covers 95% of data)\")\n",
    "    else:\n",
    "        print(f\"📏 Consider max_length: {int(stats['95th_percentile'])} (covers 95% of data)\")\n",
    "        print(\"   Or use 1024 if you want faster training with some truncation\")\n",
    "    \n",
    "    print(f\"\\nData truncated at different max_lengths:\")\n",
    "    for max_len in [256, 512, 768, 1024, 2048]:\n",
    "        pct_truncated = (token_lengths > max_len).mean() * 100\n",
    "        print(f\"  max_length={max_len}: {pct_truncated:.1f}% truncated\")\n",
    "    \n",
    "    # Create histogram\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist(token_lengths, bins=50, alpha=0.7, edgecolor='black')\n",
    "    plt.xlabel('Token Length')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Distribution of Token Lengths')\n",
    "    plt.axvline(stats['mean'], color='red', linestyle='--', label=f'Mean: {stats[\"mean\"]:.1f}')\n",
    "    plt.axvline(stats['95th_percentile'], color='orange', linestyle='--', label=f'95th %ile: {stats[\"95th_percentile\"]:.1f}')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Zoomed in view (up to 95th percentile)\n",
    "    plt.subplot(1, 2, 2)\n",
    "    zoom_limit = min(int(stats['95th_percentile'] * 1.1), stats['max'])\n",
    "    zoom_data = token_lengths[token_lengths <= zoom_limit]\n",
    "    plt.hist(zoom_data, bins=30, alpha=0.7, edgecolor='black')\n",
    "    plt.xlabel('Token Length')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title(f'Distribution (Zoomed: 0-{zoom_limit} tokens)')\n",
    "    plt.axvline(stats['mean'], color='red', linestyle='--', label=f'Mean: {stats[\"mean\"]:.1f}')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return stats, token_lengths\n",
    "\n",
    "pdft.head(2)\n",
    "pdft['input'] = pdft.apply(lambda x: data_transform(x['claim'], x['evidence']), axis=1)\n",
    "pdfd['input'] = pdfd.apply(lambda x: data_transform(x['claim'], x['evidence']), axis=1)\n",
    "\n",
    "statst, token_lengthst = analyze_token_lengths(pdft, text_column='input')\n",
    "statsd, token_lengthsd = analyze_token_lengths(pdfd, text_column='input')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82ec05c",
   "metadata": {},
   "source": [
    "> Perhaps a point of improvement - better input and separation of claims and evidence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ca1c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_fn(dataset):\n",
    "    # Combine claim and evidence into single text input\n",
    "    texts = []\n",
    "    for claim, evidence in zip(dataset['claim'], dataset['evidence']):\n",
    "        combined_text = data_transform(claim, evidence)\n",
    "        texts.append(combined_text)\n",
    "\n",
    "    # Due to issues around tensor length - get actual max length or default to what worked.\n",
    "    max_len = getattr(tokenizer, 'model_max_length', 512)\n",
    "    print('Max Tokens for Model: {max_len}')\n",
    "    max_len = min(FileConfig.MaxTokens, max_len)\n",
    "    print(f\"Token Length picked: {max_len}\")\n",
    "\n",
    "    model_inputs = tokenizer(\n",
    "        texts,\n",
    "        truncation=True,\n",
    "        padding=FileConfig.UsePadding,\n",
    "        # Tensor Size returned is 1021 and doesn't match the 512...\n",
    "        # max_length=(2**10)\n",
    "        max_length=max_len,\n",
    "        # return_tensors=None, # not necessary once correct lenght established\n",
    "    )\n",
    "\n",
    "    label_map = {\"SUPPORTS\": 0, \"REFUTES\": 1, \"NOT ENOUGH INFO\": 2}\n",
    "    if isinstance(dataset['label'], list):\n",
    "        # batched\n",
    "        model_inputs[\"label\"] = [label_map[label] for label in dataset['label']]\n",
    "    else:\n",
    "        model_inputs[\"label\"] = [label_map[dataset['label']]]\n",
    "\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a536ed82",
   "metadata": {},
   "source": [
    "#### Data Debugging\n",
    "\n",
    "The training is not working because it says the labels are a list..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aac0686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the structure of your pandas data before conversion\n",
    "if False:\n",
    "    print(\"Sample rows from pdft:\")\n",
    "    print(pdft[['claim', 'evidence', 'label']].head(2))\n",
    "    print(\"\\nData types:\")\n",
    "    print(pdft[['claim', 'evidence', 'label']].dtypes)\n",
    "    print(\"\\nSample evidence type:\")\n",
    "    print(type(pdft['evidence'].iloc[0]))\n",
    "    print(\"Sample evidence content:\")\n",
    "    print(pdft['evidence'].iloc[0])\n",
    "    print(\"\\nSample label type:\")\n",
    "    print(type(pdft['label'].iloc[0]))\n",
    "    pdft[['claim', 'evidence', 'label']].head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfb3dfb",
   "metadata": {},
   "source": [
    "### Constant...\n",
    "\n",
    "LABELS is for computing class weights later..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7f09d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = pdft['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60cf4f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "min(pdft['label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56790281",
   "metadata": {},
   "source": [
    "[This StackOverflow post](https://stackoverflow.com/questions/29576430/shuffle-dataframe-rows) lists several ways to shuffle a dataframe. \n",
    "Pandas has the builtin `df = df.sample(frac=1).reset_index(drop=True)` which apparently reassigns but does not recreate.\n",
    "SciKit Learn also has a \"shuffle\" method but that might require resetting indexes.\n",
    "There are then seemingly countless ways after that as well...\n",
    "\n",
    "Side note on sample - it now has an 'ignore_index' param to help with that reset issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc081bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert pandas DataFrames to 🤗 Dataset objects\n",
    "if FileConfig.FullRun:\n",
    "    print(\"Full Run!\")\n",
    "    dst = Dataset.from_pandas(pdft[['claim', 'evidence', 'label']])\n",
    "    dsv = Dataset.from_pandas(pdfd[['claim', 'evidence', 'label']])\n",
    "elif FileConfig.MiniRun:\n",
    "    print(\"Mini Run!\")\n",
    "    # mini-datasets for running into issues... 😖\n",
    "    # should probably go in the data processing section\n",
    "    # there's probably an easier way...\n",
    "    training_mini = pd.concat([\n",
    "        pdft[pdft['label'] == 'SUPPORTS'].sample(n=100, replace=False, ignore_index=True),\n",
    "        pdft[pdft['label'] == 'REFUTES'].sample(n=100, replace=False, ignore_index=True),\n",
    "        pdft[pdft['label'] == 'NOT ENOUGH INFO'].sample(n=100, replace=False, ignore_index=True),\n",
    "    ])\n",
    "\n",
    "    val_mini = pd.concat([\n",
    "        pdfd[pdfd['label'] == 'SUPPORTS'].sample(n=20, replace=False, ignore_index=True),\n",
    "        pdfd[pdfd['label'] == 'REFUTES'].sample(n=20, replace=False, ignore_index=True),\n",
    "        pdfd[pdfd['label'] == 'NOT ENOUGH INFO'].sample(n=20, replace=False, ignore_index=True),\n",
    "    ])\n",
    "\n",
    "    ## Shuffle data\n",
    "    for _ in range(5):\n",
    "        training_mini = training_mini.sample(frac=1, replace=False, ignore_index=True)\n",
    "        val_mini = val_mini.sample(frac=1, replace=False, ignore_index=True)\n",
    "\n",
    "    print(training_mini['label'].value_counts())\n",
    "    print(val_mini['label'].value_counts())\n",
    "    dst = Dataset.from_pandas(training_mini[['claim', 'evidence', 'label']])\n",
    "    dsv = Dataset.from_pandas(val_mini[['claim', 'evidence', 'label']])\n",
    "\n",
    "    ## REDO Labels - shouldn't be too different\n",
    "    LABLES = training_mini['label'].values\n",
    "else:\n",
    "    # This is not ready yet\n",
    "    pc = FileConfig.Percentage\n",
    "    print(f\"Partial Run of {pc:.02%}\")\n",
    "\n",
    "\n",
    "    # NEW STRATEGY\n",
    "    numr = min(pdft['label'].value_counts()) * 2\n",
    "    training_mini = pd.concat([\n",
    "        pdft[pdft['label'] == 'SUPPORTS'].sample(n=numr, replace=False, ignore_index=True),\n",
    "        pdft[pdft['label'] == 'REFUTES'].sample(n=numr, replace=False, ignore_index=True),\n",
    "        # allowing replacement here.\n",
    "        pdft[pdft['label'] == 'NOT ENOUGH INFO'].sample(n=numr, replace=True, ignore_index=True),\n",
    "    ])\n",
    "\n",
    "    val_mini = pd.concat([\n",
    "        pdfd[pdfd['label'] == 'SUPPORTS'].sample(frac=1, replace=False, ignore_index=True),\n",
    "        pdfd[pdfd['label'] == 'REFUTES'].sample(frac=1, replace=False, ignore_index=True),\n",
    "        pdfd[pdfd['label'] == 'NOT ENOUGH INFO'].sample(frac=1, replace=False, ignore_index=True),\n",
    "    ])\n",
    "\n",
    "    # Shuffling / Randomizing data\n",
    "    for _ in range(5):\n",
    "        training_mini = training_mini.sample(frac=1, replace=False, ignore_index=True)\n",
    "        val_mini = val_mini.sample(frac=1, replace=False, ignore_index=True)\n",
    "\n",
    "    print(training_mini['label'].value_counts())\n",
    "    print(val_mini['label'].value_counts())\n",
    "    dst = Dataset.from_pandas(training_mini[['claim', 'evidence', 'label']])\n",
    "    dsv = Dataset.from_pandas(val_mini[['claim', 'evidence', 'label']])\n",
    "\n",
    "    ## REDO Labels - shouldn't be too different\n",
    "    LABLES = training_mini['label'].values\n",
    "\n",
    "# Apply tokenization\n",
    "train_dataset = dst.map(\n",
    "    tokenize_fn, \n",
    "    batched=True,\n",
    "    remove_columns=dst.column_names,\n",
    ")\n",
    "val_dataset = dsv.map(\n",
    "    tokenize_fn, \n",
    "    batched=True,\n",
    "    remove_columns=dsv.column_names,\n",
    ")\n",
    "\n",
    "print(\"Data tokenized successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299c9ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b2271f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d77ab62",
   "metadata": {},
   "source": [
    "The below helped me find a mismatch in data-types between the dataset I thought I was making and what was really created...\n",
    "which the training did not like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b38ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the dataset structure after conversion from pandas\n",
    "if False:\n",
    "    print(\"Dataset columns:\", dst.column_names)\n",
    "    print(\"Dataset features:\", dst.features)\n",
    "    print(\"First example:\")\n",
    "    try:\n",
    "        print(dst[0])\n",
    "    except Exception as e:\n",
    "        print(f\"Error accessing first example: {e}\")\n",
    "\n",
    "    # Check if there are any problematic column names or types\n",
    "    for col in dst.column_names:\n",
    "        print(f\"Column '{col}' type: {type(dst[col])}\")\n",
    "        try:\n",
    "            print(f\"First value: {dst[col][0]}\")\n",
    "            print(f\"First value type: {type(dst[col][0])}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error accessing column '{col}': {e}\")\n",
    "        print(\"---\")\n",
    "\n",
    "    print(\"=\"*25)\n",
    "\n",
    "    print(\"Dataset columns:\", train_dataset.column_names)\n",
    "    print(\"Dataset features:\", train_dataset.features)\n",
    "    print(\"First example:\")\n",
    "    try:\n",
    "        print(train_dataset[0])\n",
    "    except Exception as e:\n",
    "        print(f\"Error accessing first example: {e}\")\n",
    "\n",
    "    # Check if there are any problematic column names or types\n",
    "    for col in train_dataset.column_names:\n",
    "        print(f\"Column '{col}' type: {type(train_dataset[col])}\")\n",
    "        try:\n",
    "            print(f\"First value: {train_dataset[col][0]}\")\n",
    "            print(f\"First value type: {type(train_dataset[col][0])}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error accessing column '{col}': {e}\")\n",
    "        print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e48a14",
   "metadata": {},
   "source": [
    "## Fine-Tune the Model\n",
    "\n",
    "We are doing **transfer learning** with **fine-tuning**. \n",
    "BERT was pre-trained to understand language - Thank you!\n",
    "We fine-tuning the model for a specific task - claim vs opinion here.\n",
    "The technique = Supervised learning with backpropagation\n",
    "\n",
    "Deep dive: BERT has millions of weights to understand language. We are adjusting these to suit our classification task. Only our final classification layer is learning from scratch. The rest of BERT is merely adapting instead of being completely retrained. \n",
    "BERT (I think) expects a \"[MASK]\" token to predict values. \n",
    "By fine-tuning, we add a layer like: `input text -> BERT Encoder -> Classification Head -> [Claim, Opinion] probabilities`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bb4d74",
   "metadata": {},
   "source": [
    "### Defining Training Arguments\n",
    "\n",
    "> I am taking most of this from the claim_extractor I made previously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf2ac56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up directories for saving\n",
    "datenow = datetime.now()\n",
    "timestamp = datenow.strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "# TODO: Give model name at top\n",
    "move_path = Path().cwd() / \"trainingresults\" / f'hide-modernbert_{timestamp}'\n",
    "out_path = Path().cwd() / \"trainingresults\" / \"latest\"\n",
    "metatdata_file_path = out_path / \"metadata.json\"\n",
    "\n",
    "# Below is the logic for moving previous versions\n",
    "if FileConfig.ToBuild:\n",
    "    if metatdata_file_path.exists():\n",
    "        # A model exists in latest already - move to it's timestamp\n",
    "\n",
    "        try:\n",
    "            with open(metatdata_file_path, 'r') as file:\n",
    "                tmp = json.load(file)\n",
    "                str_path = tmp.get('path', None)\n",
    "                assert str_path is not None\n",
    "                ts_path = Path(str_path)\n",
    "                # Moving the old model into its timestamp directory\n",
    "                out_path.rename(ts_path)\n",
    "        except Exception as e:\n",
    "            logger.warning(e)\n",
    "            # suggests something in directory didn't finish and should probably be deleted?\n",
    "        assert not out_path.exists()\n",
    "\n",
    "    ## Open cannot make the directories after I rename them...\n",
    "    out_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    with open(out_path / \"metadata.json\", 'w') as file:\n",
    "        json.dump({\"path\": str(move_path), \"foundation\": FileConfig.BaseModelName, 'timestamp': datenow.isoformat(), \"version\": FileConfig.ModelVersion}, file, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e07f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "## OLD\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=out_path, # Working directory during training for logs and checkpoints.\n",
    "#     num_train_epochs=3,              # Start with 3, adjust based on results\n",
    "#     ## batch size of 16 gets to 5.6/6GB of RTX 3060\n",
    "#     per_device_train_batch_size=8,  # Reduce if memory issues\n",
    "#     per_device_eval_batch_size=16,\n",
    "#     warmup_steps=500, # gradually increase learning rate over 1000 steps | prevents huge descrutive changes early on\n",
    "#     weight_decay=0.01, # Very mild 1% to prevent memorizing training data exactly. \n",
    "#     logging_dir='./logs',\n",
    "#     ## had set to 10 which is a lot of overhead\n",
    "#     logging_steps=20,\n",
    "#     eval_strategy=\"epoch\",\n",
    "#     save_strategy=\"epoch\",\n",
    "#     load_best_model_at_end=True,\n",
    "#     metric_for_best_model=\"eval_loss\",\n",
    "#     greater_is_better=False,\n",
    "#     dataloader_pin_memory=False, # can help with GPU transfer speed\n",
    "#     fp16=True, # mixed precision can speedup training if supported\n",
    "#     dataloader_num_workers=4, # parallel data loading\n",
    "# )\n",
    "\n",
    "# New'ish\n",
    "# I increased Epochs and changed the saving and eval strategies to Steps\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=out_path, # Working directory during training for logs and checkpoints.\n",
    "    num_train_epochs=3 if FileConfig.FullRun else 1,              # Start with 3, adjust based on results\n",
    "    ## batch size of 16 gets to 5.6/6GB of RTX 3060\n",
    "    per_device_train_batch_size=4,  # Reduce if memory issues\n",
    "    per_device_eval_batch_size=4,\n",
    "    warmup_steps=1000 if FileConfig.FullRun else 50, # gradually increase learning rate over 1000 steps | prevents huge descrutive changes early on\n",
    "    weight_decay=0.01, # Very mild 1% to prevent memorizing training data exactly. \n",
    "    logging_dir='./logs',\n",
    "    ## had set to 10 which is a lot of overhead\n",
    "    logging_steps=50,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=500 if FileConfig.FullRun else 100,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=500 if FileConfig.FullRun else 100,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\", # Changing for balanced data\n",
    "    greater_is_better=True, # updated\n",
    "    dataloader_pin_memory=False, # can help with GPU transfer speed\n",
    "    fp16=True, # mixed precision can speedup training if supported\n",
    "    dataloader_num_workers=4, # parallel data loading\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e953c8bc",
   "metadata": {},
   "source": [
    "### Define Evaluation Metrics\n",
    "\n",
    "SciKit Learn has some handy prebuilt functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fdec38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels, predictions, average='weighted'\n",
    "    )\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7743b935",
   "metadata": {},
   "source": [
    "### Custome Weights\n",
    "\n",
    "The data is quite unbalanced.\n",
    "There is a total of 48,205 entries:\n",
    "- supports = 31,811\n",
    "- refutes = 14,610\n",
    "- na = 1,784\n",
    "\n",
    "Rebalancing to the tiny amount is not desireable.\n",
    "The `Trainer` can take in a loss function. The signature is just 'Callable' so inside the class it takes in `(outputs, labels, num_items_in_batch)`.\n",
    "\n",
    "There are several loss functions to consider:\n",
    "- \n",
    "\n",
    "\n",
    "[SciKit-Learn's `compute_class_weight`](https://scikit-learn.org/stable/modules/generated/sklearn.utils.class_weight.compute_class_weight.html) function follows the inverse frequency approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba354733",
   "metadata": {},
   "outputs": [],
   "source": [
    "for I,J in enumerate(LabelMap):\n",
    "    print(I)\n",
    "    print(str(J))\n",
    "\n",
    "LabelMap['SUPPORTS'].name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98748903",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_weights(labels):\n",
    "    unique_labels = np.unique(labels)\n",
    "    weights_balanced = compute_class_weight('balanced', classes=unique_labels, y=labels)\n",
    "    weight_tensor = torch.zeros(3)\n",
    "\n",
    "    for index, en in enumerate(LabelMap):\n",
    "        label_inx = np.where(unique_labels == en.name)[0][0]\n",
    "        weight_tensor[index] = weights_balanced[label_inx]\n",
    "    \n",
    "    return weight_tensor\n",
    "\n",
    "class_weights = create_weights(LABELS)\n",
    "class_weights = class_weights.to(model.device)\n",
    "print(class_weights)\n",
    "\n",
    "# This needs work, I don't think weights should change nor be calucated during each batch...\n",
    "def custom_weighted_loss_fn(outputs, labels, num_items_in_batch=None):\n",
    "    logits = outputs.logits # Model's raw predictions [batch_size, 3]\n",
    "\n",
    "    # Method 1: Balanced - Inverse Frequency\n",
    "    loss_fn = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "    # This is impure function for now\n",
    "    loss = loss_fn(logits, labels)\n",
    "    return loss\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e49840",
   "metadata": {},
   "source": [
    "### Initialize and Train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8e4dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_dataset))\n",
    "print(len(val_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225f7db8",
   "metadata": {},
   "source": [
    "Randomly getting error: trying `sudo apt-get install build-essential`\n",
    "NO\n",
    "did `sudo apt-get install python3.11-dev`\n",
    "\n",
    "Seems working now..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59b595b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator if FileConfig.UsePadding else None,\n",
    "    compute_metrics=compute_metrics,\n",
    "    compute_loss_func=custom_weighted_loss_fn if FileConfig.CustomLossFn else None,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)], # if no improvements\n",
    "    optimizers=(None, None), # Letting Trainer handle optimizer\n",
    ")\n",
    "\n",
    "# Start training\n",
    "print(\"Starting training...\")\n",
    "if FileConfig.ToBuild:\n",
    "    trainer.train()\n",
    "    # Save the model\n",
    "    trainer.save_model(out_path) # Where to save model weights and config\n",
    "    tokenizer.save_pretrained(out_path) # for tokenizer stuff\n",
    "    print(\"Model saved!\")\n",
    "    print(\"Cleaning up Checkpoints...\")\n",
    "    checkpoint_dirs = glob.glob(f\"{out_path}/checkpoint-*\")\n",
    "    for checkpoint_dir in checkpoint_dirs:\n",
    "        shutil.rmtree(checkpoint_dir)\n",
    "    print(\"Clean up complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4613dcc8",
   "metadata": {},
   "source": [
    "## Test the Model\n",
    "\n",
    "This is kind of the manual process I suppose for the time being."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ac0fe1",
   "metadata": {},
   "source": [
    "### Loading Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afb15bd",
   "metadata": {},
   "source": [
    "Loading from hugging face consumes too much VRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d988c3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassifier:\n",
    "    def __init__(self, model_path):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        # Ensure token length\n",
    "        self.tokenizer.model_max_length = FileConfig.MaxTokens\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_path,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=FileConfig.Hardware,\n",
    "        )\n",
    "        self.model.eval()\n",
    "    \n",
    "    def __call__(self, text):\n",
    "        # To not track gradients to save memory - don't need back propagation\n",
    "        with torch.no_grad():\n",
    "            inputs = self.tokenizer(\n",
    "                text, \n",
    "                return_tensors=\"pt\", \n",
    "                truncation=True,\n",
    "                # max_length=FileConfig.MaxTokens, \n",
    "                padding=True\n",
    "            )\n",
    "            inputs = {k: v.to(FileConfig.Hardware) for k, v in inputs.items()}\n",
    "            outputs = self.model(**inputs)\n",
    "            \n",
    "            # Format like pipeline output\n",
    "            probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "            pred_id = torch.argmax(probs, dim=-1).item()\n",
    "            \n",
    "            # Same as Hugging Face\n",
    "            return [{\n",
    "                'label': self.model.config.id2label[pred_id],\n",
    "                'score': probs[0][pred_id].item()\n",
    "            }]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e41e3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\")\n",
    "# Load your fine-tuned model\n",
    "# TODO: UPDATE!!!\n",
    "if False:\n",
    "    the_judge = pipeline(\n",
    "        task=\"text-classification\",\n",
    "        model=str(out_path),\n",
    "        tokenizer=str(out_path),\n",
    "        device=FileConfig.Hardware,\n",
    "        # For memory:\n",
    "        torch_dtype=torch.float16,\n",
    "        model_kwargs={\"torch_dtype\": torch.float16},\n",
    "        tokenizer_kwargs={\"max_length\": 1012, \"truncation\": True}  # Force this length\n",
    "    )\n",
    "else:\n",
    "    the_judge = TextClassifier(str(out_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed64475",
   "metadata": {},
   "source": [
    "Making up some claims and support for manual testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547d7674",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_test_data = [\n",
    "    # SUPPORTS - 4 claims\n",
    "    {\n",
    "        \"id\": 1,\n",
    "        \"claim\": \"The 2024 US presidential election had the highest voter turnout in history.\",\n",
    "        \"evidence\": [\n",
    "            \"According to the Federal Election Commission, approximately 158 million Americans voted in the 2024 presidential election. This surpassed the previous record of 155 million voters set in 2020. Election officials reported that turnout reached 66.8% of eligible voters, marking a new milestone in American electoral participation.\",\n",
    "        ],\n",
    "        \"label\": \"SUPPORTS\",\n",
    "        \"context\": \"Short and factual about voter turnout.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 2, \n",
    "        \"claim\": \"Social Security benefits increased by 3.2% in 2024.\",\n",
    "        \"evidence\": [\n",
    "            \"The Social Security Administration announced a 3.2% cost-of-living adjustment for 2024 benefits. This increase affects over 67 million Social Security beneficiaries and 7 million SSI recipients. The adjustment was based on the Consumer Price Index data from the third quarter of 2023.\",\n",
    "        ],\n",
    "        \"label\": \"SUPPORTS\",\n",
    "        \"context\": \"Matching percentages.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 3,\n",
    "        \"claim\": \"The federal minimum wage in America is $7.25 per hour.\",\n",
    "        \"evidence\": [\n",
    "            \"The federal minimum wage has remained at $7.25 per hour since July 2009, when it was last increased under the Fair Minimum Wage Act. While many states have implemented higher minimum wages, the federal rate serves as the baseline for all states. Congressional attempts to raise the federal minimum wage to $15 per hour have stalled in recent legislative sessions.\",\n",
    "        ],\n",
    "        \"label\": \"SUPPORTS\",\n",
    "        \"context\": \"Simple fact about minimum wage.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 4,\n",
    "        \"claim\": \"Medicare covers prescription drug costs for seniors.\",\n",
    "        \"evidence\": [\n",
    "            \"Medicare Part D provides prescription drug coverage for Medicare beneficiaries, covering approximately 63 million seniors and disabled individuals. The program was established in 2006 and helps reduce out-of-pocket prescription costs. Recent legislation has also capped annual prescription drug costs at $2,000 starting in 2025 for Medicare recipients.\",\n",
    "        ],\n",
    "        \"label\": \"SUPPORTS\",\n",
    "        \"context\": \"More evidence for slightly more vague claim.\"\n",
    "    },\n",
    "    # REFUTES - 4 claims  \n",
    "    {\n",
    "        \"id\": 5,\n",
    "        \"claim\": \"The US Constitution requires congressional approval for all military deployments overseas.\",\n",
    "        \"evidence\": [\n",
    "            \"The Constitution grants Congress the power to declare war, but does not require congressional approval for all military actions. The President, as Commander in Chief, has authority to deploy troops for limited periods without congressional authorization. The War Powers Resolution of 1973 requires congressional approval only for deployments lasting more than 60 days.\",\n",
    "        ],\n",
    "        \"label\": \"REFUTES\",\n",
    "        \"context\": \"A constitutional misconception.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 6,\n",
    "        \"claim\": \"Climate change legislation was passed by Congress in 2023 with bipartisan support.\",\n",
    "        \"evidence\": [\n",
    "            \"No major climate change legislation received bipartisan support in Congress during 2023. The Inflation Reduction Act, which included climate provisions, was passed in 2022 with only Democratic votes. Several climate-related bills were introduced in 2023 but failed to advance due to partisan disagreements over implementation and funding mechanisms.\",\n",
    "        ],\n",
    "        \"label\": \"REFUTES\",\n",
    "        \"context\": \"A false bipartisan claim about climate legislation.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 7,\n",
    "        \"claim\": \"The federal deficit decreased by 50% in 2024.\",\n",
    "        \"evidence\": [\n",
    "            \"The Congressional Budget Office reported that the federal deficit increased by approximately 8% in fiscal year 2024, reaching $1.9 trillion. This represents a significant increase from the previous year's deficit of $1.7 trillion. Rising interest payments on national debt and increased government spending contributed to the larger deficit.\",\n",
    "        ],\n",
    "        \"label\": \"REFUTES\",\n",
    "        \"context\": \"A favourite for some, incorrect percentages.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 8,\n",
    "        \"claim\": \"All Supreme Court justices must be confirmed by a two-thirds majority in the Senate.\",\n",
    "        \"evidence\": [\n",
    "            \"Supreme Court nominees require only a simple majority vote for confirmation in the Senate, not a two-thirds majority. This threshold was established by Senate rules and precedent. The confirmation process involves Senate Judiciary Committee hearings followed by a full Senate vote, where 51 votes are sufficient for confirmation.\",\n",
    "        ],\n",
    "        \"label\": \"REFUTES\",\n",
    "        \"context\": \"Incorrect claim about voting threshold.\"\n",
    "    },\n",
    "    # NOT ENOUGH INFO - 2 claims\n",
    "    {\n",
    "        \"id\": 9,\n",
    "        \"claim\": \"The new infrastructure bill will create 500,000 jobs in rural communities specifically.\",\n",
    "        \"evidence\": [\n",
    "            \"The Infrastructure Investment and Jobs Act allocated $1.2 trillion for various infrastructure projects across the United States. The legislation includes funding for roads, bridges, broadband expansion, and water systems. Economic analysts project the bill will create millions of jobs nationwide over the next decade, with significant benefits expected for both urban and rural areas.\",\n",
    "        ],\n",
    "        \"label\": \"NOT ENOUGH INFO\",\n",
    "        \"context\": \"Evidence is about the project but without the supporting figures.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 10,\n",
    "        \"claim\": \"Congressional approval ratings reached their lowest point since 1974 last month.\",\n",
    "        \"evidence\": [\n",
    "            \"Recent polling shows Congress has historically low approval ratings, with multiple surveys indicating public dissatisfaction with legislative performance. Gallup polling has tracked congressional approval since the 1970s, showing significant fluctuations over the decades. Political polarization and gridlock have contributed to declining public confidence in the institution.\",\n",
    "        ],\n",
    "        \"label\": \"NOT ENOUGH INFO\",\n",
    "        \"context\": \"Evidence does not specify exact timeframe nor comparison to 1974.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 11,\n",
    "        \"claim\": \"Trump and Putin are meeting in Alaska to talk about ending the war in Ukraine.\",\n",
    "        \"evidence\": [],\n",
    "        \"label\": \"NOT ENOUGH INFO\",\n",
    "        \"context\": \"Very recent news as of today, withholding evidence even though it does techincally exist.\"\n",
    "    }\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426c3350",
   "metadata": {},
   "source": [
    "Above: I was going to do the multiple sentences but when we pass data into the model I zip them up anyways so... it's half done for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71771094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame and Dataset\n",
    "def create_fake_dataset():\n",
    "    \"\"\"Create fake dataset for testing judge model\"\"\"\n",
    "    df = pd.DataFrame(fake_test_data)\n",
    "    \n",
    "    print(\"Test Dataset Summary:\")\n",
    "    print(f\"Total samples: {len(df)}\")\n",
    "    print(\"\\nLabel Distribution:\")\n",
    "    label_counts = df['label'].value_counts()\n",
    "    for label, count in label_counts.items():\n",
    "        print(f\"{label}: {count}\")\n",
    "    \n",
    "    print(\"\\nClaim Length Statistics:\")\n",
    "    df['claim_length'] = df['claim'].str.len()\n",
    "    print(f\"Average claim length: {df['claim_length'].mean():.0f} characters\")\n",
    "    print(f\"Shortest claim: {df['claim_length'].min()} characters\")\n",
    "    print(f\"Longest claim: {df['claim_length'].max()} characters\")\n",
    "    \n",
    "    return Dataset.from_pandas(df)\n",
    "\n",
    "# Create the dataset\n",
    "test_dataset = create_fake_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50022e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each test_case\n",
    "for tc in test_dataset:\n",
    "    print(f\"TEST CASE: {tc['id']:02}\")\n",
    "    print(f\"Claim: {tc['claim']}\")\n",
    "    print(f\"Evidence: \")\n",
    "    for x in [y for y in tc['evidence']]:\n",
    "        print(f\"  {x}\")\n",
    "    expected = tc['label']\n",
    "    print(f\"Label: {expected}\")\n",
    "    print(f\"ABOUT: {tc['context']}\")\n",
    "    fixed = data_transform(tc['claim'], tc['evidence'])\n",
    "    result_list = the_judge(fixed)\n",
    "    result = result_list[0]\n",
    "    print(\"RESULT:\", json.dumps(result, indent=2))\n",
    "    actual = result.get('label')\n",
    "    if \"0\" in actual:\n",
    "        actual = \"SUPPORTS\"\n",
    "    elif \"1\" in actual:\n",
    "        actual = \"REFUTES\"\n",
    "    elif \"2\" in actual:\n",
    "        actual = \"NOT ENOUGH INFO\"\n",
    "    else:\n",
    "        actual = \"ERROR - WHAT?!?\"\n",
    "    print()\n",
    "    correct = actual == expected\n",
    "    if correct:\n",
    "        print(f\"✅ PREDICTED: {actual} | ACTUAL: {tc['label']}\")\n",
    "    else:\n",
    "        print(f\"❌ PREDICTED: {actual} | ACTUAL: {tc['label']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db0b54b",
   "metadata": {},
   "source": [
    "I know the validation data shouldn't be used, but just checking here for fun..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e6bf55",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_preds = []\n",
    "t_labels = []\n",
    "\n",
    "# I've created pdfv - never seen before.\n",
    "\n",
    "if not FileConfig.FullRun:\n",
    "    min_n = min(pdfd['label'].value_counts())\n",
    "    min_n = 100\n",
    "    pdfv = pd.concat([\n",
    "            pdfv[pdfv['label'] == 'SUPPORTS'].sample(n=min_n, replace=False, ignore_index=True),\n",
    "            pdfv[pdfv['label'] == 'REFUTES'].sample(n=min_n, replace=False, ignore_index=True),\n",
    "            pdfv[pdfv['label'] == 'NOT ENOUGH INFO'].sample(n=min_n, replace=False, ignore_index=True),\n",
    "        ])\n",
    "for _, row in pdfv.iterrows():\n",
    "    fixed = data_transform(row['claim'], row['evidence'])\n",
    "    result_list = the_judge(fixed)\n",
    "    result = result_list[0]\n",
    "    actual = result.get('label')\n",
    "    expected = row['label']\n",
    "    if \"0\" in actual:\n",
    "        actual = \"SUPPORTS\"\n",
    "    elif \"1\" in actual:\n",
    "        actual = \"REFUTES\"\n",
    "    elif \"2\" in actual:\n",
    "        actual = \"NOT ENOUGH INFO\"\n",
    "    else:\n",
    "        actual = \"ERROR - WHAT?!?\"\n",
    "    t_preds.append(actual)\n",
    "    t_labels.append(expected)\n",
    "\n",
    "def compute_metrics_simple(predicted_values: List[str], actual_values: List[str]):\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        actual_values, predicted_values, average='weighted'\n",
    "    )\n",
    "    accuracy = accuracy_score(actual_values, predicted_values)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "test_results = compute_metrics_simple(t_preds, t_labels)\n",
    "print(json.dumps(test_results, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2d494f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pdfv['label'].value_counts())\n",
    "print()\n",
    "print(len(t_preds))\n",
    "print()\n",
    "mycounts = {}\n",
    "for exp, act in zip(t_preds, t_labels):\n",
    "    try:\n",
    "        mycounts[exp][act] += 1\n",
    "    except:\n",
    "        one = mycounts.get(exp)\n",
    "        if one is None:\n",
    "            mycounts[exp] = {}\n",
    "        mycounts[exp][act] = 1\n",
    "\n",
    "print(json.dumps(mycounts, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0240c91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
