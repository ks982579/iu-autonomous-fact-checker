{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652b349b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import json\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import re\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    pipeline\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.utils import resample, shuffle\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import List\n",
    "import os\n",
    "from enum import Enum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af449963",
   "metadata": {},
   "source": [
    "The following is just a work in progress - but this config will help for building and testing I hope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec004d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FileConfig:\n",
    "    __FullRunContext = \"\"\"denotes using the complete dataset of just a smaller portion of it for testing.\"\"\"\n",
    "    FullRun = False\n",
    "\n",
    "    __Percentage = \"\"\"If NOT a full run, what percentage of data do we use? (Think decimal values 0 < pc < 1)\"\"\"\n",
    "    Percentage = 0.1\n",
    "    \n",
    "    __ToBuildContext = \"\"\"Do we want to build another model or run without build for testing purposes.\"\"\"\n",
    "    ToBuild = True\n",
    "\n",
    "    __CustomLossFnContext = \"\"\"For certain cases with class imbalance we need a custom loss function.\"\"\"\n",
    "    CustomLossFn = False\n",
    "\n",
    "# class Labels(Enum)\n",
    "LabelMap = Enum(\n",
    "    'LabelMap', \n",
    "    [\n",
    "        ('SUPPORTS', 0),\n",
    "        ('REFUTES', 1),\n",
    "        ('NOT ENOUGH INFO', 2),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2305bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FULL_RUN denotes using all the dataset or a small bit of it for testing the process\n",
    "FULL_RUN = False\n",
    "MODEL_VERSION = \"v0.1.3\"\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76affa0",
   "metadata": {},
   "source": [
    "Turned out the database was 52GB of Wikipedia articles, but the dataset fit OK in one file so chunking wasn't necessary...\n",
    "Keeping the logic though."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d502a8d",
   "metadata": {},
   "source": [
    "## Loading Data\n",
    "\n",
    "The data had to be processed on my Windows laptop since the wiki DB was over 50GB large and I wasn't transfering to WSL.\n",
    "I was able to create Parquet files which are good at storage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218cba25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_processed_fever(output_dir: Path, filestart: str,  max_chunks=None):\n",
    "    \"\"\"\n",
    "    Load processed FEVER data from disk\n",
    "    \n",
    "    Args:\n",
    "        output_dir: Directory containing processed chunks\n",
    "        max_chunks: Maximum number of chunks to load (None for all)\n",
    "    \"\"\"\n",
    "    output_path = Path(output_dir).resolve()\n",
    "    assert output_path.exists()\n",
    "    \n",
    "    # Find all parquet files\n",
    "    parquet_files = sorted(output_path.glob(f\"{filestart}*.parquet\"))\n",
    "    \n",
    "    if max_chunks:\n",
    "        parquet_files = parquet_files[:max_chunks]\n",
    "    \n",
    "    logger.info(f\"Loading {len(parquet_files)} chunks from {output_path}\")\n",
    "    \n",
    "    # Load and combine all chunks\n",
    "    dfs = []\n",
    "    for file in parquet_files:\n",
    "        df = pd.read_parquet(file)\n",
    "        dfs.append(df)\n",
    "        logger.info(f\"Loaded {file.name}: {len(df)} samples\")\n",
    "    \n",
    "    if dfs:\n",
    "        combined_df = pd.concat(dfs, ignore_index=True)\n",
    "        logger.info(f\"Total samples loaded: {len(combined_df)}\")\n",
    "        return Dataset.from_pandas(combined_df)\n",
    "    else:\n",
    "        logger.warning(\"No data files found!\")\n",
    "        return None\n",
    "    \n",
    "# processed_data_home = Path(__file__).resolve().parent / '.datasets' / 'processed'\n",
    "processed_data_home = Path('.').resolve() / '.datasets' / 'processed'\n",
    "pds = load_processed_fever(processed_data_home, 'fever_train_chunk')\n",
    "pdft = pd.DataFrame(pds)\n",
    "pds = load_processed_fever(processed_data_home, 'fever_dev_chunk')\n",
    "pdfd = pd.DataFrame(pds)\n",
    "pdft.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa25d7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pdft['label'].value_counts())\n",
    "print(len(pdft))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a18532",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pdft['evidence'][0])\n",
    "type(pdft['evidence'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e55629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is not enough data?\n",
    "mask = pdft[\"label\"] == \"NOT ENOUGH INFO\"\n",
    "print(mask)\n",
    "baddf = pdft[mask]\n",
    "\n",
    "# could also go with iloc\n",
    "for index, row in baddf.head(10).iterrows():\n",
    "    print(f\"{index + 1}.) {row['claim']}\")\n",
    "    print(\"Evidence:\")\n",
    "    for t in row['evidence']:\n",
    "        print(f\"  {t}\")\n",
    "    print(\"---\"*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646f2b0b",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "\n",
    "Going to follow pretty close to how I trained the claim extractor / detector model.\n",
    "Going to finetune BERT first I think as the context might not be big enough in DistilBERT for the evidence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb362d6",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "I organized the data while processing the WikiDB and training and dev files.\n",
    "The data is off balance but I will try to use what I have before making further altercations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b55f133",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"DATA IS LOADED IN FROM ABOVE: it is already split into different files as well.\")\n",
    "\n",
    "# Data Shuffle\n",
    "for _ in range(3):\n",
    "    pdft = pdft.sample(frac=1, replace=False, ignore_index=True)\n",
    "    pdfd = pdfd.sample(frac=1, replace=False, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc35f72",
   "metadata": {},
   "source": [
    "## Loading and Setup with BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076d61b1",
   "metadata": {},
   "source": [
    "### Initialize Tokenizer and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fab1e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bert-base-uncased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06423edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading tokenizer for this model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "dir(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f02269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ran into tokenization issue - All tensors in a batch should be same length\n",
    "# Some were 100 and but one was 187.\n",
    "# Use padding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166ee00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name, \n",
    "    num_labels=3,  # Yay, Nay, Not enough info\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "print(f\"Model loaded: {model_name}\")\n",
    "print(f\"Vocabulary size: {tokenizer.vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60924fa3",
   "metadata": {},
   "source": [
    "### Tokenize Data\n",
    "\n",
    "Unlike the claim_extractor model, which just had the claim and a label, this model has claims and evidence, and 3 labels. \n",
    "I believe our tokenizer must handle the proper combination."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82ec05c",
   "metadata": {},
   "source": [
    "> Perhaps a point of improvement - better input and separation of claims and evidence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ca1c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_transform(claim: str, evidence: List[str]):\n",
    "    \"\"\"\n",
    "    This will be used later when we need to actually use this model.\n",
    "    \"\"\"\n",
    "    if isinstance(evidence, list):\n",
    "        evidence_text = \" \".join(evidence)\n",
    "    else:\n",
    "        evidence_text = str(evidence)\n",
    "    \n",
    "    return f\"CLAIM: {claim} EVIDENCE: {evidence_text}\"\n",
    "\n",
    "def tokenize_fn(dataset):\n",
    "    # Combine claim and evidence into single text input\n",
    "    texts = []\n",
    "    for claim, evidence in zip(dataset['claim'], dataset['evidence']):\n",
    "        combined_text = data_transform(claim, evidence)\n",
    "        texts.append(combined_text)\n",
    "\n",
    "    # Due to issues around tensor length - get actual max length or default to what worked.\n",
    "    max_len = getattr(tokenizer, 'model_max_length', 512)\n",
    "\n",
    "    model_inputs = tokenizer(\n",
    "        texts,\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        # Tensor Size returned is 1021 and doesn't match the 512...\n",
    "        # max_length=(2**10)\n",
    "        max_length=512,\n",
    "        # return_tensors=None, # not necessary once correct lenght established\n",
    "    )\n",
    "\n",
    "    label_map = {\"SUPPORTS\": 0, \"REFUTES\": 1, \"NOT ENOUGH INFO\": 2}\n",
    "    if isinstance(dataset['label'], list):\n",
    "        # batched\n",
    "        model_inputs[\"label\"] = [label_map[label] for label in dataset['label']]\n",
    "    else:\n",
    "        model_inputs[\"label\"] = [label_map[dataset['label']]]\n",
    "\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a536ed82",
   "metadata": {},
   "source": [
    "#### Data Debugging\n",
    "\n",
    "The training is not working because it says the labels are a list..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aac0686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the structure of your pandas data before conversion\n",
    "print(\"Sample rows from pdft:\")\n",
    "print(pdft[['claim', 'evidence', 'label']].head(2))\n",
    "print(\"\\nData types:\")\n",
    "print(pdft[['claim', 'evidence', 'label']].dtypes)\n",
    "print(\"\\nSample evidence type:\")\n",
    "print(type(pdft['evidence'].iloc[0]))\n",
    "print(\"Sample evidence content:\")\n",
    "print(pdft['evidence'].iloc[0])\n",
    "print(\"\\nSample label type:\")\n",
    "print(type(pdft['label'].iloc[0]))\n",
    "pdft[['claim', 'evidence', 'label']].head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfb3dfb",
   "metadata": {},
   "source": [
    "### Constant...\n",
    "\n",
    "LABELS is for computing class weights later..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7f09d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = pdft['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60cf4f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "min(pdft['label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56790281",
   "metadata": {},
   "source": [
    "[This StackOverflow post](https://stackoverflow.com/questions/29576430/shuffle-dataframe-rows) lists several ways to shuffle a dataframe. \n",
    "Pandas has the builtin `df = df.sample(frac=1).reset_index(drop=True)` which apparently reassigns but does not recreate.\n",
    "SciKit Learn also has a \"shuffle\" method but that might require resetting indexes.\n",
    "There are then seemingly countless ways after that as well...\n",
    "\n",
    "Side note on sample - it now has an 'ignore_index' param to help with that reset issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc081bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert pandas DataFrames to 🤗 Dataset objects\n",
    "if FileConfig.FullRun:\n",
    "    print(\"Full Run!\")\n",
    "    dst = Dataset.from_pandas(pdft[['claim', 'evidence', 'label']])\n",
    "    dsv = Dataset.from_pandas(pdfd[['claim', 'evidence', 'label']])\n",
    "else:\n",
    "    # mini-datasets for running into issues... 😖\n",
    "    # should probably go in the data processing section\n",
    "    pc = FileConfig.Percentage\n",
    "    print(f\"Partial Run of {pc:.02%}\")\n",
    "\n",
    "    # there's probably an easier way...\n",
    "    # training_mini = pd.concat([\n",
    "    #     pdft[pdft['label'] == 'SUPPORTS'].sample(frac=pc, replace=False, ignore_index=True),\n",
    "    #     pdft[pdft['label'] == 'REFUTES'].sample(frac=pc, replace=False, ignore_index=True),\n",
    "    #     pdft[pdft['label'] == 'NOT ENOUGH INFO'].sample(frac=pc, replace=False, ignore_index=True),\n",
    "    # ])\n",
    "\n",
    "    # val_mini = pd.concat([\n",
    "    #     pdfd[pdfd['label'] == 'SUPPORTS'].sample(frac=pc, replace=False, ignore_index=True),\n",
    "    #     pdfd[pdfd['label'] == 'REFUTES'].sample(frac=pc, replace=False, ignore_index=True),\n",
    "    #     pdfd[pdfd['label'] == 'NOT ENOUGH INFO'].sample(frac=pc, replace=False, ignore_index=True),\n",
    "    # ])\n",
    "\n",
    "    # NEW STRATEGY\n",
    "    numr = min(pdft['label'].value_counts()) * 2\n",
    "    training_mini = pd.concat([\n",
    "        pdft[pdft['label'] == 'SUPPORTS'].sample(n=numr, replace=False, ignore_index=True),\n",
    "        pdft[pdft['label'] == 'REFUTES'].sample(n=numr, replace=False, ignore_index=True),\n",
    "        # allowing replacement here.\n",
    "        pdft[pdft['label'] == 'NOT ENOUGH INFO'].sample(n=numr, replace=True, ignore_index=True),\n",
    "    ])\n",
    "\n",
    "    val_mini = pd.concat([\n",
    "        pdfd[pdfd['label'] == 'SUPPORTS'].sample(frac=1, replace=False, ignore_index=True),\n",
    "        pdfd[pdfd['label'] == 'REFUTES'].sample(frac=1, replace=False, ignore_index=True),\n",
    "        pdfd[pdfd['label'] == 'NOT ENOUGH INFO'].sample(frac=1, replace=False, ignore_index=True),\n",
    "    ])\n",
    "\n",
    "    # Shuffling / Randomizing data\n",
    "    for _ in range(5):\n",
    "        training_mini = training_mini.sample(frac=1, replace=False, ignore_index=True)\n",
    "        val_mini = val_mini.sample(frac=1, replace=False, ignore_index=True)\n",
    "\n",
    "    print(training_mini['label'].value_counts())\n",
    "    print(val_mini['label'].value_counts())\n",
    "    dst = Dataset.from_pandas(training_mini[['claim', 'evidence', 'label']])\n",
    "    dsv = Dataset.from_pandas(val_mini[['claim', 'evidence', 'label']])\n",
    "\n",
    "    ## REDO Labels - shouldn't be too different\n",
    "    LABLES = training_mini['label'].values\n",
    "\n",
    "# Apply tokenization\n",
    "train_dataset = dst.map(\n",
    "    tokenize_fn, \n",
    "    batched=True,\n",
    "    remove_columns=dst.column_names,\n",
    ")\n",
    "val_dataset = dsv.map(\n",
    "    tokenize_fn, \n",
    "    batched=True,\n",
    "    remove_columns=dsv.column_names,\n",
    ")\n",
    "\n",
    "print(\"Data tokenized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d77ab62",
   "metadata": {},
   "source": [
    "The below helped me find a mismatch in data-types between the dataset I thought I was making and what was really created...\n",
    "which the training did not like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b38ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the dataset structure after conversion from pandas\n",
    "print(\"Dataset columns:\", dst.column_names)\n",
    "print(\"Dataset features:\", dst.features)\n",
    "print(\"First example:\")\n",
    "try:\n",
    "    print(dst[0])\n",
    "except Exception as e:\n",
    "    print(f\"Error accessing first example: {e}\")\n",
    "\n",
    "# Check if there are any problematic column names or types\n",
    "for col in dst.column_names:\n",
    "    print(f\"Column '{col}' type: {type(dst[col])}\")\n",
    "    try:\n",
    "        print(f\"First value: {dst[col][0]}\")\n",
    "        print(f\"First value type: {type(dst[col][0])}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error accessing column '{col}': {e}\")\n",
    "    print(\"---\")\n",
    "\n",
    "print(\"=\"*25)\n",
    "\n",
    "print(\"Dataset columns:\", train_dataset.column_names)\n",
    "print(\"Dataset features:\", train_dataset.features)\n",
    "print(\"First example:\")\n",
    "try:\n",
    "    print(train_dataset[0])\n",
    "except Exception as e:\n",
    "    print(f\"Error accessing first example: {e}\")\n",
    "\n",
    "# Check if there are any problematic column names or types\n",
    "for col in train_dataset.column_names:\n",
    "    print(f\"Column '{col}' type: {type(train_dataset[col])}\")\n",
    "    try:\n",
    "        print(f\"First value: {train_dataset[col][0]}\")\n",
    "        print(f\"First value type: {type(train_dataset[col][0])}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error accessing column '{col}': {e}\")\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e48a14",
   "metadata": {},
   "source": [
    "## Fine-Tune the Model\n",
    "\n",
    "We are doing **transfer learning** with **fine-tuning**. \n",
    "BERT was pre-trained to understand language - Thank you!\n",
    "We fine-tuning the model for a specific task - claim vs opinion here.\n",
    "The technique = Supervised learning with backpropagation\n",
    "\n",
    "Deep dive: BERT has millions of weights to understand language. We are adjusting these to suit our classification task. Only our final classification layer is learning from scratch. The rest of BERT is merely adapting instead of being completely retrained. \n",
    "BERT (I think) expects a \"[MASK]\" token to predict values. \n",
    "By fine-tuning, we add a layer like: `input text -> BERT Encoder -> Classification Head -> [Claim, Opinion] probabilities`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bb4d74",
   "metadata": {},
   "source": [
    "### Defining Training Arguments\n",
    "\n",
    "> I am taking most of this from the claim_extractor I made previously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf2ac56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up directories for saving\n",
    "datenow = datetime.now()\n",
    "timestamp = datenow.strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "# TODO: Give model name at top\n",
    "move_path = Path().cwd() / \"trainingresults\" / f'hide-bert_{timestamp}'\n",
    "out_path = Path().cwd() / \"trainingresults\" / \"latest\"\n",
    "metatdata_file_path = out_path / \"metadata.json\"\n",
    "\n",
    "# Below is the logic for moving previous versions\n",
    "if FileConfig.ToBuild:\n",
    "    if metatdata_file_path.exists():\n",
    "        # A model exists in latest already - move to it's timestamp\n",
    "\n",
    "        try:\n",
    "            with open(metatdata_file_path, 'r') as file:\n",
    "                tmp = json.load(file)\n",
    "                str_path = tmp.get('path', None)\n",
    "                assert str_path is not None\n",
    "                ts_path = Path(str_path)\n",
    "                # Moving the old model into its timestamp directory\n",
    "                out_path.rename(ts_path)\n",
    "        except Exception as e:\n",
    "            logger.warning(e)\n",
    "            # suggests something in directory didn't finish and should probably be deleted?\n",
    "        assert not out_path.exists()\n",
    "\n",
    "    ## Open cannot make the directories after I rename them...\n",
    "    out_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    with open(out_path / \"metadata.json\", 'w') as file:\n",
    "        json.dump({\"path\": str(move_path), \"foundation\": model_name, 'timestamp': datenow.isoformat(), \"version\": MODEL_VERSION}, file, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e07f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "## OLD\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=out_path, # Working directory during training for logs and checkpoints.\n",
    "#     num_train_epochs=3,              # Start with 3, adjust based on results\n",
    "#     ## batch size of 16 gets to 5.6/6GB of RTX 3060\n",
    "#     per_device_train_batch_size=8,  # Reduce if memory issues\n",
    "#     per_device_eval_batch_size=16,\n",
    "#     warmup_steps=500, # gradually increase learning rate over 1000 steps | prevents huge descrutive changes early on\n",
    "#     weight_decay=0.01, # Very mild 1% to prevent memorizing training data exactly. \n",
    "#     logging_dir='./logs',\n",
    "#     ## had set to 10 which is a lot of overhead\n",
    "#     logging_steps=20,\n",
    "#     eval_strategy=\"epoch\",\n",
    "#     save_strategy=\"epoch\",\n",
    "#     load_best_model_at_end=True,\n",
    "#     metric_for_best_model=\"eval_loss\",\n",
    "#     greater_is_better=False,\n",
    "#     dataloader_pin_memory=False, # can help with GPU transfer speed\n",
    "#     fp16=True, # mixed precision can speedup training if supported\n",
    "#     dataloader_num_workers=4, # parallel data loading\n",
    "# )\n",
    "\n",
    "# New'ish\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=out_path, # Working directory during training for logs and checkpoints.\n",
    "    num_train_epochs=5,              # Start with 3, adjust based on results\n",
    "    ## batch size of 16 gets to 5.6/6GB of RTX 3060\n",
    "    per_device_train_batch_size=8,  # Reduce if memory issues\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=1000, # gradually increase learning rate over 1000 steps | prevents huge descrutive changes early on\n",
    "    weight_decay=0.01, # Very mild 1% to prevent memorizing training data exactly. \n",
    "    logging_dir='./logs',\n",
    "    ## had set to 10 which is a lot of overhead\n",
    "    logging_steps=50,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_f1_macro\", # Changing for balanced data\n",
    "    greater_is_better=True, # updated\n",
    "    dataloader_pin_memory=False, # can help with GPU transfer speed\n",
    "    fp16=True, # mixed precision can speedup training if supported\n",
    "    dataloader_num_workers=4, # parallel data loading\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e953c8bc",
   "metadata": {},
   "source": [
    "### Define Evaluation Metrics\n",
    "\n",
    "SciKit Learn has some handy prebuilt functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fdec38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels, predictions, average='weighted'\n",
    "    )\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7743b935",
   "metadata": {},
   "source": [
    "### Custome Weights\n",
    "\n",
    "The data is quite unbalanced.\n",
    "There is a total of 48,205 entries:\n",
    "- supports = 31,811\n",
    "- refutes = 14,610\n",
    "- na = 1,784\n",
    "\n",
    "Rebalancing to the tiny amount is not desireable.\n",
    "The `Trainer` can take in a loss function. The signature is just 'Callable' so inside the class it takes in `(outputs, labels, num_items_in_batch)`.\n",
    "\n",
    "There are several loss functions to consider:\n",
    "- \n",
    "\n",
    "\n",
    "[SciKit-Learn's `compute_class_weight`](https://scikit-learn.org/stable/modules/generated/sklearn.utils.class_weight.compute_class_weight.html) function follows the inverse frequency approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba354733",
   "metadata": {},
   "outputs": [],
   "source": [
    "for I,J in enumerate(LabelMap):\n",
    "    print(I)\n",
    "    print(str(J))\n",
    "\n",
    "LabelMap['SUPPORTS'].name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98748903",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_weights(labels):\n",
    "    unique_labels = np.unique(labels)\n",
    "    weights_balanced = compute_class_weight('balanced', classes=unique_labels, y=labels)\n",
    "    weight_tensor = torch.zeros(3)\n",
    "\n",
    "    for index, en in enumerate(LabelMap):\n",
    "        label_inx = np.where(unique_labels == en.name)[0][0]\n",
    "        weight_tensor[index] = weights_balanced[label_inx]\n",
    "    \n",
    "    return weight_tensor\n",
    "\n",
    "class_weights = create_weights(LABELS)\n",
    "class_weights = class_weights.to(model.device)\n",
    "print(class_weights)\n",
    "\n",
    "# This needs work, I don't think weights should change nor be calucated during each batch...\n",
    "def custom_weighted_loss_fn(outputs, labels, num_items_in_batch=None):\n",
    "    logits = outputs.logits # Model's raw predictions [batch_size, 3]\n",
    "\n",
    "    # Method 1: Balanced - Inverse Frequency\n",
    "    loss_fn = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "    # This is impure function for now\n",
    "    loss = loss_fn(logits, labels)\n",
    "    return loss\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e49840",
   "metadata": {},
   "source": [
    "### Initialize and Train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8e4dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_dataset))\n",
    "print(len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59b595b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    compute_loss_func=custom_weighted_loss_fn if FileConfig.CustomLossFn else None,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "print(\"Starting training...\")\n",
    "if FileConfig.ToBuild:\n",
    "    trainer.train()\n",
    "    # Save the model\n",
    "    trainer.save_model(out_path) # Where to save model weights and config\n",
    "    tokenizer.save_pretrained(out_path) # for tokenizer stuff\n",
    "    print(\"Model saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4613dcc8",
   "metadata": {},
   "source": [
    "## Test the Model\n",
    "\n",
    "This is kind of the manual process I suppose for the time being."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ac0fe1",
   "metadata": {},
   "source": [
    "### Loading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e41e3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\")\n",
    "# Load your fine-tuned model\n",
    "# TODO: UPDATE!!!\n",
    "the_judge = pipeline(\n",
    "    task=\"text-classification\",\n",
    "    model=str(out_path),\n",
    "    tokenizer=str(out_path),\n",
    "    device='cuda'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed64475",
   "metadata": {},
   "source": [
    "Making up some claims and support for manual testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547d7674",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_test_data = [\n",
    "    # SUPPORTS - 4 claims\n",
    "    {\n",
    "        \"id\": 1,\n",
    "        \"claim\": \"The 2024 US presidential election had the highest voter turnout in history.\",\n",
    "        \"evidence\": [\n",
    "            \"According to the Federal Election Commission, approximately 158 million Americans voted in the 2024 presidential election. This surpassed the previous record of 155 million voters set in 2020. Election officials reported that turnout reached 66.8% of eligible voters, marking a new milestone in American electoral participation.\",\n",
    "        ],\n",
    "        \"label\": \"SUPPORTS\",\n",
    "        \"context\": \"Short and factual about voter turnout.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 2, \n",
    "        \"claim\": \"Social Security benefits increased by 3.2% in 2024.\",\n",
    "        \"evidence\": [\n",
    "            \"The Social Security Administration announced a 3.2% cost-of-living adjustment for 2024 benefits. This increase affects over 67 million Social Security beneficiaries and 7 million SSI recipients. The adjustment was based on the Consumer Price Index data from the third quarter of 2023.\",\n",
    "        ],\n",
    "        \"label\": \"SUPPORTS\",\n",
    "        \"context\": \"Matching percentages.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 3,\n",
    "        \"claim\": \"The federal minimum wage in America is $7.25 per hour.\",\n",
    "        \"evidence\": [\n",
    "            \"The federal minimum wage has remained at $7.25 per hour since July 2009, when it was last increased under the Fair Minimum Wage Act. While many states have implemented higher minimum wages, the federal rate serves as the baseline for all states. Congressional attempts to raise the federal minimum wage to $15 per hour have stalled in recent legislative sessions.\",\n",
    "        ],\n",
    "        \"label\": \"SUPPORTS\",\n",
    "        \"context\": \"Simple fact about minimum wage.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 4,\n",
    "        \"claim\": \"Medicare covers prescription drug costs for seniors.\",\n",
    "        \"evidence\": [\n",
    "            \"Medicare Part D provides prescription drug coverage for Medicare beneficiaries, covering approximately 63 million seniors and disabled individuals. The program was established in 2006 and helps reduce out-of-pocket prescription costs. Recent legislation has also capped annual prescription drug costs at $2,000 starting in 2025 for Medicare recipients.\",\n",
    "        ],\n",
    "        \"label\": \"SUPPORTS\",\n",
    "        \"context\": \"More evidence for slightly more vague claim.\"\n",
    "    },\n",
    "    # REFUTES - 4 claims  \n",
    "    {\n",
    "        \"id\": 5,\n",
    "        \"claim\": \"The US Constitution requires congressional approval for all military deployments overseas.\",\n",
    "        \"evidence\": [\n",
    "            \"The Constitution grants Congress the power to declare war, but does not require congressional approval for all military actions. The President, as Commander in Chief, has authority to deploy troops for limited periods without congressional authorization. The War Powers Resolution of 1973 requires congressional approval only for deployments lasting more than 60 days.\",\n",
    "        ],\n",
    "        \"label\": \"REFUTES\",\n",
    "        \"context\": \"A constitutional misconception.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 6,\n",
    "        \"claim\": \"Climate change legislation was passed by Congress in 2023 with bipartisan support.\",\n",
    "        \"evidence\": [\n",
    "            \"No major climate change legislation received bipartisan support in Congress during 2023. The Inflation Reduction Act, which included climate provisions, was passed in 2022 with only Democratic votes. Several climate-related bills were introduced in 2023 but failed to advance due to partisan disagreements over implementation and funding mechanisms.\",\n",
    "        ],\n",
    "        \"label\": \"REFUTES\",\n",
    "        \"context\": \"A false bipartisan claim about climate legislation.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 7,\n",
    "        \"claim\": \"The federal deficit decreased by 50% in 2024.\",\n",
    "        \"evidence\": [\n",
    "            \"The Congressional Budget Office reported that the federal deficit increased by approximately 8% in fiscal year 2024, reaching $1.9 trillion. This represents a significant increase from the previous year's deficit of $1.7 trillion. Rising interest payments on national debt and increased government spending contributed to the larger deficit.\",\n",
    "        ],\n",
    "        \"label\": \"REFUTES\",\n",
    "        \"context\": \"A favourite for some, incorrect percentages.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 8,\n",
    "        \"claim\": \"All Supreme Court justices must be confirmed by a two-thirds majority in the Senate.\",\n",
    "        \"evidence\": [\n",
    "            \"Supreme Court nominees require only a simple majority vote for confirmation in the Senate, not a two-thirds majority. This threshold was established by Senate rules and precedent. The confirmation process involves Senate Judiciary Committee hearings followed by a full Senate vote, where 51 votes are sufficient for confirmation.\",\n",
    "        ],\n",
    "        \"label\": \"REFUTES\",\n",
    "        \"context\": \"Incorrect claim about voting threshold.\"\n",
    "    },\n",
    "    # NOT ENOUGH INFO - 2 claims\n",
    "    {\n",
    "        \"id\": 9,\n",
    "        \"claim\": \"The new infrastructure bill will create 500,000 jobs in rural communities specifically.\",\n",
    "        \"evidence\": [\n",
    "            \"The Infrastructure Investment and Jobs Act allocated $1.2 trillion for various infrastructure projects across the United States. The legislation includes funding for roads, bridges, broadband expansion, and water systems. Economic analysts project the bill will create millions of jobs nationwide over the next decade, with significant benefits expected for both urban and rural areas.\",\n",
    "        ],\n",
    "        \"label\": \"NOT ENOUGH INFO\",\n",
    "        \"context\": \"Evidence is about the project but without the supporting figures.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 10,\n",
    "        \"claim\": \"Congressional approval ratings reached their lowest point since 1974 last month.\",\n",
    "        \"evidence\": [\n",
    "            \"Recent polling shows Congress has historically low approval ratings, with multiple surveys indicating public dissatisfaction with legislative performance. Gallup polling has tracked congressional approval since the 1970s, showing significant fluctuations over the decades. Political polarization and gridlock have contributed to declining public confidence in the institution.\",\n",
    "        ],\n",
    "        \"label\": \"NOT ENOUGH INFO\",\n",
    "        \"context\": \"Evidence does not specify exact timeframe nor comparison to 1974.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 11,\n",
    "        \"claim\": \"Trump and Putin are meeting in Alaska to talk about ending the war in Ukraine.\",\n",
    "        \"evidence\": [],\n",
    "        \"label\": \"NOT ENOUGH INFO\",\n",
    "        \"context\": \"Very recent news as of today, withholding evidence even though it does techincally exist.\"\n",
    "    }\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426c3350",
   "metadata": {},
   "source": [
    "Above: I was going to do the multiple sentences but when we pass data into the model I zip them up anyways so... it's half done for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71771094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame and Dataset\n",
    "def create_fake_dataset():\n",
    "    \"\"\"Create fake dataset for testing judge model\"\"\"\n",
    "    df = pd.DataFrame(fake_test_data)\n",
    "    \n",
    "    print(\"Test Dataset Summary:\")\n",
    "    print(f\"Total samples: {len(df)}\")\n",
    "    print(\"\\nLabel Distribution:\")\n",
    "    label_counts = df['label'].value_counts()\n",
    "    for label, count in label_counts.items():\n",
    "        print(f\"{label}: {count}\")\n",
    "    \n",
    "    print(\"\\nClaim Length Statistics:\")\n",
    "    df['claim_length'] = df['claim'].str.len()\n",
    "    print(f\"Average claim length: {df['claim_length'].mean():.0f} characters\")\n",
    "    print(f\"Shortest claim: {df['claim_length'].min()} characters\")\n",
    "    print(f\"Longest claim: {df['claim_length'].max()} characters\")\n",
    "    \n",
    "    return Dataset.from_pandas(df)\n",
    "\n",
    "# Create the dataset\n",
    "test_dataset = create_fake_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50022e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each test_case\n",
    "for tc in test_dataset:\n",
    "    print(f\"TEST CASE: {tc['id']:02}\")\n",
    "    print(f\"Claim: {tc['claim']}\")\n",
    "    print(f\"Evidence: \")\n",
    "    for x in [y for y in tc['evidence']]:\n",
    "        print(f\"  {x}\")\n",
    "    expected = tc['label']\n",
    "    print(f\"Label: {expected}\")\n",
    "    print(f\"ABOUT: {tc['context']}\")\n",
    "    fixed = data_transform(tc['claim'], tc['evidence'])\n",
    "    result_list = the_judge(fixed)\n",
    "    result = result_list[0]\n",
    "    print(\"RESULT:\", json.dumps(result, indent=2))\n",
    "    actual = result.get('label')\n",
    "    if \"0\" in actual:\n",
    "        actual = \"SUPPORTS\"\n",
    "    elif \"1\" in actual:\n",
    "        actual = \"REFUTES\"\n",
    "    elif \"2\" in actual:\n",
    "        actual = \"NOT ENOUGH INFO\"\n",
    "    else:\n",
    "        actual = \"ERROR - WHAT?!?\"\n",
    "    print()\n",
    "    correct = actual == expected\n",
    "    if correct:\n",
    "        print(f\"✅ PREDICTED: {actual} | ACTUAL: {tc['label']}\")\n",
    "    else:\n",
    "        print(f\"❌ PREDICTED: {actual} | ACTUAL: {tc['label']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db0b54b",
   "metadata": {},
   "source": [
    "I know the validation data shouldn't be used, but just checking here for fun..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e6bf55",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "labels = []\n",
    "for _, row in pdfd.iterrows():\n",
    "    fixed = data_transform(row['claim'], row['evidence'])\n",
    "    result_list = the_judge(fixed)\n",
    "    result = result_list[0]\n",
    "    actual = result.get('label')\n",
    "    expected = row['label']\n",
    "    if \"0\" in actual:\n",
    "        actual = \"SUPPORTS\"\n",
    "    elif \"1\" in actual:\n",
    "        actual = \"REFUTES\"\n",
    "    elif \"2\" in actual:\n",
    "        actual = \"NOT ENOUGH INFO\"\n",
    "    else:\n",
    "        actual = \"ERROR - WHAT?!?\"\n",
    "    preds.append(actual)\n",
    "    labels.append(expected)\n",
    "\n",
    "def compute_metrics_simple(predicted_values: List[str], actual_values: List[str]):\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        actual_values, predicted_values, average='weighted'\n",
    "    )\n",
    "    accuracy = accuracy_score(actual_values, predicted_values)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "test_results = compute_metrics_simple(preds, labels)\n",
    "print(json.dumps(test_results, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2d494f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pdfd['label'].value_counts())\n",
    "print(pdfd['label'].value_counts()['NOT ENOUGH INFO'])\n",
    "mycounts = {}\n",
    "print(len(preds))\n",
    "for exp, act in zip(preds, labels):\n",
    "    try:\n",
    "        mycounts[exp][act] += 1\n",
    "    except:\n",
    "        one = mycounts.get(exp)\n",
    "        if one is None:\n",
    "            mycounts[exp] = {}\n",
    "        mycounts[exp][act] = 1\n",
    "\n",
    "print(json.dumps(mycounts, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0240c91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
