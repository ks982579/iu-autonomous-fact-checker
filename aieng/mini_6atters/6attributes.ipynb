{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cd5eed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ksull18/code/iu-autonomous-fact-checker/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# built-in libraries\n",
    "import gc\n",
    "import json\n",
    "from pathlib import Path\n",
    "import re\n",
    "import unicodedata\n",
    "from typing import List\n",
    "\n",
    "# 3rd party libraries\n",
    "import datasets\n",
    "import pandas as pd\n",
    "import requests\n",
    "import torch\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# custom libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b192f825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ksull18/code/iu-autonomous-fact-checker/aieng/mini_6atters/.data_sets/liar_dataset\n"
     ]
    }
   ],
   "source": [
    "data_sets_dir = Path('.').resolve() / '.data_sets' / 'liar_dataset'\n",
    "print(data_sets_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e68f9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = [\n",
    "    'id', 'label', 'statement', 'subject', 'speaker', \n",
    "    'speaker_job_title', 'state', 'party', 'barely_true_counts',\n",
    "    'false_counts', 'half_true_counts', 'mostly_true_counts', 'pants_on_fire_counts', 'context',\n",
    "]\n",
    "training_df = pd.read_csv(data_sets_dir / 'train.tsv', sep='\\t', encoding='utf-8', header=None, names=column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16adc9c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Says the Annies List political group supports third-trimester abortions on demand.\n",
      "When did the decline of coal start? It started when natural gas took off that started to begin in (President George W.) Bushs administration.\n",
      "Hillary Clinton agrees with John McCain \"by voting to give George Bush the benefit of the doubt on Iran.\"\n",
      "Health care reform legislation is likely to mandate free sex change surgeries.\n",
      "The economic turnaround started at the end of my term.\n",
      "The Chicago Bears have had more starting quarterbacks in the last 10 years than the total number of tenured (UW) faculty fired during the last two decades.\n",
      "Jim Dunnam has not lived in the district he represents for years now.\n",
      "I'm the only person on this stage who has worked actively just last year passing, along with Russ Feingold, some of the toughest ethics reform since Watergate.\n",
      "However, it took $19.5 million in Oregon Lottery funds for the Port of Newport to eventually land the new NOAA Marine Operations Center-Pacific.\n",
      "Says GOP primary opponents Glenn Grothman and Joe Leibham cast a compromise vote that cost $788 million in higher electricity costs.\n"
     ]
    }
   ],
   "source": [
    "for statement in training_df['statement'].head(10):\n",
    "    print(statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe7792dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id                         0\n",
      "label                      0\n",
      "statement                  0\n",
      "subject                    2\n",
      "speaker                    2\n",
      "speaker_job_title       2898\n",
      "state                   2210\n",
      "party                      2\n",
      "barely_true_counts         2\n",
      "false_counts               2\n",
      "half_true_counts           2\n",
      "mostly_true_counts         2\n",
      "pants_on_fire_counts       2\n",
      "context                  102\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for missing data\n",
    "print(training_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23de6b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "half-true      2114\n",
      "false          1995\n",
      "mostly-true    1962\n",
      "true           1676\n",
      "barely-true    1654\n",
      "pants-fire      839\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# distribtuion of labels\n",
    "print(training_df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920964b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/datasets/roupenminassian/twitter-misinformation?library=datasets\n",
    "\n",
    "# twitter_ds = datasets.load_dataset(\"roupenminassian/twitter-misinformation\")\n",
    "# train_df = twitter_ds['train'].to_pandas()\n",
    "# test_df = twitter_ds['test'].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9ea68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df.to_csv('.data_sets/twitter_misinformation/train.csv', index=False)\n",
    "# test_df.to_csv('.data_sets/twitter_misinformation/test.csv', index=False)\n",
    "# train_df.to_json('.data_sets/twitter_misinformation/train.json', orient='records')\n",
    "# test_df.to_json('.data_sets/twitter_misinformation/test.json', orient='records')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af060e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dfs: List[pd.DataFrame] = []\n",
    "with open(Path('./.data_sets/twitter_misinformation/train.json'), 'r') as file:\n",
    "    raw_dfs.append(pd.DataFrame(json.load(file)))\n",
    "\n",
    "with open(Path('./.data_sets/twitter_misinformation/test.json'), 'r') as file:\n",
    "    raw_dfs.append(pd.DataFrame(json.load(file)))\n",
    "\n",
    "df = pd.concat(raw_dfs)\n",
    "\n",
    "# tweet_train_df = pd.read_csv('./.data_sets/twitter_misinformation/train.csv')\n",
    "# tweet_test_df = pd.read_csv('./.data_sets/twitter_misinformation/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "515c40f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local Charlotte, NC news station WSOCTV is reporting that sources tell them dash cameras captured Keith Scott getting out of car and coming towards officers with a gun in his hand:#BREAKING: Sources tell Channel 9 dash camera video shows #KeithScott getting out car, coming toward officers with gun in his hand pic.twitter.com/GGuM2Ow3wk  WSOCTV (@wsoctv) September 21, 2016For a second night, protests over a deadly officer-involved shooting in Charlotte, North Carolina, turned violent, with police firing tear gas and demonstrators throwing objects and trying to damage vehicles.Keith Lamont Scott, a father of seven, was killed by police in an apartment complex parking lot Tuesday as officers looked for another man named in a warrant they were trying to serve. The shooting set off a long night of violent protests and Wednesday the demonstrations continued for a second night, starting off as a peaceful march through downtown Charlotte. But when the demonstrators neared an Omni Hotel, some people climbed the roof of an outdoor mall and started throwing objects at the crowd.Other people were banging on the doors of the hotel, and police in riot gear emerged in the entrance.His family said Scott, an African-American, was unarmed and sitting in his car reading a book, waiting for his son to come home from school.But Charlotte-Mecklenburg Police Chief Kerr Putney said Scott exited his car with a gun, not a book. He said officers couldn t find a book at the scene. It s time for the voiceless majority to stand up and be heard,  said the police chief, who is black. It s time to change the narrative because I can tell you from the facts that the story s a little bit different as to how it s been portrayed so far, especially through social media.   CNNBut b b but what about the book?Neighbor who says she saw police shoot #KeithScott disputes CMDP's claim that he was armed with a gun. She says he had a book. @wsoctv pic.twitter.com/X1PWw5QJ7O  Mark Barber (@MBarberWSOC9) September 21, 2016And of course, there s Hillary s irresponsible tweet confirming that the cops were in the wrong, without even having any facts to back up her ignorant statement:Keith Lamont Scott. Terence Crutcher. Too many others. This has got to end. -H  Hillary Clinton (@HillaryClinton) September 21, 2016 \n",
      "The tsunami has started President Obama s Kenyan half-brother wants to make America great again   so he s voting for Donald Trump. I like Donald Trump because he speaks from the heart,  Malik Obama told The Post from his home in the rural village of Kogelo.  Make America Great Again is a great slogan. I would like to meet him. Obama, 58, a longtime Democrat, said his  deep disappointment  in his brother Barack s administration has led him to recently switch allegiance to  the party of Lincoln. The last straw, he said, came earlier this month when FBI Director James Comey recommended not prosecuting Democratic presidential candidate Hillary Clinton over her use of a private e-mail servers while secretary of state. She should have known better as the custodian of classified information,  said Obama.He s also annoyed that Clinton and President Obama killed Libyan leader Moammar Khadafy, whom he called one of his best friends.Malik Obama dedicated his 2012 biography of his late father to Khadafy and others who were  making this world a better place. I still feel that getting rid of Khadafy didn t make things any better in Libya,  he said.  My brother and the secretary of state disappointed me in that regard. But what bothers him even more is the Democratic Party s support of same-sex marriage.Obama plans to trek back to the US to vote for Trump in November. Obama used to live in Maryland, where he worked for many years as an accountant and is registered to vote there, public records show. Mr. Trump is providing something new and something fresh,  he said.For entire story: NYP\n",
      "The only reality show Donald Trump should have ever been featured in is The Biggest Loser because he just got his ass handed to him in court.Two years ago, Trump National Doral Miami golf resort signed a contract worth $200,000 for a local business called The Paint Spot to provide paint used to renovate the golf course.Well, guess who tried to stiff The Paint Spot of the final $34,863 payment in the deal?Yeah, that would be Republican nominee Donald J. Trump.Trump and his company refused to honor the contract by not paying the final payment, saying that they ve  paid enough  for the paint. In other words, Trump negotiated a deal that ended up costing him more in the end, just like the kinds of deals he wants to negotiate for America with the rest of the world.Anyway, Paint Spot owner Juan Carlos Enriquez filed suit against Trump in court, and Judge Jorge Cueto just slapped Trump and his company with a $300,000 hit to cover The Paint Shop s attorney and court fees, nearly ten times more money than the $34,863 owed. And Trump STILL hasn t paid that debt, so how are we supposed to trust him to pay down the debt of en entire nation if he can t meet his own obligations? I m happy I have a judgment,  Enriquez told the Miami Herald.  But he [Trump] hasn t paid yet. You know how he says he ll surround himself with the greatest people if he is president? In this case, he might not be surrounded by the right people. This isn t even the first fine Trump has been ordered to pay this month.On the same day as his coronation as the Republican Party nominee last week, the National Labor Relations Board slapped Trump with an $11,200 fine for treating employees like shit because they tried to join a labor union.As it turns out, one employee was wrongfully fired and the other was retaliated against by denying them the promotion they had earned.For someone who claims to be the  law and order  candidate, Trump sure does break the law a lot.Featured Image: Christopher Furlong/Getty Images\n",
      "No Food, No FEMA: Hurricane Michael’s Survivors Are Furious https://thebea.st/2CIXNoz \n",
      "WASHINGTON (Reuters) - Here are some of the highlights of the Reuters interview with U.S. President Donald Trump on Thursday. “There’s a chance that we could end up having a major, major, conflict with North Korea, absolutely.” QUESTION: Is that your biggest global worry at this point? “Yes, I would say that’s true, yes. ... North Korea would be certainly that.” ON GETTING SOUTH KOREA TO PAY FOR THAAD MISSILE DEFENSE SYSTEM “On the THAAD system, it’s about a billion dollars. I said,  ‘Why are we paying? Why are we paying a billion dollars? We’re protecting. Why are we paying a billion dollars?’ So I informed South Korea it would be appropriate if they paid. Nobody’s going to do that. Why are we paying a billion dollars? It’s a billion dollar system. It’s phenomenal. It’s the most incredible equipment you’ve ever seen - shoots missiles right out of the sky. And it protects them and I want to protect them. We’re going to protect them. But they should pay for that, and they understand that.” ON WHETHER THE WAR AGAINST ISLAMIST EXTREMISM WILL EVER END “Yours is the toughest question. Because at what point does it end? But we can’t let them come over here. I have to say, there is an end. And it has to be humiliation. There is an end. Otherwise it’s really tough. But there is an end. We are really eradicating some very bad people. When you take a look at what’s going on with the cutting off of the heads. We haven’t seen that since Medieval times. Right?”  ON CHINESE PRESIDENT XI’S EFFORTS TO REIN IN NORTH KOREA “He certainly doesn’t want to see turmoil and death. He doesn’t want to see it. He’s a good man. He’s a very good man and I got to know him very well ... We’ll see how it all works out. I know he would like to be able to do something. Perhaps it’s possible that he can’t. But I think he’d like to be able to do something.” “He’s 27 years old, his father dies, took over a regime, so say what you want but that’s not easy, especially at that age. You know you have plenty of generals in there and plenty of other people that would like to do what he’s doing. So I’ve said this before and I’ve, I’m just telling you, and I’m not giving him credit or not giving him credit. I’m just saying that’s a very hard thing to do.” “As to whether or not he’s rational, I have no opinion on it. I hope he’s rational.” “I get a call from Mexico yesterday, ‘We hear you’re going to terminate NAFTA.’ I said that’s right. They said, ‘Is there any way we can do something without you – without termination?’ I said, ‘What do you want to do?’ He said, ‘Well, we’d like to negotiate.’ I said we’ll think about it. Then I get a call, and they call me, I get a call from Justin Trudeau and he said, ‘We’d like to see if we can work something out,’ and I said that’s fine. Because I’ve always - I’ve been very consistent. It’s much less disruptive if we can make a fair trade deal than if we terminate.” “It’s unacceptable. It’s a horrible deal made by Hillary. It’s a horrible deal. And we’re going to renegotiate that deal, or terminate it.”  QUESTION: When will you announce it? “Very soon. I’m announcing it now.” “By the way, with South Korea, just so you know. They’re ready for it. Mike Pence was representing me, he was just over there, he’s told them. And we have the five-year anniversary coming up very shortly. And we thought that would be a good time to start ... It’s a great deal for South Korea. It’s a terrible deal for us.” “Frankly, Saudi Arabia has not treated us fairly, because we are losing a tremendous amount of money in defending Saudi Arabia.” “Well, my problem is that I’ve established a very good personal relationship with (Chinese) President Xi. And I really feel that he is doing everything in his power to help us with a big situation, so I wouldn’t want to be causing difficulty right now for him ... So I would certainly want to speak to him first.” “If there’s closure, there’s closure. We’ll see what happens. If there’s a shutdown. It’s the Democrats’ fault. Not our fault. It’s the Democrats’ fault. Maybe they’d like to see a shutdown.”  ON TRUMP’S PLAN TO GENERATE REVENUE TO OFFSET TAX CUTS “We will do trade deals that are going to make up for a tremendous amount of the deficit. We are going to be doing trade deals that are going to be much better trade deals ...  “There will be other ways that we are going to raise revenues. But we are going to run the country properly, and we are going to be reimbursed when we do things. Why should we be paying for somebody else’s military?” ON MIDDLE EAST PEACE AND POSSIBLE TRIP TO ISRAEL, SAUDI ARABIA “It’s a possibility, we’re talking to both. It’s a possibility, but I want to see peace with Israel and the Palestinians. There is no reason there’s not peace between Israel and the Palestinians - none whatsoever. So we’re looking at that and we’re also looking at the potential of going to Saudi Arabia.” \n",
      "House Speaker Paul Ryan finally admitted the reason he is refusing to hold town hall meetings: He s too afraid of protesters. Speaking at a Boy Scouts event in Wisconsin, Ryan told the crowd that he just doesn t have the guts to face his own constituents when he knows they are p*ssed that he s trying to steal their health care. Aside from the obvious security concerns, what we have found is there are people who are trying to come in from out of the district to disrupt town hall meetings and not have a civil discussion, so what I have been doing is looking for new and creative ways to interact with my constituents in a civil way,  Ryan said. That s why I have done a number of telephone town hall meetings, which I find very effective as people don t have to travel. I do office hours. I just did them this morning in Janesville. In addition, I am doing a lot of business ones,  the Speaker said.Ryan said that he has found other ways to meet with his constituents, like the town-hall style meeting he held on Thursday that was closed to the public with only 25 employees of the business he was touring in attendance. I find when you guys are there, people kind of clam up. They get a little nervous, but when you do business town halls without media it is very interactive, so I am finding a lot of different ways to have a good civil dialogue with constituents,  he continued.Ryan then said that if people want to talk to him, that s what his office hours are for, which he explained help keep him safe from all those big, mean protesters. Additionally, if you want to come do office hours, they schedule office hours because I don t want to have a situation where we just have a screaming fest, a shouting fest where people are being bussed in from out of the district to get on TV because they are yelling at somebody. That does nobody any good, and what I want to do is have a civil, good conversation with constituents, and that s why I do all these different things, whether it s planned tours, telephone town halls, office hours and the rest,  he said.So, that s Ryan s master plan to deal with the ever increasing public anger over his plan to take health care from millions. He s just going to hide like the spineless coward that he is. Sounds legit.Featured image via Win McNamee/Getty Images\n",
      "Donald Trump just can t accept that he has completely demolished any and all chances of winning over the Hispanic vote.Refusing to be discouraged by the fact that he s offended the Hispanic community since day one of his disgraceful presidential campaign and continuously pummeled the minority group throughout the election cycle, the presumptive Republican nominee is trying to reach out to the very community that he said was made up of  rapists  and  drug dealers. Trump gave his  first direct message to Hispanic conservatives  over the phone, while sitting alone on a private plane. It s truly bizarre and uncomfortable to watch, but then again so is everything else The Donald does. During his 2-minute message, which was aired at the National Hispanic Christian Leadership Conference in California, Trump promised to do away with minority unemployment and stop illegal drug cartels. While Trump rattled off a few more talking points, the business mogul s main message to Hispanics could be summed up in these few sentences: National. Hispanic. Christian. Three great words. We re going to take care of you. We re going to work with you. You re going to be very happy. You re gonna like President Trump. You can watch Trump s awkward message below:The idea of Trump sincerely trying to portray himself as a great candidate for Hispanic voter is, quite frankly, hilarious. As much as Trump loves his polls, he must be absolutely oblivious or in severe denial   it s been widely reported that Trump is still struggling big time to get support from the Hispanic community, and that s probably not going to change anytime soon.Many Hispanic voters have pledged their support to Trump s rival, Democratic front runner Hillary Clinton, with only 23% of them stating that they would vote for Trump in November. In fact, many Hispanics registered for the first time this year for the sole purpose of voting AGAINST Trump. No matter how hard Trump tries to undo the damage he did in the beginning of his campaign, his attempts will likely fall short. The Hispanic community will not forget his offensive, hateful rhetoric, and neither should the rest of us.Featured image via Scott Olson / Getty Images\n",
      "Would-be looter in Hurricane Michael-ravaged Florida shot, killed after trying to steal law enforcement vehicle: report\n",
      "\n",
      "https://www.foxnews.com/us/would-be-looter-in-hurricane-michael-ravaged-florida-shot-killed-after-trying-to-steal-law-enforcement-vehicle-report ...\n",
      "... All looters need to be handled the same way!\n",
      "His argument is correct. Hurricane Rita killed over 100 people on the road, drowned them in their own cars. pic.twitter.com/kulwIZW2EI\n",
      "At around the 4:00 mark in the video, Maxine points out that Obama will become more aggressive in his second term. Waters suggests Obama s ability to push immigration and some of the other radical issues on his agenda during first term were thwarted by the opposition, or more specifically by the  Right  and by the  Tea Party . Martin then tells Waters that Obama better get busy and get done what needs to be done during his second term. Roland tells Waters, The inauguration represented the beginning of the second term, but it also represented the countdown of the end of his presidency. And the reality is, uh, like anything else, you better get what you can while he s there, because look come 2016, that s it.  Waters responds,  Well you know, I don t know. And I think some people are missing something here. The President has put in place an organization that contains a kind of database that no one has ever seen before in life. That s going to be very, very powerful.   Martin asks,  In terms of the  Organizing for America  that he s now shifting to become a 501C4 ?  Waters responds,  That s right. That s right. And that database will have information about everything on every individual in ways that it s never been done before. Here is the shortened clip:Here is the full interview. The bombshell revelation by Congresswoman Maxine Waters (D-CA) starts at around the 4:00 mark:\n"
     ]
    }
   ],
   "source": [
    "df['text'] = df['text'].apply(lambda x: unicodedata.normalize('NFKD', x))\n",
    "df.to_json('./.data_sets/twitter_misinformation/cleaned.json', orient='records')\n",
    "\n",
    "\n",
    "\n",
    "for tweet in df['text'].head(10):\n",
    "    print(tweet)\n",
    "    if r\"\\u\" in tweet:\n",
    "        print(\"Yes\")\n",
    "    # print(\"---\"*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc5ca64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following will require ollama to be running as well - or a model of your choosing.\n",
    "\n",
    "# Did not work...\n",
    "def extract_5w1h(tweet_text):\n",
    "    prompt = f\"\"\"\n",
    "    Extract the 5W1H elements from this tweet. Return as JSON:\n",
    "    \n",
    "    Tweet: \"{tweet_text}\"\n",
    "    \n",
    "    Extract:\n",
    "    - WHO: The main person/entity being discussed\n",
    "    - WHAT: The main action or claim\n",
    "    - WHERE: Location (if mentioned)  \n",
    "    - WHEN: Time reference (if mentioned)\n",
    "    - WHY: Reason or motivation (if mentioned)\n",
    "    - HOW: Method or manner (if mentioned)\n",
    "    \n",
    "    Return only valid JSON. Use null for missing elements.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Call Ollama API\n",
    "    # response = requests.post('http://172.17.0.1:11434/api/generate',\n",
    "    # response = requests.post('http://172.26.112.1:11434/api/generate',\n",
    "    response = requests.post('http://localhost:11434/api/generate',\n",
    "                           json={'model': 'deepseek-llm:7b', 'prompt': prompt, \"stream\": False})\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e37bcd",
   "metadata": {},
   "source": [
    "https://superuser.com/questions/1679757/accessing-windows-localhost-from-wsl2\n",
    "WSL2 is like a VM in Windows. It has a virual router on the windows host to connect to Windows host and outside world. \n",
    "Use `ip route` and look for \"default via\"\n",
    "\n",
    "I used the \"172.17.0.1\" IP address because it is another known name for WSL2 - localhost did not work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82283588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing!!!\n",
    "\n",
    "# Could not get WSL2 to succesfully connect to Ollama on Windows and I don't want to install again in WSL = new approach\n",
    "# thing = extract_5w1h(df.iloc[1]['text'])\n",
    "# print(thing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb765f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying small hugging face model\n",
    "# def get_info_hf_small(text: str):\n",
    "#     pl = pipeline(\"text2text-generation\", model=\"google/flan-t5-small\") # ~308M\n",
    "#     prompt = f\"Extract WHO, WHAT, WHERE, WHEN, WHY, HOW FROM: {text}\"\n",
    "#     return pl(prompt, max_length=150, do_sample=False)\n",
    "\n",
    "# result = get_info_hf_small(df['text'].iloc[1])\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b2135c",
   "metadata": {},
   "source": [
    "https://huggingface.co/HuggingFaceTB/SmolLM3-3B\n",
    "could try SmolLm3\n",
    "Smol Stats -> 5GB and 1GB\n",
    "But it keeps crashing the Kernel\n",
    "\n",
    "http://huggingface.co/HuggingFaceTB/SmolLM2-1.7B-Instruct\n",
    "SmolLM3 only comes in 3B flavor so dropping down to SmolLM2 because 3 is crashing Kernel\n",
    "SmolLM2 -> 3.42G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6be12d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"HuggingFaceTB/SmolLM3-3B\"\n",
    "# device = \"cuda\"  # for GPU usage or \"cpu\" for CPU usage\n",
    "\n",
    "# # load the tokenizer and the model\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_name,\n",
    "# ).to(device)\n",
    "\n",
    "# # prepare the model input\n",
    "# prompt = \"Give me a brief explanation of gravity in simple terms.\"\n",
    "# messages_think = [\n",
    "#     {\"role\": \"user\", \"content\": prompt}\n",
    "# ]\n",
    "\n",
    "# text = tokenizer.apply_chat_template(\n",
    "#     messages_think,\n",
    "#     tokenize=False,\n",
    "#     add_generation_prompt=True,\n",
    "# )\n",
    "# model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# # Generate the output\n",
    "# generated_ids = model.generate(**model_inputs, max_new_tokens=32768)\n",
    "\n",
    "# # Get and decode the output\n",
    "# output_ids = generated_ids[0][len(model_inputs.input_ids[0]) :]\n",
    "# print(tokenizer.decode(output_ids, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "928a7add",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model for first time...\n",
      "Model loaded\n",
      "Tweet: Local Charlotte, NC news station WSOCTV is reporting that sources tell them dash cameras captured Keith Scott getting out of car and coming towards officers with a gun in his hand:#BREAKING: Sources tell Channel 9 dash camera video shows #KeithScott getting out car, coming toward officers with gun in his hand pic.twitter.com/GGuM2Ow3wk  WSOCTV (@wsoctv) September 21, 2016For a second night, protests over a deadly officer-involved shooting in Charlotte, North Carolina, turned violent, with police firing tear gas and demonstrators throwing objects and trying to damage vehicles.Keith Lamont Scott, a father of seven, was killed by police in an apartment complex parking lot Tuesday as officers looked for another man named in a warrant they were trying to serve. The shooting set off a long night of violent protests and Wednesday the demonstrations continued for a second night, starting off as a peaceful march through downtown Charlotte. But when the demonstrators neared an Omni Hotel, some people climbed the roof of an outdoor mall and started throwing objects at the crowd.Other people were banging on the doors of the hotel, and police in riot gear emerged in the entrance.His family said Scott, an African-American, was unarmed and sitting in his car reading a book, waiting for his son to come home from school.But Charlotte-Mecklenburg Police Chief Kerr Putney said Scott exited his car with a gun, not a book. He said officers couldn t find a book at the scene. It s time for the voiceless majority to stand up and be heard,  said the police chief, who is black. It s time to change the narrative because I can tell you from the facts that the story s a little bit different as to how it s been portrayed so far, especially through social media.   CNNBut b b but what about the book?Neighbor who says she saw police shoot #KeithScott disputes CMDP's claim that he was armed with a gun. She says he had a book. @wsoctv pic.twitter.com/X1PWw5QJ7O  Mark Barber (@MBarberWSOC9) September 21, 2016And of course, there s Hillary s irresponsible tweet confirming that the cops were in the wrong, without even having any facts to back up her ignorant statement:Keith Lamont Scott. Terence Crutcher. Too many others. This has got to end. -H  Hillary Clinton (@HillaryClinton) September 21, 2016 \n",
      "Result: {'WHO': 'person', 'WHAT': 'action', 'WHERE': 'location', 'WHEN': 'time', 'WHY': 'reason', 'HOW': 'method'}\n",
      "Tweet: The tsunami has started President Obama s Kenyan half-brother wants to make America great again   so he s voting for Donald Trump. I like Donald Trump because he speaks from the heart,  Malik Obama told The Post from his home in the rural village of Kogelo.  Make America Great Again is a great slogan. I would like to meet him. Obama, 58, a longtime Democrat, said his  deep disappointment  in his brother Barack s administration has led him to recently switch allegiance to  the party of Lincoln. The last straw, he said, came earlier this month when FBI Director James Comey recommended not prosecuting Democratic presidential candidate Hillary Clinton over her use of a private e-mail servers while secretary of state. She should have known better as the custodian of classified information,  said Obama.He s also annoyed that Clinton and President Obama killed Libyan leader Moammar Khadafy, whom he called one of his best friends.Malik Obama dedicated his 2012 biography of his late father to Khadafy and others who were  making this world a better place. I still feel that getting rid of Khadafy didn t make things any better in Libya,  he said.  My brother and the secretary of state disappointed me in that regard. But what bothers him even more is the Democratic Party s support of same-sex marriage.Obama plans to trek back to the US to vote for Trump in November. Obama used to live in Maryland, where he worked for many years as an accountant and is registered to vote there, public records show. Mr. Trump is providing something new and something fresh,  he said.For entire story: NYP\n",
      "Extra data: line 2 column 1 (char 112)\n",
      "Result: {'raw': '{\"WHO\": \"Malik Obama\", \"WHAT\": \"says\", \"WHERE\": \"text\", \"WHEN\": \"2016\", \"WHY\": \"disappointment\", \"HOW\": \"says\"}\\n{\"WHO\": \"Malik Obama\", \"WHAT\": \"says\", \"WHERE\": \"text\", \"WHEN\": \"2016\", \"WHY\": \"disappointment\", \"HOW\": \"says\"}\\n{\"WHO\": \"Malik Obama\", \"WHAT\": \"says\", \"WHERE'}\n",
      "Tweet: The only reality show Donald Trump should have ever been featured in is The Biggest Loser because he just got his ass handed to him in court.Two years ago, Trump National Doral Miami golf resort signed a contract worth $200,000 for a local business called The Paint Spot to provide paint used to renovate the golf course.Well, guess who tried to stiff The Paint Spot of the final $34,863 payment in the deal?Yeah, that would be Republican nominee Donald J. Trump.Trump and his company refused to honor the contract by not paying the final payment, saying that they ve  paid enough  for the paint. In other words, Trump negotiated a deal that ended up costing him more in the end, just like the kinds of deals he wants to negotiate for America with the rest of the world.Anyway, Paint Spot owner Juan Carlos Enriquez filed suit against Trump in court, and Judge Jorge Cueto just slapped Trump and his company with a $300,000 hit to cover The Paint Shop s attorney and court fees, nearly ten times more money than the $34,863 owed. And Trump STILL hasn t paid that debt, so how are we supposed to trust him to pay down the debt of en entire nation if he can t meet his own obligations? I m happy I have a judgment,  Enriquez told the Miami Herald.  But he [Trump] hasn t paid yet. You know how he says he ll surround himself with the greatest people if he is president? In this case, he might not be surrounded by the right people. This isn t even the first fine Trump has been ordered to pay this month.On the same day as his coronation as the Republican Party nominee last week, the National Labor Relations Board slapped Trump with an $11,200 fine for treating employees like shit because they tried to join a labor union.As it turns out, one employee was wrongfully fired and the other was retaliated against by denying them the promotion they had earned.For someone who claims to be the  law and order  candidate, Trump sure does break the law a lot.Featured Image: Christopher Furlong/Getty Images\n",
      "Result: {'raw': '{\"WHO\": \"Donald J. Trump\", \"WHAT\": \"action\", \"WHERE\": \"location\", \"WHEN\": \"2016\", \"WHY\": \"refused to honor the contract\", \"HOW\": \"signed a contract worth $200,000 for a local business called The Paint Spot to provide paint used to renovate the golf course\", \"REASON\": \"refused to pay the final payment\", \"METHOD\": \"Judge Jorge Cueto slapped Trump and his company with a $300,'}\n",
      "Tweet: No Food, No FEMA: Hurricane Michael’s Survivors Are Furious https://thebea.st/2CIXNoz \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 67\u001b[39m\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m tweet \u001b[38;5;129;01min\u001b[39;00m df[\u001b[33m'\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m'\u001b[39m].head(\u001b[32m25\u001b[39m):\n\u001b[32m     66\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTweet: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtweet\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m     result = \u001b[43mextract_5w1h_chat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtweet\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     68\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mResult: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     69\u001b[39m     test_results.append({\n\u001b[32m     70\u001b[39m         \u001b[33m'\u001b[39m\u001b[33minput\u001b[39m\u001b[33m'\u001b[39m: tweet,\n\u001b[32m     71\u001b[39m         \u001b[33m'\u001b[39m\u001b[33moutput\u001b[39m\u001b[33m'\u001b[39m: result\n\u001b[32m     72\u001b[39m     })\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 32\u001b[39m, in \u001b[36mextract_5w1h_chat\u001b[39m\u001b[34m(text)\u001b[39m\n\u001b[32m     29\u001b[39m input_text = tokenizer.apply_chat_template(messages, tokenize=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     30\u001b[39m inputs = tokenizer(input_text, return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m, padding=\u001b[38;5;28;01mTrue\u001b[39;00m).to(device)\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m outputs = \u001b[43msmol_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m.\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Fix the warning\u001b[39;49;00m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m120\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpad_token_id\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m response = tokenizer.decode(outputs[\u001b[32m0\u001b[39m][inputs.input_ids.shape[\u001b[32m1\u001b[39m]:], skip_special_tokens=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     43\u001b[39m \u001b[38;5;66;03m# Clean up the response - extract just JSON part\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/iu-autonomous-fact-checker/.venv/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/iu-autonomous-fact-checker/.venv/lib/python3.11/site-packages/transformers/generation/utils.py:2633\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[39m\n\u001b[32m   2625\u001b[39m     input_ids, model_kwargs = \u001b[38;5;28mself\u001b[39m._expand_inputs_for_generation(\n\u001b[32m   2626\u001b[39m         input_ids=input_ids,\n\u001b[32m   2627\u001b[39m         expand_size=generation_config.num_return_sequences,\n\u001b[32m   2628\u001b[39m         is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2629\u001b[39m         **model_kwargs,\n\u001b[32m   2630\u001b[39m     )\n\u001b[32m   2632\u001b[39m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2633\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2634\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2635\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2636\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2637\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2638\u001b[39m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m=\u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2639\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2640\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2641\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2643\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode.BEAM_SAMPLE, GenerationMode.BEAM_SEARCH):\n\u001b[32m   2644\u001b[39m     \u001b[38;5;66;03m# 11. interleave input_ids with `num_beams` additional sequences per batch\u001b[39;00m\n\u001b[32m   2645\u001b[39m     input_ids, model_kwargs = \u001b[38;5;28mself\u001b[39m._expand_inputs_for_generation(\n\u001b[32m   2646\u001b[39m         input_ids=input_ids,\n\u001b[32m   2647\u001b[39m         expand_size=generation_config.num_beams,\n\u001b[32m   2648\u001b[39m         is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2649\u001b[39m         **model_kwargs,\n\u001b[32m   2650\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/iu-autonomous-fact-checker/.venv/lib/python3.11/site-packages/transformers/generation/utils.py:3617\u001b[39m, in \u001b[36mGenerationMixin._sample\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[39m\n\u001b[32m   3615\u001b[39m     is_prefill = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   3616\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3617\u001b[39m     outputs = \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   3619\u001b[39m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[32m   3620\u001b[39m model_kwargs = \u001b[38;5;28mself\u001b[39m._update_model_kwargs_for_generation(\n\u001b[32m   3621\u001b[39m     outputs,\n\u001b[32m   3622\u001b[39m     model_kwargs,\n\u001b[32m   3623\u001b[39m     is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   3624\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/iu-autonomous-fact-checker/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/iu-autonomous-fact-checker/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/iu-autonomous-fact-checker/.venv/lib/python3.11/site-packages/transformers/utils/generic.py:961\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    959\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_dict_passed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    960\u001b[39m     return_dict = return_dict_passed\n\u001b[32m--> \u001b[39m\u001b[32m961\u001b[39m output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    962\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    963\u001b[39m     output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/iu-autonomous-fact-checker/.venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:460\u001b[39m, in \u001b[36mLlamaForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, cache_position, logits_to_keep, **kwargs)\u001b[39m\n\u001b[32m    428\u001b[39m \u001b[38;5;129m@can_return_tuple\u001b[39m\n\u001b[32m    429\u001b[39m \u001b[38;5;129m@auto_docstring\u001b[39m\n\u001b[32m    430\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m   (...)\u001b[39m\u001b[32m    441\u001b[39m     **kwargs: Unpack[TransformersKwargs],\n\u001b[32m    442\u001b[39m ) -> CausalLMOutputWithPast:\n\u001b[32m    443\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    444\u001b[39m \u001b[33;03m    Example:\u001b[39;00m\n\u001b[32m    445\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    458\u001b[39m \u001b[33;03m    \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\u001b[39;00m\n\u001b[32m    459\u001b[39m \u001b[33;03m    ```\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m460\u001b[39m     outputs: BaseModelOutputWithPast = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    461\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    462\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    463\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    464\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    465\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    466\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    467\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    468\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    469\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    471\u001b[39m     hidden_states = outputs.last_hidden_state\n\u001b[32m    472\u001b[39m     \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/iu-autonomous-fact-checker/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/iu-autonomous-fact-checker/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/iu-autonomous-fact-checker/.venv/lib/python3.11/site-packages/transformers/utils/generic.py:1069\u001b[39m, in \u001b[36mcheck_model_inputs.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1066\u001b[39m                 module.forward = make_capture_wrapper(module, original_forward, key, specs.index)\n\u001b[32m   1067\u001b[39m                 monkey_patched_layers.append((module, original_forward))\n\u001b[32m-> \u001b[39m\u001b[32m1069\u001b[39m outputs = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1070\u001b[39m \u001b[38;5;66;03m# Restore original forward methods\u001b[39;00m\n\u001b[32m   1071\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m module, original_forward \u001b[38;5;129;01min\u001b[39;00m monkey_patched_layers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/iu-autonomous-fact-checker/.venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:390\u001b[39m, in \u001b[36mLlamaModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, cache_position, use_cache, **kwargs)\u001b[39m\n\u001b[32m    387\u001b[39m position_embeddings = \u001b[38;5;28mself\u001b[39m.rotary_emb(hidden_states, position_ids)\n\u001b[32m    389\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m decoder_layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.layers[: \u001b[38;5;28mself\u001b[39m.config.num_hidden_layers]:\n\u001b[32m--> \u001b[39m\u001b[32m390\u001b[39m     hidden_states = \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    391\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    392\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    393\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    394\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    395\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    396\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    397\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    398\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    400\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.norm(hidden_states)\n\u001b[32m    401\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m BaseModelOutputWithPast(\n\u001b[32m    402\u001b[39m     last_hidden_state=hidden_states,\n\u001b[32m    403\u001b[39m     past_key_values=past_key_values,\n\u001b[32m    404\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/iu-autonomous-fact-checker/.venv/lib/python3.11/site-packages/transformers/modeling_layers.py:94\u001b[39m, in \u001b[36mGradientCheckpointingLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     91\u001b[39m         logger.warning(message)\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m, **kwargs), *args)\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/iu-autonomous-fact-checker/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/iu-autonomous-fact-checker/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/iu-autonomous-fact-checker/.venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:304\u001b[39m, in \u001b[36mLlamaDecoderLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, position_ids, past_key_value, use_cache, cache_position, position_embeddings, **kwargs)\u001b[39m\n\u001b[32m    302\u001b[39m residual = hidden_states\n\u001b[32m    303\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.post_attention_layernorm(hidden_states)\n\u001b[32m--> \u001b[39m\u001b[32m304\u001b[39m hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    305\u001b[39m hidden_states = residual + hidden_states\n\u001b[32m    306\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/iu-autonomous-fact-checker/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/iu-autonomous-fact-checker/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/iu-autonomous-fact-checker/.venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:152\u001b[39m, in \u001b[36mLlamaMLP.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m--> \u001b[39m\u001b[32m152\u001b[39m     down_proj = \u001b[38;5;28mself\u001b[39m.down_proj(\u001b[38;5;28mself\u001b[39m.act_fn(\u001b[38;5;28mself\u001b[39m.gate_proj(x)) * \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mup_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    153\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m down_proj\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/iu-autonomous-fact-checker/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/iu-autonomous-fact-checker/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/iu-autonomous-fact-checker/.venv/lib/python3.11/site-packages/torch/nn/modules/linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# reloading the same model I think caused RAM issues\n",
    "if 'smol_model' not in globals():\n",
    "    print(\"Loading model for first time...\")\n",
    "    checkpoint = \"HuggingFaceTB/SmolLM2-1.7B-Instruct\"\n",
    "    device = \"cpu\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "    smol_model = AutoModelForCausalLM.from_pretrained(checkpoint).to(device)\n",
    "\n",
    "    # Fix the pad token issue\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    print(\"Model loaded\")\n",
    "else:\n",
    "    print(\"Model already loaded, skip loading again...\")\n",
    "\n",
    "# TODO: Proper Class for the return type might make this easier.\n",
    "# The Function  \n",
    "def extract_5w1h_chat(text: str):\n",
    "    messages = [{\n",
    "        \"role\": \"user\", \n",
    "        \"content\": f\"\"\"Extract 5W1H information from this text. Return only valid JSON. If a value cannot be determined, please indicate with 'null':\n",
    "\n",
    "Text: {text}\n",
    "\n",
    "JSON format:\n",
    "{{\"WHO\": \"person\", \"WHAT\": \"action\", \"WHERE\": \"location\", \"WHEN\": \"time\", \"WHY\": \"reason\", \"HOW\": \"method\"}}\"\"\"\n",
    "    }]\n",
    "    \n",
    "    input_text = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True).to(device)\n",
    "    \n",
    "    outputs = smol_model.generate(\n",
    "        inputs.input_ids,\n",
    "        attention_mask=inputs.attention_mask,  # Fix the warning\n",
    "        max_new_tokens=120,\n",
    "        temperature=0.1,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.pad_token_id\n",
    "    )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
    "    \n",
    "    # Clean up the response - extract just JSON part\n",
    "    response = response.strip()\n",
    "\n",
    "    # Remove \"assistant\" prefix if present\n",
    "    if response.startswith(\"assistant\"):\n",
    "        response = response[len(\"assistant\"):].strip()\n",
    "    \n",
    "    json_match = re.search(r'\\{.*\\}', response, re.DOTALL)\n",
    "    if json_match:\n",
    "        json_str = json_match.group(0)\n",
    "        try:\n",
    "            parsed = json.loads(json_str)\n",
    "            return parsed\n",
    "        except json.JSONDecodeError as err:\n",
    "            print(err)\n",
    "            return {\"raw\": response}\n",
    "\n",
    "    return {\"raw\": response}\n",
    "\n",
    "# Test\n",
    "# test_text = \"Biden announced new climate policies in Washington yesterday\"\n",
    "test_results = []\n",
    "for tweet in df['text'].head(25):\n",
    "    print(f\"Tweet: {tweet}\")\n",
    "    result = extract_5w1h_chat(tweet)\n",
    "    print(f\"Result: {result}\")\n",
    "    test_results.append({\n",
    "        'input': tweet,\n",
    "        'output': result\n",
    "    })\n",
    "\n",
    "with open('./test_results.json', 'w') as file:\n",
    "    json.dump(test_results, file, indent=2)\n",
    "\n",
    "# Try to parse as JSON\n",
    "# print(\"Clean JSON:\")\n",
    "# print(json.dumps(result, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169186df",
   "metadata": {},
   "source": [
    "First SmolLM2-1.7B-Instruct test was not successful but did obtain results. \n",
    "Then Kernel crashed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18bda1b5",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
