{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddb4a4f8-6852-4470-8c79-b836c8477b90",
   "metadata": {},
   "source": [
    "# Statement Classifier\n",
    "\n",
    "This project's goal is to train a model that can determine if a statement is either a claim that can be fact-checked, or some other statement like an opinion that cannot be fact checked. \n",
    "\n",
    "## TODO:\n",
    "\n",
    "- [ ] Before training again, setup file structure for saving the 'latest' model, and moving them back into time-stamped dirs, either save some metadata file or something. \n",
    "- [ ] Config at top to control what runs when you click GO.\n",
    "- [ ] function-ize processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "06276f02-8227-4961-926d-68698649d640",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    pipeline\n",
    ")\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.utils import resample\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import re\n",
    "from typing import List\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5ecb714a-dd3c-4522-a851-08f102d32b5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "cell": {
        "!": "OSMagics",
        "HTML": "Other",
        "SVG": "Other",
        "bash": "Other",
        "capture": "ExecutionMagics",
        "code_wrap": "ExecutionMagics",
        "debug": "ExecutionMagics",
        "file": "Other",
        "html": "DisplayMagics",
        "javascript": "DisplayMagics",
        "js": "DisplayMagics",
        "latex": "DisplayMagics",
        "markdown": "DisplayMagics",
        "perl": "Other",
        "prun": "ExecutionMagics",
        "pypy": "Other",
        "python": "Other",
        "python2": "Other",
        "python3": "Other",
        "ruby": "Other",
        "script": "ScriptMagics",
        "sh": "Other",
        "svg": "DisplayMagics",
        "sx": "OSMagics",
        "system": "OSMagics",
        "time": "ExecutionMagics",
        "timeit": "ExecutionMagics",
        "writefile": "OSMagics"
       },
       "line": {
        "alias": "OSMagics",
        "alias_magic": "BasicMagics",
        "autoawait": "AsyncMagics",
        "autocall": "AutoMagics",
        "automagic": "AutoMagics",
        "autosave": "KernelMagics",
        "bookmark": "OSMagics",
        "cat": "Other",
        "cd": "OSMagics",
        "clear": "KernelMagics",
        "code_wrap": "ExecutionMagics",
        "colors": "BasicMagics",
        "conda": "PackagingMagics",
        "config": "ConfigMagics",
        "connect_info": "KernelMagics",
        "cp": "Other",
        "debug": "ExecutionMagics",
        "dhist": "OSMagics",
        "dirs": "OSMagics",
        "doctest_mode": "BasicMagics",
        "ed": "Other",
        "edit": "KernelMagics",
        "env": "OSMagics",
        "gui": "BasicMagics",
        "hist": "Other",
        "history": "HistoryMagics",
        "killbgscripts": "ScriptMagics",
        "ldir": "Other",
        "less": "KernelMagics",
        "lf": "Other",
        "lk": "Other",
        "ll": "Other",
        "load": "CodeMagics",
        "load_ext": "ExtensionMagics",
        "loadpy": "CodeMagics",
        "logoff": "LoggingMagics",
        "logon": "LoggingMagics",
        "logstart": "LoggingMagics",
        "logstate": "LoggingMagics",
        "logstop": "LoggingMagics",
        "ls": "Other",
        "lsmagic": "BasicMagics",
        "lx": "Other",
        "macro": "ExecutionMagics",
        "magic": "BasicMagics",
        "mamba": "PackagingMagics",
        "man": "KernelMagics",
        "matplotlib": "PylabMagics",
        "micromamba": "PackagingMagics",
        "mkdir": "Other",
        "more": "KernelMagics",
        "mv": "Other",
        "notebook": "BasicMagics",
        "page": "BasicMagics",
        "pastebin": "CodeMagics",
        "pdb": "ExecutionMagics",
        "pdef": "NamespaceMagics",
        "pdoc": "NamespaceMagics",
        "pfile": "NamespaceMagics",
        "pinfo": "NamespaceMagics",
        "pinfo2": "NamespaceMagics",
        "pip": "PackagingMagics",
        "popd": "OSMagics",
        "pprint": "BasicMagics",
        "precision": "BasicMagics",
        "prun": "ExecutionMagics",
        "psearch": "NamespaceMagics",
        "psource": "NamespaceMagics",
        "pushd": "OSMagics",
        "pwd": "OSMagics",
        "pycat": "OSMagics",
        "pylab": "PylabMagics",
        "qtconsole": "KernelMagics",
        "quickref": "BasicMagics",
        "recall": "HistoryMagics",
        "rehashx": "OSMagics",
        "reload_ext": "ExtensionMagics",
        "rep": "Other",
        "rerun": "HistoryMagics",
        "reset": "NamespaceMagics",
        "reset_selective": "NamespaceMagics",
        "rm": "Other",
        "rmdir": "Other",
        "run": "ExecutionMagics",
        "save": "CodeMagics",
        "sc": "OSMagics",
        "set_env": "OSMagics",
        "store": "StoreMagics",
        "sx": "OSMagics",
        "system": "OSMagics",
        "tb": "ExecutionMagics",
        "time": "ExecutionMagics",
        "timeit": "ExecutionMagics",
        "unalias": "OSMagics",
        "unload_ext": "ExtensionMagics",
        "uv": "PackagingMagics",
        "who": "NamespaceMagics",
        "who_ls": "NamespaceMagics",
        "whos": "NamespaceMagics",
        "xdel": "NamespaceMagics",
        "xmode": "BasicMagics"
       }
      },
      "text/plain": [
       "Available line magics:\n",
       "%alias  %alias_magic  %autoawait  %autocall  %automagic  %autosave  %bookmark  %cat  %cd  %clear  %code_wrap  %colors  %conda  %config  %connect_info  %cp  %debug  %dhist  %dirs  %doctest_mode  %ed  %edit  %env  %gui  %hist  %history  %killbgscripts  %ldir  %less  %lf  %lk  %ll  %load  %load_ext  %loadpy  %logoff  %logon  %logstart  %logstate  %logstop  %ls  %lsmagic  %lx  %macro  %magic  %mamba  %man  %matplotlib  %micromamba  %mkdir  %more  %mv  %notebook  %page  %pastebin  %pdb  %pdef  %pdoc  %pfile  %pinfo  %pinfo2  %pip  %popd  %pprint  %precision  %prun  %psearch  %psource  %pushd  %pwd  %pycat  %pylab  %qtconsole  %quickref  %recall  %rehashx  %reload_ext  %rep  %rerun  %reset  %reset_selective  %rm  %rmdir  %run  %save  %sc  %set_env  %store  %sx  %system  %tb  %time  %timeit  %unalias  %unload_ext  %uv  %who  %who_ls  %whos  %xdel  %xmode\n",
       "\n",
       "Available cell magics:\n",
       "%%!  %%HTML  %%SVG  %%bash  %%capture  %%code_wrap  %%debug  %%file  %%html  %%javascript  %%js  %%latex  %%markdown  %%perl  %%prun  %%pypy  %%python  %%python2  %%python3  %%ruby  %%script  %%sh  %%svg  %%sx  %%system  %%time  %%timeit  %%writefile\n",
       "\n",
       "Automagic is ON, % prefix IS NOT needed for line magics."
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%lsmagic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "099785cc-6380-4a5a-848f-1ad363d1a3c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03ed394",
   "metadata": {},
   "source": [
    "Received the following error whilst training the model in first few attempts:\n",
    "\n",
    "```\n",
    "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
    "To disable this warning, you can either:\n",
    "    - Avoid using `tokenizers` before the fork if possible\n",
    "    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
    "```\n",
    "\n",
    "To address this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b847075",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715da55e-e7f7-4eb2-8932-3953fb368d85",
   "metadata": {},
   "source": [
    "## Step 2: Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec527565-22e2-44c0-b0c6-813108d0d6ce",
   "metadata": {},
   "source": [
    "### Sample Data\n",
    "\n",
    "I started with the CSV data, but I did not need the extra information so settling with the JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f0ee1883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- part 01 ---\n",
      "   sentence_id  label                                               text\n",
      "0        27247      1                We're 9 million jobs short of that.\n",
      "1        10766      1  You know, last year up to this time, we've los...\n",
      "2         3327      1  And in November of 1975 I was the first presid...\n",
      "3        19700      1  And what we've done during the Bush administra...\n",
      "4        12600      1  Do you know we don't have a single program spo...\n",
      "        sentence_id        label\n",
      "count   9674.000000  9674.000000\n",
      "mean   16268.353628     0.285714\n",
      "std     9388.575939     0.451777\n",
      "min       16.000000     0.000000\n",
      "25%     8344.000000     0.000000\n",
      "50%    16455.500000     0.000000\n",
      "75%    24086.250000     1.000000\n",
      "max    34458.000000     1.000000\n",
      "\n",
      "\n",
      "--- part 02 ---\n",
      "   sentence_id  label                                               text\n",
      "0        15083      1  When I made my decision to stop all trade with...\n",
      "1        16799      1  We've got the highest inflation we've had in t...\n",
      "2        32570      1  They started from that little area, and now th...\n",
      "3        17644      1  Yes, there has been an increase in poverty, bu...\n",
      "4        32512      1  And, yes, when I was a senator, I did vote to ...\n",
      "        sentence_id        label\n",
      "count   8292.000000  8292.000000\n",
      "mean   16231.974433     0.333333\n",
      "std     9412.544929     0.471433\n",
      "min       20.000000     0.000000\n",
      "25%     8379.500000     0.000000\n",
      "50%    16394.500000     0.000000\n",
      "75%    24028.250000     1.000000\n",
      "max    34458.000000     1.000000\n",
      "\n",
      "\n",
      "--- part 03 ---\n",
      "   sentence_id  label                                               text\n",
      "0         8967      1  In other words, I have seen his program costed...\n",
      "1        27385      1  Our Navy is old -- excuse me, our Navy is smal...\n",
      "2         9818      1  The unemployment, the number of people who are...\n",
      "3        16794      1  Mr. Ford uh - actually has fewer people now in...\n",
      "4        17588      1  Today it is up to about $38,000 of earnings th...\n",
      "        sentence_id         label\n",
      "count  11056.000000  11056.000000\n",
      "mean   16330.066299      0.250000\n",
      "std     9405.549195      0.433032\n",
      "min       16.000000      0.000000\n",
      "25%     8419.750000      0.000000\n",
      "50%    16491.500000      0.000000\n",
      "75%    24134.250000      0.250000\n",
      "max    34457.000000      1.000000\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "here = Path().cwd()\n",
    "cbdata_path = here / \".data_sets\" / \"ClaimBuster_Datasets\" / \"datasets\" # ClaimBuster data location\n",
    "raw_dfs: List[pd.DataFrame] = []\n",
    "\n",
    "for file_path in cbdata_path.iterdir():\n",
    "    if file_path.exists() and file_path.is_file() and file_path.suffix == \".json\":\n",
    "        with open(file_path, 'r') as fileo:\n",
    "            raw_dfs.append(pd.DataFrame(json.load(fileo)))\n",
    "\n",
    "assert len(raw_dfs) > 0\n",
    "\n",
    "for i, j in enumerate(raw_dfs):\n",
    "    assert j is not None\n",
    "    assert type(j) is pd.DataFrame\n",
    "    print(f\"--- part {i+1:02} ---\")\n",
    "    print(j.head())\n",
    "    print(j.describe())\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d59e4630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        sentence_id         label\n",
      "count  29022.000000  29022.000000\n",
      "mean   16281.469161      0.285714\n",
      "std     9401.659478      0.451762\n",
      "min       16.000000      0.000000\n",
      "25%     8384.500000      0.000000\n",
      "50%    16455.500000      0.000000\n",
      "75%    24089.000000      1.000000\n",
      "max    34458.000000      1.000000\n",
      "Dataset Size: 29022\n"
     ]
    }
   ],
   "source": [
    "df = pd.concat(raw_dfs)\n",
    "print(df.describe())\n",
    "print(f\"Dataset Size: {len(df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf2798f",
   "metadata": {},
   "source": [
    "### Additional Data Exploring\n",
    "\n",
    "After building the model and performing some manual testing, the statement, \"Barack Obama was president from 2009 to 2017,\" kept being returned as an opinion when it is actually a verifiable claim.\n",
    "\n",
    "Ended up moving this to its own file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5a925905",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    for sent in df[\"text\"]:\n",
    "        if \"obama\" in sent.casefold():\n",
    "            print(sent)\n",
    "\n",
    "if False:\n",
    "    obama_mask = df[\"text\"].str.contains(\"Obama\", case=False, na=False)\n",
    "    obama_df = df.copy()[obama_mask] # Only Obama entries\n",
    "    obama_claims_df = obama_df[obama_df[\"label\"] == 1 ]\n",
    "    obama_opinions_df = obama_df[obama_df[\"label\"] == 0 ]\n",
    "\n",
    "    obama_mentions_count = len(obama_df)\n",
    "    obama_claims_count = len(obama_claims_df)\n",
    "    obama_opinions_count = len(obama_opinions_df)\n",
    "    print(f\"Total Obama mentions: {obama_mentions_count}\")\n",
    "    print(f\"Obama Claims (LABEL_1): {obama_claims_count}\")\n",
    "    print(f\"Obama Opinions (LABEL_0): {obama_opinions_count}\")\n",
    "    print(f\"Obama Claim Percentage: {(obama_claims_count / obama_mentions_count) * 100}%\")\n",
    "\n",
    "    print(\"\\nSample Obama Entries as Claims\")\n",
    "    print(\"---\" * 5 + \" Claims \" + \"---\" * 5)\n",
    "    print(obama_claims_df.head(10))\n",
    "    print(\"---\" * 5 + \" Opinions \" + \"---\" * 5)\n",
    "    print(obama_opinions_df.head(10))\n",
    "    # for i, text in enumerate(obama_claims_df[\"text\"].head(10)):\n",
    "    #     print(f\"{i}.) \\\"{text}\\\"\")\n",
    "    # print(\"\\nSample Obama Entries as Claims\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f2bdf8-3fc2-475c-ac28-8adac2cf89b9",
   "metadata": {},
   "source": [
    "## Data Fixing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d4eee1ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CLAIMBUSTERS DATA QUALITY ANALYSIS ===\n",
      "\n",
      "1. LABEL DISTRIBUTION:\n",
      "Total samples: 29022\n",
      "Claims (LABEL_1): 8292 (28.6%)\n",
      "Non-claims (LABEL_0): 20730 (71.4%)\n"
     ]
    }
   ],
   "source": [
    "print(\"=== CLAIMBUSTERS DATA QUALITY ANALYSIS ===\\n\")\n",
    "\n",
    "# 1. Check label distribution\n",
    "print(\"1. LABEL DISTRIBUTION:\")\n",
    "print(f\"Total samples: {len(df)}\")\n",
    "print(f\"Claims (LABEL_1): {len(df[df['label'] == 1])} ({len(df[df['label'] == 1])/len(df)*100:.1f}%)\")\n",
    "print(f\"Non-claims (LABEL_0): {len(df[df['label'] == 0])} ({len(df[df['label'] == 0])/len(df)*100:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602f61b9",
   "metadata": {},
   "source": [
    "### Balancing Data\n",
    "\n",
    "The current data is unbalanced - caused issues in the first model.\n",
    "\n",
    "Options appear to be \"Downsampling\" or computing weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f50246d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-Claims: 20730\n",
      "Claims: 8292\n",
      "Non-Claims: 20730\n",
      "Non-Claims Down Sampled: 8292\n",
      "Claims: 8292\n"
     ]
    }
   ],
   "source": [
    "# split into majority / minority\n",
    "df_nonclaims_majority = df[df[\"label\"] == 0] # non-claims\n",
    "df_claims_minority = df[df[\"label\"] == 1] # claims\n",
    "\n",
    "print(f\"Non-Claims: {len(df_nonclaims_majority)}\")\n",
    "print(f\"Claims: {len(df_claims_minority)}\")\n",
    "\n",
    "df_nonclaims_downsampled = resample(\n",
    "    df_nonclaims_majority,\n",
    "    replace=False, # sample without replacement\n",
    "    n_samples=len(df_claims_minority),\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Non-Claims: {len(df_nonclaims_majority)}\")\n",
    "print(f\"Non-Claims Down Sampled: {len(df_nonclaims_downsampled)}\")\n",
    "print(f\"Claims: {len(df_claims_minority)}\")\n",
    "\n",
    "# Data Frame Balanced: equal parts claim and non-claim\n",
    "dfb = pd.concat([df_nonclaims_downsampled, df_claims_minority])\n",
    "dfb.describe()\n",
    "\n",
    "unused_df = df_nonclaims_majority.drop(df_nonclaims_downsampled.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3cb6ed-5c27-4318-9b4c-39402fd7b19e",
   "metadata": {},
   "source": [
    "### Split Data\n",
    "\n",
    "Splitting the data into train and validation/test. \n",
    "Also, I think above we sorted the data by label, so [with this strategy](https://stackoverflow.com/questions/29576430/shuffle-dataframe-rows) we can shuffle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "16678117-8b8a-4724-a9b9-97aca917cc66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 13267\n",
      "Validation samples: 3317\n"
     ]
    }
   ],
   "source": [
    "# Shuffle\n",
    "dfb = dfb.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Split into train/validation sets\n",
    "# Data Frame Training\n",
    "# Data Frame Validation\n",
    "dft, dfv = train_test_split(\n",
    "    dfb,\n",
    "    test_size=0.2, \n",
    "    random_state=42,\n",
    "    shuffle=True,\n",
    "    stratify=dfb[\"label\"],\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(dft)}\")\n",
    "print(f\"Validation samples: {len(dfv)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc2e45e-15a3-4daa-ab59-8066867ac160",
   "metadata": {},
   "source": [
    "## Step 3: Load and Setup BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351c3397-3f67-484f-bec4-de0f32417a92",
   "metadata": {},
   "source": [
    "### Initialize Tokenizer and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ef607283-a81b-4ac3-ad58-e1c5047eeee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded: bert-base-uncased\n",
      "Vocabulary size: 30522\n"
     ]
    }
   ],
   "source": [
    "# Choose your BERT variant\n",
    "# TODO: Add in config at top\n",
    "model_name = \"bert-base-uncased\"  # Good starting point\n",
    "# Alternatives: \"roberta-base\", \"distilbert-base-uncased\" (faster)\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# Ran into tokenization issue - All tensors in a batch should be same length\n",
    "# Some were 100 and but one was 187.\n",
    "# Use padding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name, \n",
    "    num_labels=2  # Binary classification: claim vs opinion\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "print(f\"Model loaded: {model_name}\")\n",
    "print(f\"Vocabulary size: {tokenizer.vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443c15ed-27fe-4269-a29a-1d34df6e8117",
   "metadata": {},
   "source": [
    "### Tokenize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ee539200",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples['text'], \n",
    "        truncation=True, \n",
    "        padding=True, \n",
    "        max_length=256  # Adjust based on your text length\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fcb9b832-9c74-4d30-ad9b-fd218b948b9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf5e5e974a064409b57eae5ffa6fa308",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/13267 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "764471445f5f4312959d21e29b7ac902",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3317 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data tokenized successfully!\n"
     ]
    }
   ],
   "source": [
    "# Convert pandas DataFrames to 🤗 Dataset objects\n",
    "dst = Dataset.from_pandas(dft)\n",
    "dsv = Dataset.from_pandas(dfv)\n",
    "\n",
    "# Apply tokenization\n",
    "train_dataset = dst.map(tokenize_function, batched=True)\n",
    "val_dataset = dsv.map(tokenize_function, batched=True)\n",
    "\n",
    "print(\"Data tokenized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c760390-6573-4142-9d3c-4f2d6fba541f",
   "metadata": {},
   "source": [
    "## Step 4: Fine-Tune Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23d54fa-fa26-49da-886b-7953a0313307",
   "metadata": {},
   "source": [
    "We are doing **transfer learning** with **fine-tuning**. \n",
    "BERT was pre-trained to understand language - Thank you!\n",
    "We fine-tuning the model for a specific task - claim vs opinion here.\n",
    "The technique = Supervised learning with backpropagation\n",
    "\n",
    "Deep dive: BERT has millions of weights to understand language. We are adjusting these to suit our classification task. Only our final classification layer is learning from scratch. The rest of BERT is merely adapting instead of being completely retrained. \n",
    "BERT (I think) expects a \"[MASK]\" token to predict values. \n",
    "By fine-tuning, we add a layer like: `input text -> BERT Encoder -> Classification Head -> [Claim, Opinion] probabilities`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f8686b-5379-4460-aa4f-b61f01096350",
   "metadata": {},
   "source": [
    "### Define Training Arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb8524b-2529-4bc4-bf37-2e108cd72272",
   "metadata": {},
   "source": [
    "[transformers.TrainingArguments](https://huggingface.co/docs/transformers/v4.52.3/en/main_classes/trainer#transformers.TrainingArguments) has a lot of parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27a93b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up directories for saving\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "# TODO: Give model name at top\n",
    "move_path = Path().cwd() / \"trainingresults\" / f'hide-bert_{timestamp}'\n",
    "out_path = Path().cwd() / \"trainingresults\" / \"latest\"\n",
    "metatdata_file_path = out_path / \"metadata.json\"\n",
    "if metatdata_file_path.exists():\n",
    "    # A model exists in latest already - move to it's timestamp\n",
    "    with open(metatdata_file_path, 'r') as file:\n",
    "        tmp = json.load(file)\n",
    "        ts_path = Path(tmp.path)\n",
    "        out_path.rename(ts_path)\n",
    "    assert not out_path.exists()\n",
    "\n",
    "with open(out_path / \"metadata.json\", 'w') as file:\n",
    "    json.dump({\"path\": str(out_path), \"foundation\": model_name}, file, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f45727b-67f8-4f14-85a9-0c9e27c51bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=out_path, # Working directory during training for logs and checkpoints.\n",
    "    num_train_epochs=3,              # Start with 3, adjust based on results\n",
    "    per_device_train_batch_size=16,  # Reduce if memory issues\n",
    "    per_device_eval_batch_size=16,\n",
    "    warmup_steps=500, # gradually increase learning rate over 500 steps | prevents huge descrutive changes early on\n",
    "    weight_decay=0.01, # Very mild 1% to prevent memorizing training data exactly. \n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    dataloader_pin_memory=False, # can help with GPU transfer speed\n",
    "    fp16=True, # mixed precision can speedup training if supported\n",
    "    dataloader_num_workers=4, # parallel data loading\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ccafc1-ad8d-4f2e-b5a2-72942d938e2d",
   "metadata": {},
   "source": [
    "### Define Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "289bdcd7-1660-4980-b12b-3d8552b3a583",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels, predictions, average='weighted'\n",
    "    )\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585a5fb7-bb35-4f47-bf03-c712c233cf8e",
   "metadata": {},
   "source": [
    "### Initialize and Train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801df9d2-3381-4c84-a1c1-f0369de2bc8f",
   "metadata": {},
   "source": [
    "This is the fun part we all want to do :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d04c1ff8-c218-4086-a2d9-aa722c602f33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2490' max='2490' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2490/2490 07:59, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.152500</td>\n",
       "      <td>0.221319</td>\n",
       "      <td>0.923726</td>\n",
       "      <td>0.923563</td>\n",
       "      <td>0.927361</td>\n",
       "      <td>0.923726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.086400</td>\n",
       "      <td>0.118954</td>\n",
       "      <td>0.970455</td>\n",
       "      <td>0.970455</td>\n",
       "      <td>0.970466</td>\n",
       "      <td>0.970455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.065500</td>\n",
       "      <td>0.141335</td>\n",
       "      <td>0.970757</td>\n",
       "      <td>0.970747</td>\n",
       "      <td>0.971395</td>\n",
       "      <td>0.970757</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved!\n"
     ]
    }
   ],
   "source": [
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "\n",
    "# TODO: Update Path - the latest idea and switching...\n",
    "# Save the model\n",
    "trainer.save_model(out_path) # Where to save model weights and config\n",
    "tokenizer.save_pretrained(out_path) # for tokenizer stuff\n",
    "print(\"Model saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dadf6e7-9380-4abd-9f70-d0621a8e2890",
   "metadata": {},
   "source": [
    "## Step 5: Test Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a071d9c-eeda-430d-ba9c-c6899d498285",
   "metadata": {},
   "source": [
    "### Load Trained Model for Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9f66975a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValidationEntry:\n",
    "    def __init__(self, statement: str, expected: int):\n",
    "        self.statement = statement\n",
    "        self.expected = expected\n",
    "    \n",
    "    def __str__(self):\n",
    "        line1 = f\"{self.__class__}\\n\"\n",
    "        line2 = f\"  Statement: {self.statement}\\n\"\n",
    "        line3 = f\"  Expectation: {'Opinion' if 0 else 'Claim'}\\n\"\n",
    "        return line1 + line2 + line3\n",
    "\n",
    "# ADD LATER\n",
    "manual_tests = [\n",
    "    (\"John Smith was elected mayor in 2020\", 1),\n",
    "    (\"The company reported $2 million in revenue\", 1),\n",
    "    (\"She graduated from Harvard University\", 1),\n",
    "    (\"Billy Joe graduated from Harvard University\", 1),\n",
    "    (\"The meeting was scheduled for 3 PM\", 1),\n",
    "    (\"COVID-19 cases increased by 15% last month\", 1),\n",
    "    (\"This is the best restaurant in town\", 0),\n",
    "    (\"We should invest more in education\", 0),\n",
    "    (\"That movie was terrible\", 0),\n",
    "    (\"This policy is unfair to working families\", 0),\n",
    "    (\"Climate change is the most important issue\", 0),\n",
    "    (\"Barack Obama was president from 2009 to 2017\", 1),\n",
    "    (\"Pizza is the most delicious food ever\", 0),\n",
    "    (\"The stock market closed at 4,500 points\", 1),\n",
    "    (\"This movie deserves an Oscar\", 0),\n",
    "    (\"The man Barack Obama served as Senator from Illinois before becoming president.\", 1),\n",
    "    (\"The man John Doe served as Senator from Illinois before becoming president.\", 1),\n",
    "    (\"Barack Obama won the Nobel Peace Prize in 2009\", 1),\n",
    "    (\"George Washington won the Nobel Peace Prize in 2009\", 1),\n",
    "    (\"Ada Lovelace wrote the first computer program way back in the 1840s!\", 1),\n",
    "    (\"The unemployment rate in the artic is close to 0, that's amazing!\", 1),\n",
    "    (\"Donald Trump only serves himself and the top 1%.\", 0),\n",
    "    (\"Donald Trump's Big Beautiful Bill implements the biggest cut to medicaid in American history.\", 1),\n",
    "]\n",
    "\n",
    "validation_items = []\n",
    "\n",
    "for thing in manual_tests:\n",
    "    validation_items.append(ValidationEntry(thing[0], thing[1]))\n",
    "\n",
    "# for item in validation_items:\n",
    "#     print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "225e1608-98ec-4887-ad9b-2c9673b82ae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "=== Testing the model ===\n",
      "--------------------------------------------------\n",
      "Test 1: FAIL\n",
      "[{'label': 'LABEL_0', 'score': 0.9942793846130371}]\n",
      "Text: 'John Smith was elected mayor in 2020'\n",
      "Prediction: Opinion (confidence: 0.994)\n",
      "Expected: Claim\n",
      "--------------------------------------------------\n",
      "Test 2: PASS\n",
      "[{'label': 'LABEL_1', 'score': 0.9977515339851379}]\n",
      "Text: 'The company reported $2 million in revenue'\n",
      "Prediction: Claim (confidence: 0.998)\n",
      "Expected: Claim\n",
      "--------------------------------------------------\n",
      "Test 3: FAIL\n",
      "[{'label': 'LABEL_0', 'score': 0.9954456090927124}]\n",
      "Text: 'She graduated from Harvard University'\n",
      "Prediction: Opinion (confidence: 0.995)\n",
      "Expected: Claim\n",
      "--------------------------------------------------\n",
      "Test 4: FAIL\n",
      "[{'label': 'LABEL_0', 'score': 0.9969039559364319}]\n",
      "Text: 'Billy Joe graduated from Harvard University'\n",
      "Prediction: Opinion (confidence: 0.997)\n",
      "Expected: Claim\n",
      "--------------------------------------------------\n",
      "Test 5: FAIL\n",
      "[{'label': 'LABEL_0', 'score': 0.9953515529632568}]\n",
      "Text: 'The meeting was scheduled for 3 PM'\n",
      "Prediction: Opinion (confidence: 0.995)\n",
      "Expected: Claim\n",
      "--------------------------------------------------\n",
      "Test 6: PASS\n",
      "[{'label': 'LABEL_1', 'score': 0.9982427358627319}]\n",
      "Text: 'COVID-19 cases increased by 15% last month'\n",
      "Prediction: Claim (confidence: 0.998)\n",
      "Expected: Claim\n",
      "--------------------------------------------------\n",
      "Test 7: PASS\n",
      "[{'label': 'LABEL_0', 'score': 0.9987187385559082}]\n",
      "Text: 'This is the best restaurant in town'\n",
      "Prediction: Opinion (confidence: 0.999)\n",
      "Expected: Opinion\n",
      "--------------------------------------------------\n",
      "Test 8: PASS\n",
      "[{'label': 'LABEL_0', 'score': 0.9989656209945679}]\n",
      "Text: 'We should invest more in education'\n",
      "Prediction: Opinion (confidence: 0.999)\n",
      "Expected: Opinion\n",
      "--------------------------------------------------\n",
      "Test 9: PASS\n",
      "[{'label': 'LABEL_0', 'score': 0.9985939860343933}]\n",
      "Text: 'That movie was terrible'\n",
      "Prediction: Opinion (confidence: 0.999)\n",
      "Expected: Opinion\n",
      "--------------------------------------------------\n",
      "Test 10: PASS\n",
      "[{'label': 'LABEL_0', 'score': 0.9987159967422485}]\n",
      "Text: 'This policy is unfair to working families'\n",
      "Prediction: Opinion (confidence: 0.999)\n",
      "Expected: Opinion\n",
      "--------------------------------------------------\n",
      "Test 11: PASS\n",
      "[{'label': 'LABEL_0', 'score': 0.9988341927528381}]\n",
      "Text: 'Climate change is the most important issue'\n",
      "Prediction: Opinion (confidence: 0.999)\n",
      "Expected: Opinion\n",
      "--------------------------------------------------\n",
      "Test 12: FAIL\n",
      "[{'label': 'LABEL_0', 'score': 0.9953344464302063}]\n",
      "Text: 'Barack Obama was president from 2009 to 2017'\n",
      "Prediction: Opinion (confidence: 0.995)\n",
      "Expected: Claim\n",
      "--------------------------------------------------\n",
      "Test 13: PASS\n",
      "[{'label': 'LABEL_0', 'score': 0.9983810186386108}]\n",
      "Text: 'Pizza is the most delicious food ever'\n",
      "Prediction: Opinion (confidence: 0.998)\n",
      "Expected: Opinion\n",
      "--------------------------------------------------\n",
      "Test 14: PASS\n",
      "[{'label': 'LABEL_1', 'score': 0.997996985912323}]\n",
      "Text: 'The stock market closed at 4,500 points'\n",
      "Prediction: Claim (confidence: 0.998)\n",
      "Expected: Claim\n",
      "--------------------------------------------------\n",
      "Test 15: PASS\n",
      "[{'label': 'LABEL_0', 'score': 0.9985142350196838}]\n",
      "Text: 'This movie deserves an Oscar'\n",
      "Prediction: Opinion (confidence: 0.999)\n",
      "Expected: Opinion\n",
      "--------------------------------------------------\n",
      "Test 16: FAIL\n",
      "[{'label': 'LABEL_0', 'score': 0.9973509311676025}]\n",
      "Text: 'The man Barack Obama served as Senator from Illinois before becoming president.'\n",
      "Prediction: Opinion (confidence: 0.997)\n",
      "Expected: Claim\n",
      "--------------------------------------------------\n",
      "Test 17: FAIL\n",
      "[{'label': 'LABEL_0', 'score': 0.9973539113998413}]\n",
      "Text: 'The man John Doe served as Senator from Illinois before becoming president.'\n",
      "Prediction: Opinion (confidence: 0.997)\n",
      "Expected: Claim\n",
      "--------------------------------------------------\n",
      "Test 18: FAIL\n",
      "[{'label': 'LABEL_0', 'score': 0.9776122570037842}]\n",
      "Text: 'Barack Obama won the Nobel Peace Prize in 2009'\n",
      "Prediction: Opinion (confidence: 0.978)\n",
      "Expected: Claim\n",
      "--------------------------------------------------\n",
      "Test 19: FAIL\n",
      "[{'label': 'LABEL_0', 'score': 0.9627904295921326}]\n",
      "Text: 'George Washington won the Nobel Peace Prize in 2009'\n",
      "Prediction: Opinion (confidence: 0.963)\n",
      "Expected: Claim\n",
      "--------------------------------------------------\n",
      "Test 20: FAIL\n",
      "[{'label': 'LABEL_0', 'score': 0.9884048700332642}]\n",
      "Text: 'Ada Lovelace wrote the first computer program way back in the 1840s!'\n",
      "Prediction: Opinion (confidence: 0.988)\n",
      "Expected: Claim\n",
      "--------------------------------------------------\n",
      "Test 21: PASS\n",
      "[{'label': 'LABEL_1', 'score': 0.9959955215454102}]\n",
      "Text: 'The unemployment rate in the artic is close to 0, that's amazing!'\n",
      "Prediction: Claim (confidence: 0.996)\n",
      "Expected: Claim\n",
      "--------------------------------------------------\n",
      "Test 22: FAIL\n",
      "[{'label': 'LABEL_1', 'score': 0.952358603477478}]\n",
      "Text: 'Donald Trump only serves himself and the top 1%.'\n",
      "Prediction: Claim (confidence: 0.952)\n",
      "Expected: Opinion\n",
      "--------------------------------------------------\n",
      "Test 23: PASS\n",
      "[{'label': 'LABEL_1', 'score': 0.997689962387085}]\n",
      "Text: 'Donald Trump's Big Beautiful Bill implements the biggest cut to medicaid in American history.'\n",
      "Prediction: Claim (confidence: 0.998)\n",
      "Expected: Claim\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "12 Correct\n",
      "11 Wrong\n",
      "23 Total\n",
      "Rate of Success: 52.1739%\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\")\n",
    "# Load your fine-tuned model\n",
    "# TODO: UPDATE!!!\n",
    "classifier = pipeline(\n",
    "    task=\"text-classification\",\n",
    "    model=out_path,\n",
    "    tokenizer=out_path,\n",
    "    device='cuda'\n",
    ")\n",
    "\n",
    "success_cnt = 0\n",
    "print(\"=== Testing the model ===\")\n",
    "print(\"-\" * 50)\n",
    "for i, item in enumerate(validation_items):\n",
    "    result = classifier(item.statement)\n",
    "    actual = 0 if result[0]['label'] == 'LABEL_0' else 1\n",
    "    success = actual == item.expected\n",
    "    success_label = \"PASS\" if success else \"FAIL\"\n",
    "    if success:\n",
    "        success_cnt += 1\n",
    "    prediction_label = \"Claim\" if result[0]['label'] == 'LABEL_1' else \"Opinion\"\n",
    "    expected_label = \"Claim\" if item.expected == 1 else \"Opinion\"\n",
    "    confidence = result[0]['score']\n",
    "    print(f\"Test {i+1}: {success_label}\")\n",
    "    print(result)\n",
    "    print(f\"Text: '{item.statement}'\")\n",
    "    print(f\"Prediction: {prediction_label} (confidence: {confidence:.3f})\")\n",
    "    print(f\"Expected: {expected_label}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(f\"{success_cnt} Correct\")\n",
    "print(f\"{len(validation_items) - success_cnt} Wrong\")\n",
    "print(f\"{len(validation_items)} Total\")\n",
    "print(f\"Rate of Success: {(success_cnt / len(validation_items))*100:.4f}%\")\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97e1955-2f55-4616-be90-8a728c543d59",
   "metadata": {},
   "source": [
    "### Manual Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c8f05c-239c-487e-b10f-58cda5550b3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.970\n",
      "Precision: 0.970\n",
      "Recall: 0.970\n",
      "F1-score: 0.970\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(texts, true_labels):\n",
    "    \"\"\"Evaluate model on a list of texts with known labels\"\"\"\n",
    "    predictions = []\n",
    "    \n",
    "    for text in texts:\n",
    "        result = classifier(text)\n",
    "        # Convert to binary (0 or 1)\n",
    "        pred = 1 if result[0]['label'] == 'LABEL_1' else 0\n",
    "        predictions.append(pred)\n",
    "    \n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        true_labels, predictions, average='weighted'\n",
    "    )\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy:.3f}\")\n",
    "    print(f\"Precision: {precision:.3f}\")\n",
    "    print(f\"Recall: {recall:.3f}\")\n",
    "    print(f\"F1-score: {f1:.3f}\")\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# Warning of using \"pipeline\" sequentially on GPU - use dataset instead.\n",
    "predictions = evaluate_model(dfv[\"text\"], dfv[\"label\"])\n",
    "# predictions = evaluate_model(val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74403df4-62bc-42e0-9d0e-a1a0617727b7",
   "metadata": {},
   "source": [
    "## Step 6: Integration With Fact-Checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "33f9eb55-bc44-44fe-865f-3c273e10c4bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['My opponent Denver Riggleman, running mate of Corey Stewart, was caught on camera campaigning with a white supremacist', 'Now he has been exposed as a devotee of Bigfoot erotica', 'This is not what we need on Capitol Hill.']\n",
      "My opponent Denver Riggleman, running mate of Corey Stewart, was caught on camera campaigning with a white supremacist\n",
      "[{'label': 'LABEL_1', 'score': 0.9840406179428101}]\n",
      "Now he has been exposed as a devotee of Bigfoot erotica\n",
      "[{'label': 'LABEL_0', 'score': 0.6842691898345947}]\n",
      "This is not what we need on Capitol Hill.\n",
      "[{'label': 'LABEL_0', 'score': 0.9990630745887756}]\n",
      "Extracted claims: [{'text': 'My opponent Denver Riggleman, running mate of Corey Stewart, was caught on camera campaigning with a white supremacist', 'confidence': 0.9840406179428101}]\n",
      "- My opponent Denver Riggleman, running mate of Corey Stewart, was caught on camera campaigning with a white supremacist (confidence: 0.984)\n"
     ]
    }
   ],
   "source": [
    "def extract_claims_from_text(text):\n",
    "    \"\"\"\n",
    "    Extract potential factual claims from text\n",
    "    Returns list of sentences classified as factual claims\n",
    "    \"\"\"\n",
    "    # Simple sentence splitting (you might want to use spaCy for better results)\n",
    "    sentences = text.split('. ')\n",
    "    print(sentences)\n",
    "    \n",
    "    claims = []\n",
    "    for sentence in sentences:\n",
    "        if len(sentence.strip()) > 10:  # Skip very short sentences\n",
    "            print(sentence)\n",
    "            result = classifier(sentence)\n",
    "            print(result)\n",
    "            if result[0]['label'] == 'LABEL_1':  # Factual claim\n",
    "                claims.append({\n",
    "                    'text': sentence,\n",
    "                    'confidence': result[0]['score']\n",
    "                })\n",
    "    \n",
    "    return claims\n",
    "\n",
    "# Test with a Twitter example\n",
    "twitter_text = \"\"\"My opponent Denver Riggleman, running mate of Corey Stewart, was caught on camera campaigning with a white supremacist. Now he has been exposed as a devotee of Bigfoot erotica. This is not what we need on Capitol Hill.\"\"\"\n",
    "\n",
    "claims = extract_claims_from_text(twitter_text)\n",
    "print(f\"Extracted claims: {claims}\")\n",
    "for claim in claims:\n",
    "    print(f\"- {claim['text']} (confidence: {claim['confidence']:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77962dd3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
